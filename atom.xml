<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>CV home</title>
  
  <subtitle>Computer vision paper notes</subtitle>
  <link href="https://www.zjp97.top/atom.xml" rel="self"/>
  
  <link href="https://www.zjp97.top/"/>
  <updated>2022-06-11T03:11:39.872Z</updated>
  <id>https://www.zjp97.top/</id>
  
  <author>
    <name>摸鱼的SAO年</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Global Tracking via Ensemble of Local Trackers</title>
    <link href="https://www.zjp97.top/tracking/60-GTELT/"/>
    <id>https://www.zjp97.top/tracking/60-GTELT/</id>
    <published>2022-06-10T07:47:08.000Z</published>
    <updated>2022-06-11T03:11:39.872Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/60-GTELT/20220610154935.png" alt=""></p><p><a href="https://arxiv.org/abs/2203.16092">论文</a> <a href="https://github.com/ZikunZhou/GTELT">代码</a> 暂未开源</p><p>针对长时跟踪问题，通过集成多个局部跟踪器实现全局跟踪，以全局视角跟踪目标并且利用时间上下文。</p><span id="more"></span><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p><strong>问题：</strong>长时跟踪的难点在于出视野或遮挡导致的目标运动不连续；</p><p><strong>现有方法：</strong></p><ul><li>局部-全局切换的策略——局部跟踪器进行平滑跟踪，目标丢失后切换到全局重检测。</li></ul><p>局限：难以确定切换时机，是否切换完全由局部跟踪器决定，有可能漂移到干扰物上还认为在稳定跟踪导致没有进入重捕。</p><ul><li>在整张图全局搜索——全局的one-shot detection，如GlobalTrack</li></ul><p>局限：忽视了时序上下文，对目标外观变化和背景干扰敏感</p><p> <strong>本文方法：</strong>通过集成多个局部跟踪器实现全局跟踪，结合了上述方法的优点：以全局视角跟踪目标并且利用时间上下文。</p><p> <strong>实现：</strong></p><ul><li>在整张图像不同参考位置上分别设置局部跟踪器，每个局部跟踪器在参考位置周围的局部区域内搜索目标，所有局部跟踪器的搜索区域就可以覆盖整个图像。</li><li>当目标平滑移动时，一个局部跟踪器可以稳定处理；当目标丢失时，靠近目标的另一个局部跟踪器就会被激活接管跟踪，而之前的跟踪器会被重置到初始位置。</li><li>局部跟踪器稳定跟踪目标期间，利用时间上下文提高局部跟踪的鲁棒性</li></ul><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/60-GTELT/20220610155357.png" alt=""></p><p><strong>创新点：</strong></p><ul><li>deformable attention-based local tracker 通过动态移动局部搜索区域，在全局视野中模拟局部跟踪</li><li>temporal context transferring scheme 探索局部跟踪器中的时序上下文</li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/60-GTELT/20220610155457.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/60-GTELT/20220610155528.png" alt=""></p><p>有点借鉴Deformable DETR来做跟踪的意思。设置了N个query，每个query在其参考点附近做Deformable Attention来生成一个候选目标向量，候选目标向量送入预测头生成置信度和目标框，然后通过匈牙利算法匹配真实目标计算损失。</p><p>当选中一个query进行稳定跟踪后，就用跟踪结果来更新该query对应的参考点位置，不断在每一帧上进行检测，这就是一个动态变化搜索区域的局部跟踪器。同时利用图3的时域聚合模块来更新这个query。</p><p>当目标丢失时，即跟踪置信度小于阈值，选择目标附近的另一个query接管跟踪，而先前的query会被重置到它的初始参考点位置。</p><p>具体实现可以参考论文，挺有意思的。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/60-GTELT/20220610161831.png" alt=""></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/60-GTELT/20220610154935.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.16092&quot;&gt;论文&lt;/a&gt; &lt;a href=&quot;https://github.com/ZikunZhou/GTELT&quot;&gt;代码&lt;/a&gt; 暂未开源&lt;/p&gt;
&lt;p&gt;针对长时跟踪问题，通过集成多个局部跟踪器实现全局跟踪，以全局视角跟踪目标并且利用时间上下文。&lt;/p&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="transform" scheme="https://www.zjp97.top/tags/transform/"/>
    
    <category term="CVPR2022" scheme="https://www.zjp97.top/tags/CVPR2022/"/>
    
    <category term="long-term" scheme="https://www.zjp97.top/tags/long-term/"/>
    
  </entry>
  
  <entry>
    <title>Transformer Tracking with Cyclic Shifting Window Attention</title>
    <link href="https://www.zjp97.top/tracking/59-CSWinTT/"/>
    <id>https://www.zjp97.top/tracking/59-CSWinTT/</id>
    <published>2022-06-09T15:44:48.000Z</published>
    <updated>2022-06-11T03:11:29.356Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/59-CSWinTT/20220609234559.png" alt=""></p><p><a href="https://arxiv.org/abs/2205.03806">论文</a> <a href="https://github.com/SkyeSong38/CSWinTT">代码</a></p><p>具有循环移位窗口注意力</p><span id="more"></span><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p><strong>问题：</strong>现在基于transformer的跟踪器都采用逐像素的attention，破坏了目标的完整性，并丢失了像素间的相对位置信息。（感觉就是说attention没有像卷积那样的局部归纳偏置）</p><p><strong>创新点：</strong></p><ul><li>multi-scale cyclic shifting window attention，将注意力机制从像素级提升到窗口级。（这个名字跟叠buff一样我们拆成三要素来看 ：window——保留目标的完整性和相对位置关系；cyclic shifting——增加window的数量； multi-scale——在attention的多头上分别设置不同的window 大小来表示多尺度）</li><li>空间正则化mask，消除循环位移的边界效应影响</li><li>计算优化策略减小计算冗余</li></ul><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/59-CSWinTT/20220609235231.png" alt=""></p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/59-CSWinTT/20220609235347.png" alt=""></p><h2 id="Multi-Scale-Cyclic-Shifting-Window-Attention"><a href="#Multi-Scale-Cyclic-Shifting-Window-Attention" class="headerlink" title="Multi-Scale Cyclic Shifting Window Attention"></a>Multi-Scale Cyclic Shifting Window Attention</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/59-CSWinTT/20220609235402.png" alt=""></p><p>论文描述的比较复杂，我这里就简化一下，按照开始提到的三要素来介绍：</p><p><strong>构建window序列</strong> 经过特征提取后，将维度为$H \times W \times d$的特征按窗口大小 $r$ reshape成一个窗口序列$H/r \times W/r \times r \times r \times d$，然后把窗口看成一个整体，将窗口中的每个空间位置拉平到通道维度，变成$(H/r \times W/r) \times (r \times r \times d)$，相当于有 $H/r \times W/r$ 个维度为 $r \times r \times d$ 的向量。对模板和搜索特征做相同的操作并且将两个窗口序列拼接。</p><p><strong>multi-scale</strong> 通过在attention的多头上分别设置不同的窗口大小来表示多尺度。</p><p><strong>cyclic shifting</strong> 设计循环位移主要有两个动机：</p><ol><li>window attention 减少了序列的个数，即从$H \times W$下降到$H/r \times W/r$，使得attention map的分辨率降低，相似性分数比较粗糙。</li><li>多头注意力在每个头设置了不同的窗口大小，使每个头的尺寸$H/r \times W/r$不一样，难以融合。</li></ol><p>因此作者设计了循环位移增加窗口的数量，参照相关滤波中的循环位移，将窗口沿着x和y方向分别每次移动一个像素，最远移动距离是$r$，即一个$r\times r$的窗口经过循环位移后产生$r^2$个同样大小的窗口样本。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/59-CSWinTT/20220610103521.png" alt=""></p><p>经过循环位移后，得到的窗口序列维度为$(H/r \times W/r \times \textcolor{Red}{r \times r}) \times (\textcolor{Green}{r \times r} \times d)$，其中$\textcolor{Red}{r \times r}$表示循环位移窗口数量，$\textcolor{Green}{r \times r}$表示窗口的大小。然后拿它去做标准的multi-head attention。</p><h2 id="Spatially-regularized-attention-mask"><a href="#Spatially-regularized-attention-mask" class="headerlink" title="Spatially regularized attention mask"></a>Spatially regularized attention mask</h2><p>类似相关滤波中的空间正则化，用于抑制循环位移边界效应带来的性能下降，正则化mask直接叠加到attention map上。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/59-CSWinTT/20220610105959.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/59-CSWinTT/20220610105638.png" alt=""></p><h2 id="Computational-optimization"><a href="#Computational-optimization" class="headerlink" title="Computational optimization"></a>Computational optimization</h2><p>循环移位操作极大地增加了计算冗余，提出了3种优化策略提升计算效率：</p><ul><li>query不使用循环位移：此时Q的维度保持$(H/r \times W/r) \times (\textcolor{Green}{r \times r} \times d)$，而K,V的维度是$(H/r \times W/r \times \textcolor{Red}{r \times r}) \times (\textcolor{Green}{r \times r} \times d)$。这样每个head计算$Attention(Q, K, V)$后的维度是$(H/r \times W/r) \times (\textcolor{Green}{r \times r} \times d)$，多头融合时似乎又面临尺度不一致的问题。在代码实现中，通过两次<code>view</code>将$\textcolor{Green}{r \times r}$提到前面又变回了$H \times W \times d$，然后再进行多头融合。这么看来，不管有没有循环位移，都可以进行多头融合。那么循环位移的唯一目的就是增加窗口数量，获得更精细的注意力相似图。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t1 = merge_feat[:, :first_L, :]  <span class="comment"># (b, first_L, window_size*window_size*c)</span></span><br><span class="line">t1 = t1.view(b, self.h2 // window_size_h,</span><br><span class="line">             self.w2 // window_size_w, window_size_h, window_size_w, c)</span><br><span class="line">t1 = t1.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(b, -<span class="number">1</span>, c)</span><br></pre></td></tr></table></figure><ul><li><p>移位周期减半：将循环位移的生成的的样本数量从$(2r-1)^2$减半变成$r^2$，主要是为了避免重复样本。比如一个一维数组[1,2,3,4,5]，将其向左位移4次[5,1,2,3,4]和向右位移1次[5,1,2,3,4]，其结果是一样的，所以减半了位移次数（向左和向右都只位移2次）。本博文都是按 $r^2$ 的样本数量来描述。</p></li><li><p>编程技巧：使用矩阵坐标来进行循环移位，而不是对矩阵进行直接平移。</p></li></ul><h2 id="和Swin-Transformer差异"><a href="#和Swin-Transformer差异" class="headerlink" title="和Swin Transformer差异"></a>和Swin Transformer差异</h2><p>本文和Swin Transformer都使用了位移和窗口两个概念，主要有三点差异：</p><div class="table-container"><table><thead><tr><th style="text-align:left"></th><th style="text-align:left">Swin Transformer</th><th style="text-align:left">CSWinTT</th></tr></thead><tbody><tr><td style="text-align:left">(1) 注意力应用</td><td style="text-align:left">计算每个窗口内部的像素注意力。</td><td style="text-align:left">将每个窗口看作一个整体，计算窗口间的注意力。</td></tr><tr><td style="text-align:left">(2) 多尺度策略</td><td style="text-align:left">每层用同样大小的窗口，分层合并窗口，在更深的层得到更大的窗口</td><td style="text-align:left">不同的注意力头使用不同的窗口尺寸</td></tr><tr><td style="text-align:left">(3) 窗口位移</td><td style="text-align:left">移位整个特征图，在不同窗口之间交换信息和提供连接</td><td style="text-align:left">在每个窗口中应用独立的循环位移。  每个窗口根据其尺寸移位多次。</td></tr></tbody></table></div><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/59-CSWinTT/20220610115500.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/59-CSWinTT/20220610115512.png" alt=""></p><p><strong>Ablation Study</strong></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/59-CSWinTT/20220610115614.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/59-CSWinTT/20220610115632.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/59-CSWinTT/20220610115643.png" alt=""></p><p>本文提出的三个优化策略虽然大幅提升了速度，但是依然存在计算冗余，主要是循环位移的窗口内依然有很多重复的pixel计算。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/59-CSWinTT/20220610115658.png" alt=""></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/59-CSWinTT/20220609234559.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2205.03806&quot;&gt;论文&lt;/a&gt; &lt;a href=&quot;https://github.com/SkyeSong38/CSWinTT&quot;&gt;代码&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;具有循环移位窗口注意力&lt;/p&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="transform" scheme="https://www.zjp97.top/tags/transform/"/>
    
    <category term="CVPR2022" scheme="https://www.zjp97.top/tags/CVPR2022/"/>
    
  </entry>
  
  <entry>
    <title>Unsupervised Domain Adaptation for Nighttime Aerial Tracking</title>
    <link href="https://www.zjp97.top/tracking/58-UDAT/"/>
    <id>https://www.zjp97.top/tracking/58-UDAT/</id>
    <published>2022-06-09T03:55:46.000Z</published>
    <updated>2022-06-11T03:11:29.356Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/58-UDAT/20220609115914.png" alt=""></p><p><a href="https://arxiv.org/abs/2203.10541">论文</a> <a href="https://github.com/vision4robotics/UDAT">代码</a></p><p>无监督域适应的夜间空中跟踪 + 夜间跟踪benchmark</p><span id="more"></span><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>由于白天和夜间场景的域分布差异（低对比度、亮度、信噪比），导致现有的跟踪器在夜间场景表现较差。而标注大量夜间数据成本过高，因此本文提出一种无监督域适应的方式，即源域（白天）中的训练数据具有手动注释的标签，而目标域中（夜间）的训练数据没有标注。具体贡献如下：</p><ul><li>object discovery 用于预处理，从无标注数据中提取训练patch</li><li>Transformer-based bridging layer  对齐不同域的特征</li><li>Transformer day/night feature discriminator 通过对抗学习进一步缩小域差异</li><li>提出一个夜间空中跟踪数据集NAT2021，包括训练和测试</li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/58-UDAT/20220609153243.png" alt=""></p><p>首先对未标记的目标域数据预处理，采用基于显著性检测的策略来定位潜在目标和裁剪成对训练patch。训练过程中，使用 bridging layer 调制，使得不同域的特征对齐。后面接一个判别器用于区分源域和目标域的特征。整个过程通过对抗性学习来减少源域和目标域之间的特征分布差异。</p><h2 id="Data-preprocessing"><a href="#Data-preprocessing" class="headerlink" title="Data preprocessing"></a>Data preprocessing</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/58-UDAT/20220609154323.png" alt=""></p><ol><li>low-light enhancement：<a href="https://arxiv.org/abs/2103.00860">Learning to Enhance Low-Light Image via Zero-Reference Deep Curve Estimation</a></li><li>salient object detection：<a href="https://drive.google.com/file/d/1czSEpwTaAL4eZpqigzYNPqeyY1Z_XREu/view">Dynamic Context-Sensitive Filtering Network for Video Salient Object Detection</a></li><li>dynamic programming ：根据帧与帧之间候选框的归一化距离进行动态规划，确保轨迹是平滑的。对于动态规划没有选取候选框的帧，采用相邻帧的线性插值得到一个候选框。最后，根据得到的框序列从原始图像中裁剪出成对的训练patch。</li></ol><h2 id="Transformer-bridging-layer"><a href="#Transformer-bridging-layer" class="headerlink" title="Transformer bridging layer"></a>Transformer bridging layer</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/58-UDAT/20220609154428.png" alt=""></p><p>在特征提取后面接一个transformer来缩小白天和夜间特征分布的gap，效果如图4所示，直接通过backbone提取的特征有明显的差异，而由桥接层修改的特征在分布上有重合。</p><h2 id="Transformer-discriminator"><a href="#Transformer-discriminator" class="headerlink" title="Transformer discriminator"></a>Transformer discriminator</h2><p>首先将特征通过gradient reverse layer (GRL) 反转梯度 （参考<a href="https://www.zhihu.com/question/266710153/answer/1338864403">Gradient Reversal Layer指什么？ - Just4Fan的回答 - 知乎</a> )</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/58-UDAT/20220609154917.png" alt=""></p><p>然后增加一个classification token，送入两层Transformer layers输出域分类结果</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/58-UDAT/20220609154943.png" alt=""></p><p>判别器需要正确区分源域和目标域，而前面的特征提取和对齐需要混淆二者，它们形成对抗训练。</p><h1 id="NAT2021"><a href="#NAT2021" class="headerlink" title="NAT2021"></a>NAT2021</h1><p>为了对夜间空中跟踪进行性能评估，并为无监督训练提供足够的无标签夜间跟踪视频，作者提出了 nighttime aerial tracking benchmark (NAT2021)。 训练集没有标注，并且增加了一个新的光照属性</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/58-UDAT/20220609155413.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/58-UDAT/20220609155425.png" alt=""></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/58-UDAT/20220609234059.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/58-UDAT/20220609234121.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/58-UDAT/20220609234257.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/58-UDAT/20220609234307.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/58-UDAT/20220609234341.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/58-UDAT/20220609234356.png" alt=""></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/58-UDAT/20220609115914.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.10541&quot;&gt;论文&lt;/a&gt; &lt;a href=&quot;https://github.com/vision4robotics/UDAT&quot;&gt;代码&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;无监督域适应的夜间空中跟踪 + 夜间跟踪benchmark&lt;/p&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="Domain adaption" scheme="https://www.zjp97.top/tags/Domain-adaption/"/>
    
    <category term="CVPR2022" scheme="https://www.zjp97.top/tags/CVPR2022/"/>
    
    <category term="Unsupervised" scheme="https://www.zjp97.top/tags/Unsupervised/"/>
    
  </entry>
  
  <entry>
    <title>SparseTT: Visual Tracking with Sparse Transformers</title>
    <link href="https://www.zjp97.top/tracking/57-SparseTT/"/>
    <id>https://www.zjp97.top/tracking/57-SparseTT/</id>
    <published>2022-06-08T03:16:26.000Z</published>
    <updated>2022-06-11T03:11:29.356Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/57-SparseTT/20220608111802.png" alt=""></p><p><a href="https://arxiv.org/abs/2205.03776">论文</a> <a href="https://github.com/fzh0917/SparseTT">代码</a></p><p>Transformer中自注意力的全局视角导致主要信息（如搜索区域中的目标）聚焦不足，而次要信息（如搜索区域中的背景）聚焦过度，使前景和背景的区分变得模糊，从而降低了跟踪性能。简单来说就是和每一个点计算注意力，导致背景部分占据了过大的权重，一定程度上削弱了目标。本文使用稀疏注意力缓解这一问题，来突出搜索区域中的潜在目标。</p><span id="more"></span><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><ul><li>自注意力缺乏对搜索区域中最相关信息的关注，因此很容易被背景分散注意力</li><li>设计稀疏注意力关注搜索区域中最相关的信息</li><li>设计双头预测器，提高分类和回归的精度</li><li>稀疏注意力更容易收敛，训练时间相比TransT减少了75%</li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/57-SparseTT/20220608112131.png" alt=""></p><p>标准的三段结构，特征提取，目标聚焦网络和双头预测器。</p><h2 id="目标聚焦网络"><a href="#目标聚焦网络" class="headerlink" title="目标聚焦网络"></a>目标聚焦网络</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/57-SparseTT/20220608112259.png" alt=""></p><p>目标聚焦网络是encoder-decoder架构，encoder输入模板特征，decoder输入搜索特征。其中encoder重要但非必要，后续实验会证明。而本文的核心创新在于decoder橙色部分的<strong>稀疏多头注意力Sparse Multi-Head Self-Attention</strong>。</p><p>朴素MSA中，注意力特征的每个像素值都是由输入特征的所有像素值来计算的，这使得前景边缘区域变得模糊。本文提出的稀疏方法中，注意力特征的每个像素值都只由与其最相似的K个像素值决定，这使得前景更加集中，前景边缘区域更加具有分辨力。</p><p>具体实现如图4中间所示，首先计算query和key的相似度矩阵，然后仅使用softmax函数对相似矩阵每行的K个最大元素进行归一化，其他元素置0。最后将相似度矩阵和value相乘，得到最终结果。图4最右边展示了两种注意力归一ecise RoI-Pooling化的区别，朴素点积注意力放大了相对较小的相似权重，这使得输出特征容易受到噪声和背景干扰的影响。然而，稀疏缩放点积注意力可以显著缓解这个问题。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/57-SparseTT/20220608113034.png" alt=""></p><h2 id="双头预测器"><a href="#双头预测器" class="headerlink" title="双头预测器"></a>双头预测器</h2><p>对于分类和回归，均同时使用2层FC层和L个卷积层来预测。</p><p>推理阶段，对于分类任务，融合FC头和卷积头的分类分数；对于回归任务，只取卷积头输出的预测偏移量。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>在4张2080ti上训练60个小时，相比TransT在同等硬件下训练10天。</p><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p>编码器数量：即使不使用编码器效果依然很好，编码器数量过多导致过拟合。最佳数量为2</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/57-SparseTT/20220608114210.png" alt=""></p><p>解码器数量：最佳数量为2</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/57-SparseTT/20220608114342.png" alt=""></p><p>SMSA中的稀疏度K：最佳数量为32</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/57-SparseTT/20220608114417.png" alt=""></p><h2 id="SOTA-Comparison"><a href="#SOTA-Comparison" class="headerlink" title="SOTA Comparison"></a>SOTA Comparison</h2><p>LaSOT</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/57-SparseTT/20220608114627.png" alt=""></p><p>Got10k</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/57-SparseTT/20220608114659.png" alt=""></p><p>OTB UAV</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/57-SparseTT/20220608114746.png" alt=""></p><p>TrackingNet</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/57-SparseTT/20220608114759.png" alt=""></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/57-SparseTT/20220608111802.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2205.03776&quot;&gt;论文&lt;/a&gt; &lt;a href=&quot;https://github.com/fzh0917/SparseTT&quot;&gt;代码&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Transformer中自注意力的全局视角导致主要信息（如搜索区域中的目标）聚焦不足，而次要信息（如搜索区域中的背景）聚焦过度，使前景和背景的区分变得模糊，从而降低了跟踪性能。简单来说就是和每一个点计算注意力，导致背景部分占据了过大的权重，一定程度上削弱了目标。本文使用稀疏注意力缓解这一问题，来突出搜索区域中的潜在目标。&lt;/p&gt;</summary>
    
    
    
    
    <category term="transform" scheme="https://www.zjp97.top/tags/transform/"/>
    
    <category term="IJCAI2022" scheme="https://www.zjp97.top/tags/IJCAI2022/"/>
    
    <category term="sparse attention" scheme="https://www.zjp97.top/tags/sparse-attention/"/>
    
  </entry>
  
  <entry>
    <title>Unified Transformer Tracker for Object Tracking</title>
    <link href="https://www.zjp97.top/tracking/56-UTT/"/>
    <id>https://www.zjp97.top/tracking/56-UTT/</id>
    <published>2022-06-07T02:43:54.000Z</published>
    <updated>2022-06-11T03:11:29.356Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/56-UTT/20220607104510.png" alt=""></p><p><a href="https://arxiv.org/abs/2203.15175">论文</a></p><p>核心：联合单目标和多目标跟踪</p><span id="more"></span><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>问题：跟踪现在分成了单目标（SOT）和多目标（MOT）两个独立的社区。由于两种任务的训练数据集和跟踪对象的不同，使得它们的方法无法互通</p><p>现有方法：UniTrack使用共享的外观模型和多个多个不用训练的跟踪头来处理不同跟踪任务。但是无法利用大规模跟踪数据集进行训练，并且在SOT表现较差</p><p>本文方法：</p><ul><li>提出Unified Transformer Tracker (UTT) ，构建一个Transformer Tracker在SOT和MOT中跟踪目标，利用目标特征和跟踪帧特征之间的<strong>相关性</strong>来定位目标；</li><li>在各自的数据集上交替优化SOT和MOT，使得一个框架可以同时处理两个任务</li></ul><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/56-UTT/20220607104830.png" alt=""></p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>首先回顾SOT和MOT的定义：</p><ul><li>SOT——给定一个初始帧目标位置B0，定位后续T帧中该目标的位置。跟踪对象可以是任意未知类别；</li><li>MOT——检测并跟踪一组特定类别的目标。MOT需要将第t-1帧中检测到的N个目标，和第t帧中检测到的M个目标进行关联，对于相同的目标赋予唯一的ID。同时需要考虑旧目标的丢失和新目标的出现。</li></ul><p>为了统一两个任务，作者将SOT的初始帧和MOT的前一帧都看成是参考帧，SOT的初始框和MOT中的检测框都看成参考目标框。依此设计了 UTT，如图2所示，经过特征提取后，将参考帧特征 $I^0,I^{t-1}$、跟踪帧特征 $I^t$，以及参考帧中的目标框 $B$ 输入Track Transformer 用于预测当前跟踪帧中的目标位置。Track Transformer有三部分：首先Target Decoder提取目标特征，然后Proposal Decoder在跟踪帧中为每个目标产生候选搜索区域。最后将目标特征和搜索特征都送入Target Tansformer预测目标位置。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/56-UTT/20220607105015.png" alt=""></p><p><strong>Target Decoder</strong></p><p>为了引入更多上下文信息，使用交叉注意力对目标特征和跟踪帧特征进行交互</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/56-UTT/20220607105324.png" alt=""></p><p><strong>Proposal Decoder</strong></p><p>将每一个目标特征与完整的跟踪特征做相关计算量太大，因此需要裁剪一个更小的搜索区域。对于MOT，直接根据前一帧的跟踪框来确定proposal；而对于SOT，考虑到目标丢失导致前一帧跟踪框中没有目标的情况，作者将目标特征与跟踪帧特征相关联来产生更精确的proposal。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/56-UTT/20220607105603.png" alt=""></p><p>这个过程类似stark，公式5得到的heatmap $H$表示两个角点的分布概率，对所有位置的概率求和得到proposal $P^t$。虽然公式5中的维度是N，其实只会生成SOT的一个heatmap，因为对MOT每个目标计算一遍成本还是太高。</p><p><strong>Target Transformer</strong></p><p>通过Self-Attention和Correlation将目标特征与proposal 关联。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/56-UTT/20220607105759.png" alt=""></p><p>公式11的FC 层是将$K^2C$的通道维度降维到$C$，相当于把$K^2$的空间特征压缩成一维向量。最后将得到的N个向量送入boxhead预测相对proposal的偏移量得到预测跟踪框。</p><p>作者迭代了L次target transformer来进行目标定位。第一次的proposal是通过proposal decoder生成的，而后续的proposal就是前一次迭代预测的目标框。</p><h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/56-UTT/20220607110351.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/56-UTT/20220607110400.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/56-UTT/20220607110409.png" alt=""></p><ul><li><p>分别构建SOT和MOT两个dataloader<strong>交替训练</strong>。SOT将图片crop到352，MOT直接对原图进行随机resize；</p></li><li><p>SOT的初始proposal由proposal decoder 生成，并且该proposal（B0）也用于计算loss；MOT的初始proposal是在GT上添加高斯抖动生成的；</p></li><li><p>对于MOT还需要一个额外的 detection head用于检测每一帧的目标类别和位置。MOT的推理阶段需要将track transformer生成的跟踪框和Detection head 生成的检测框进行关联（图5）</p></li></ul><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/56-UTT/20220607110811.png" alt=""></p><h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>SOT在LaSOT、TrackingNet和Got-10k测试，MOT在MOT16测试</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/56-UTT/20220607111020.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/56-UTT/20220607111032.png" alt=""></p><p>SOT相比UniTrack提点明显，MOT的效果还有提升空间</p><p><strong>Ablative Studies</strong></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/56-UTT/20220607111143.png" alt=""></p><p>高亮部分似乎应该是MSA，不知是否是笔误</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/56-UTT/20220607111226.png" alt=""></p><p>表5对比了联合训练和分别单独训练。总体来说联合训练效果更好，只有MOT的IDF1指标比单独用MOT训练低一些。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/56-UTT/20220607111301.png" alt=""></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>本文证明了一个模型能够同时解决SOT和MOT任务。这将鼓励社区开发更多统一的跟踪算法，应用于更复杂的跟踪场景，比如VR/AR应用中经常需要进行SOT和MOT的切换。同时这种联合的范式也更加符合人类的直觉。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/56-UTT/20220607104510.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.15175&quot;&gt;论文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心：联合单目标和多目标跟踪&lt;/p&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="transform" scheme="https://www.zjp97.top/tags/transform/"/>
    
    <category term="CVPR2022" scheme="https://www.zjp97.top/tags/CVPR2022/"/>
    
  </entry>
  
  <entry>
    <title>Transforming Model Prediction for Tracking</title>
    <link href="https://www.zjp97.top/tracking/55-ToMP/"/>
    <id>https://www.zjp97.top/tracking/55-ToMP/</id>
    <published>2022-06-02T03:01:08.000Z</published>
    <updated>2022-06-11T03:11:29.355Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602110219.png" alt=""></p><p><a href="https://arxiv.org/abs/2203.11192">论文</a> <a href="https://github.com/visionml/pytracking">代码</a></p><p>马丁团队的新工作，依然是延续dimp的故事，这次是用transformer作为目标预测模型，取代了之前延续好几代的基于优化的模型预测方法。</p><span id="more"></span><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>基于优化的跟踪方法取得了广泛的成功，其通过最小化目标函数来优化一个目标模型预测模块(target model prediction module)，从而提供有效的全局推理。但是这种方式会产生严重的归纳偏差，限制了网络的表达能力。</p><p>本文提出一种基于Transformer的模型预测模块。Transformer以很小的归纳偏差捕获全局关系，使其能够学习更强大的目标模型预测。此外，进一步扩展model predictor来预测边框回归的权重（之前的方法如dimp的predictor只用于分类，回归采用额外的iounet）。因此最终的跟踪器可以依靠训练和测试帧transductive地预测所有的权重。(transductive不好翻译，个人理解大概就是直推式地生成权重而不是优化迭代生成)</p><p>实验在三个数据集达到SOTA，LaSOT的AUC达到68.5。</p><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p><strong>研究背景：</strong>DCF通过最小化判别式目标函数来学习一个目标模型，用于定位每一帧中的目标。本文的作者团队几乎以一己之力推动了DCF在深度学习时代端到端模型的发展。具体来说，就是将目标函数设置成一个卷积核，这个卷积核可以为跟踪目标提供一个紧凑且通用的表达。然后设计一些快速优化方法使其能够借助少量训练帧在少数迭代次数下进行学习，代表性的ATOM,DiMP,superDiMP等。</p><p><strong>发现问题：</strong>DCF的优点是同时考虑了历史帧的前景和背景，可以提供有效的全局推理。但缺点就是严重的<strong>归纳偏差</strong>(inductive bias)。因为训练目标函数的样本只有历史的几帧，限制模型的灵活性。并且，不能集成先验知识，非常容易过拟合。</p><p><strong>解决方案</strong>：Transformer通过注意力机制同样可以跨帧全局推理，因此作者将二者结合，用Transformer来估计目标模型的权重，从而避开了迭代优化，这样可以得到更powerful的目标模型。</p><p><strong>具体贡献：</strong></p><ol><li>提出一个基于Transformer的模型预测器取代传统的基于优化的方法</li><li>扩展模型预测器不但用于分类还用于回归</li><li>设计了两种编码方式嵌入目标位置和目标扩展，后续的Transformer预测器可以利用这些信息</li><li>在测试阶段提出一个并行的两阶段跟踪过程解耦目标定位和边框回归，得到更精确的目标检测结果</li><li>充分的实验</li></ol><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602113256.png" alt=""></p><p>左边是传统的基于优化的方法，如ATOM, DiMP等，具有如下局限：</p><ol><li>只使用历史帧的有限信息来计算目标模型，无法嵌入先验知识</li><li>无法以transductive的方式利用当前测试帧来计算模型权重</li><li>基于优化的方法需要设置多个超参数，容易过拟合/欠拟合</li><li>只有测试帧（搜索帧）被送入目标模型，如果能用训练帧（模板帧）中所包含的目标状态信息来增强特征，可以更可靠地区分目标和背景。</li></ol><p>右边是本文提出的方法，设计了一个基于Transformer的Model Predictor替代了左边的Model Optimizer，主要包括以下特点：</p><ol><li>通过端到端纯数据训练来学习直接预测目标模型，而不是在线优化目标函数。好处：嵌入目标特定的先验，使得模型更专注于目标的特性，以及区分目标和背景的特征。</li><li>模型预测器除了历史训练帧的特征，还利用了当前测试帧的特征，以transductive的方法预测目标模型。好处：得到一个更合适的目标模型。</li><li>利用目标信息为每一帧动态构建一个更具辨别性的特征空间。</li></ol><p>下面详细介绍各个模块，整体框架如图3所示，我在上面注释了一部分流程：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602114145.png" alt=""></p><h2 id="基于Transformer的模型预测器"><a href="#基于Transformer的模型预测器" class="headerlink" title="基于Transformer的模型预测器"></a>基于Transformer的模型预测器</h2><p><strong>Target Location Encoding</strong> 嵌入训练帧的位置信息，设置了一个可学习的token embedding来表示前景 $e_{fg} \in \mathbb{R}^{1 \times C}$，将其与以目标位置为中心的高斯图 $y_i \in \mathbb{R}^{H \times W \times 1} $ 点乘作为编码：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602115643.png" alt=""></p><p>然后将目标编码与原始特征叠加</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602115659.png" alt=""></p><p>这样得到的训练特征 $v_i \in \mathbb{R}^{H \times W \times C} $ 是编码了目标状态信息的。为了对称，对测试特征也增加了一个编码</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602115923.png" alt=""></p><p>因为测试帧不知道目标位置，所以将token $e_{test}$ 叠加到每一个patch位置上。</p><p><strong>Transformer Encoder</strong> 和stark类似，将训练和测试特征堆叠后送入encoder，输出用于decoder的特征，此外还保留增强后的测试特征 $z_{test}$ 用于后续分类回归。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602151024.png" alt=""></p><p><strong>Transformer Decoder</strong> 将目标状态编码中的 $e_{fg}$ 作为query，和encoder生成的 $z_i, z_{test}$ 一起输入decoder，来预测目标模型权重。</p><p><strong>Target Model</strong> 利用DCF模型来获得目标分类分数</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602151709.png" alt=""></p><p>其中，$\omega \in \mathbb{R}^{1 \times C}$ 是decoder输出的convolution filter的权重，$z_{test}$是encoder中经过训练特征增强后的测试特征，可以更有效定位目标。</p><h2 id="联合定位和边框回归"><a href="#联合定位和边框回归" class="headerlink" title="联合定位和边框回归"></a>联合定位和边框回归</h2><p>上面的方法只是用于预测目标中心，在之前的DiMP中是额外通过iounet来预测回归框。本文提出要联合预测定位和回归，因为这两者可以相互受益。为了实现这一点，需要额外扩展两个地方：1. 生成目标状态编码时，不仅需要目标中心位置，还需要编码目标大小信息，为模型预测器提供更丰富的输入；2. 除了目标模型的权重，还需扩展模型预测器来估计边框回归的权重。</p><p><strong>Target Extent Encoding</strong> 增加一个目标bbox的编码，其实就是计算特征图上每个点距离目标框上下左右四条边的距离，得到一个密集分布的目标框表示 $d=(l,t,r,b), d\in \mathbb{R}^{H \times W \times 4} $</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602153254.png" alt=""></p><p>然后经过一个MLP升维后与上文公式3中的图像特征和目标位置编码叠加一起送入encoder</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602153434.png" alt=""></p><p><strong>Model Prediction</strong> 将decoder输出的权重$\omega$经过一个线性层得到边框回归权重$\omega_{bbreg}$和目标分类权重$\omega_{cls}$。</p><p><strong>Bounding Box Regression</strong> 为了让encoder输出的测试特征$z_{test}$是目标感知的，仿照stark，计算$\omega_{bbreg}*z_{test}$得到一个注意力图，再将其与$z_{test}$点乘后输入回归分支，回归分支是采用FCOS的边框回归。</p><h2 id="离线训练"><a href="#离线训练" class="headerlink" title="离线训练"></a>离线训练</h2><p>类似之前DCF类的训练，从一个序列中采样两张训练图片和一张测试图片送入网络，损失采用FCOS的损失类型，但没有centerness loss。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602154416.png" alt=""></p><h2 id="在线跟踪"><a href="#在线跟踪" class="headerlink" title="在线跟踪"></a>在线跟踪</h2><p>在线跟踪中的训练图像包含一张初始帧和其标注，以及一个在线更新的历史中间帧，其分类得分必须高于阈值。作者发现用这个训练集相比只用初始帧可以提高定位精度，但是会下降回归精度。作者解释为中间帧的预测框不准确可能会影响回归精度，因此提出一个两阶段的跟踪方式。1. 使用完整的两帧来获得分类权重；2. 只用初始帧来计算回归权重。</p><p>这两个步骤可以并行处理，复制一份数据在batch维度堆叠，然后在预测回归模型时通过key_padding_mask来控制某些key不参与计算attention。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>两个模型ToMP-101和ToMP-50，2080Ti运行速度分别是19.6和24.8FPS</p><h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602155815.png" alt=""></p><p><strong>Target State Encoding</strong> 表1分析了不同目标状态编码组合的效果，目标位置编码中除了正文中提到的前景嵌入$e_{fg}$和测试嵌入$e_{test}$ ，额外增加了一个背景嵌入$e_{bg}$。</p><ul><li>增加背景嵌入没什么用，反而性能下降 （4 vs 5）</li><li>只用target extent encoding $\phi$也能取得不错的效果（1）</li><li>解耦Decoder的query和前景嵌入$e_{fg}$会导致性能下降（6）</li><li>嵌入边框信息target extent encoding非常重要，如果没有性能会剧烈下降（7）</li></ul><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602160907.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602161522.png" alt=""></p><p><strong>Model Predictor</strong> 表2显示用两个独立的query分别做分类和回归，效果会大幅下降，原因还是query和前景嵌入$e_{fg}$解耦了。</p><p><strong>Inference Settings</strong> 表3显示增加历史跟踪帧比只用初始帧效果好，但如果不用本文提出的两阶段方法，效果会下降。原因还是边框预测器对历史帧中不准确的预测框敏感。</p><p><strong>Transforming Model Prediction Step-by-Step</strong> 表4和图1展示了在基于优化的DiMP上一步步替换本文的transformer的效果提升。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602161714.png" alt=""></p><p>表10展示了使用更多的帧训练效果可以进一步提升，但是速度会下降。</p><h2 id="SOTA比较"><a href="#SOTA比较" class="headerlink" title="SOTA比较"></a>SOTA比较</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602161928.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602162013.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602162058.png" alt=""></p><h2 id="可视化结果"><a href="#可视化结果" class="headerlink" title="可视化结果"></a>可视化结果</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602162325.png" alt=""></p><p>对比SuperDiMP的响应图，ToMP的响应更加干净，抑制了周围的干扰。但这样带来一个风险，一旦目标丢失，跟踪如果转移到干扰物被后基本不可能恢复，因为此时真实目标就被当成干扰抑制了。</p><h2 id="方法局限"><a href="#方法局限" class="headerlink" title="方法局限"></a>方法局限</h2><ol><li>attention的计算量</li><li>遮挡和干扰同时出现</li></ol><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602162623.png" alt=""></p><p>最后瞎逼逼一些感想，每年马丁团队的文章我都会抽一个完整的时间来仔细研读，因为总是包含很多优美巧妙的数学推导需要慢慢嚼。但今年的文章意外地很好读，因为偏向模型设计方面了。包括去年的<a href="https://www.zjp97.top/tracking/36-learning-target-candidate-association/?highlight=keep">KeepTrack</a>（一作和本文相同），出发点和实现依然巧妙，但是慢慢地都开始减少数学优化方面的工作了。大家开玩笑常说，端到端优化的深度学习DCF框架只有马丁自己能够继续更新，而他们现在也不继续做优化方面的工作了。那么基于优化的DCF框架在后续会如何发展？这篇文章给出了一个不错的答案。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/55-ToMP/20220602110219.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.11192&quot;&gt;论文&lt;/a&gt; &lt;a href=&quot;https://github.com/visionml/pytracking&quot;&gt;代码&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;马丁团队的新工作，依然是延续dimp的故事，这次是用transformer作为目标预测模型，取代了之前延续好几代的基于优化的模型预测方法。&lt;/p&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="transform" scheme="https://www.zjp97.top/tags/transform/"/>
    
    <category term="CVPR2022" scheme="https://www.zjp97.top/tags/CVPR2022/"/>
    
  </entry>
  
  <entry>
    <title>MixFormer: End-to-End Tracking with Iterative Mixed Attention</title>
    <link href="https://www.zjp97.top/tracking/54-mixformer/"/>
    <id>https://www.zjp97.top/tracking/54-mixformer/</id>
    <published>2022-05-24T09:20:54.000Z</published>
    <updated>2022-06-11T03:11:29.352Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/54-MixFormer/20220524172159.png" alt=""></p><p><a href="https://arxiv.org/abs/2203.11082">论文</a> <a href="https://github.com/MCG-NJU/MixFormer">代码</a></p><p>核心：用transfrom架构整合特征提取和特征融合</p><span id="more"></span><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/54-MixFormer/20220524172748.png" alt=""></p><p>主流的跟踪框架分三步：特征提取、特征融合、预测头分类回归</p><p>其中特征融合是关键，下图展示了不同的融合方法。（摘自 <a href="https://www.bilibili.com/video/BV1L541127MD?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click">【VALSE论文速览-68期】MixFormer:更加简洁的端到端单目标跟踪器</a>）</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/54-MixFormer/20220524172920.png" alt=""></p><p>最近的研究使用transformer进行融合，但仍然依赖CNN提取特征，这其中存在一些局限：</p><ol><li>attention只作用在高层抽象的特征表示空间，忽略了浅层特征；</li><li>CNN对通用对象识别进行预训练，可能会忽略用于跟踪的更精细的结构信息；</li><li>CNN的表征能力是局部的，缺乏长距离建模的能力</li></ol><p>解决方案：</p><p>提出一个通用的transformer结构同时进行特征提取和特征融合。</p><p>具有如下好处:</p><ol><li>使特征提取更具体到相应的跟踪目标，并捕获更多目标特定的判别特征;</li><li>让目标信息更extensive的融合进搜索区域；</li><li>结构更加紧凑简洁。</li></ol><p>主要创新点：</p><ul><li>提出了 MAM 模块，应用 attnetion 机制同时进行特征提取与信息交互</li><li>提出SPM模块进行模板更新</li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/54-MixFormer/20220524213439.png" alt=""></p><p>网络整体框架如图3所示，包括两部分：模板和搜索图像经过基于mixed attention module(MAM)的backbone进行特征提取和融合，再通过预测头输出结果。backbone部分包含3个stage，每个stage输入特征首先经过patch embedding变成一系列token，然后送入MAM模块提取并融合特征。预测头部分直接将融合后的搜索区域的token输入进行预测。</p><h2 id="Mixed-Attention-Module-MAM"><a href="#Mixed-Attention-Module-MAM" class="headerlink" title="Mixed Attention Module (MAM)"></a>Mixed Attention Module (MAM)</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/54-MixFormer/20220524220836.png" alt=""></p><p>本文的核心模块MAM，目的是同时提取并融合模板和搜索图像的特征，因此设计了dual attention分别用于二者。具体来说，MAM输入模板和搜索特征拼接成的 token序列，首先会将输入分开并reshape 成二维的模板和搜索特征，经过$3\times3$ DW卷积编码局部上下文和线性映射生成q，k，v后，同时进行 self-attention和 cross-attention。</p><p>注意MAM 是一个非对称的attention，删去了target-to-search的cross-attention。如图2所示，模板的q只会和模板自己的k，v计算attention（黄色虚线）；而搜索图的q会同时和模板和搜索图的k，v计算attention（蓝色虚线）。用公式表达为：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/54-MixFormer/20220524222647.png" alt=""></p><p>这样做可以使得模板的token在跟踪过程中保持不变，避免被动态的搜索区域影响。为后续引入多个在线模板做铺垫，无需每帧重新计算模板token。</p><h2 id="Localization-Head"><a href="#Localization-Head" class="headerlink" title="Localization Head"></a>Localization Head</h2><p>采用类似stark的角点预测模式。作者也额外尝试了类似detr的采用一个query进行预测的方式。均无需后处理。</p><h2 id="Template-Online-Update"><a href="#Template-Online-Update" class="headerlink" title="Template Online Update"></a>Template Online Update</h2><p>在线更新模板能够很好的利用时序信息处理一些形变和外观变化，然而低质量的更新模板可能使得结果变差。本文设计了一个score prediction module (SPM)，根据预测置信度得分来选择可靠的在线模板，如图4所示。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/54-MixFormer/20220524224125.png" alt=""></p><p>SPM由两个attention和一个三层的MLP组成，该模块接在backbone最后一个stage后，和预测头是并行的。首先输入一个可学习的score token，与search ROI token计算attention，对搜索图中挖掘的目标信息进行编码。然后将score token与第一帧的模板token做attention，隐式地将挖掘的目标与初始目标进行比较。最后过一个MLP预测出置信度得分，小于0.5判断为不可靠。</p><h2 id="Training-and-Inference"><a href="#Training-and-Inference" class="headerlink" title="Training and Inference"></a>Training and Inference</h2><p>作者设计了两种网络架构MixFormer 和 MixFormer-L ，分别基于CVT-21 和 CVT24-W，也就是说可以使用CVT在Imagenet上预训练的权重来初始化backbone（虽然原始的CVT并没有两个输入，计算attention的方式也不一样，但是每个block的参数是一样的）。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/54-MixFormer/20220524225610.png" alt=""></p><p>训练过程分为两步，首先用500个epoch训练backbone和head；最后用40个epoch单独训练SPM，冻结其他部分参数。这个训练流程和stark类似。</p><p>推理阶段每隔200帧更新一次模板，选择区间中得分最高的模板替换先前的模板。本文的框架允许输入任意张数的模板，代码实现中只包含两张模板，一张初始模板，一张在线更新模板。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="SOTA比较"><a href="#SOTA比较" class="headerlink" title="SOTA比较"></a>SOTA比较</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/54-MixFormer/20220524234306.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/54-MixFormer/20220524234318.png" alt=""></p><p>SOTA性能就一个字：恐怖！</p><h2 id="探究实验"><a href="#探究实验" class="headerlink" title="探究实验"></a>探究实验</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/54-MixFormer/20220524234504.png" alt=""></p><ul><li>1 2 3 8 统一特征提取和融合的MAM比先提特征SAM再融合CAM要好，因为耦合的方式可以互相促进。</li><li>4 5 6 7 8 MAM的数量越多越好，因为这样可以获得更extensive 的目标感知特征提取和分层融合。</li><li>8 9 corner head比query head效果更好</li></ul><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/54-MixFormer/20220524235111.png" alt=""></p><ul><li>使用非堆成结构效果略有下降，但是速度提升了</li><li>从固定间隔中随机采样更新模板效果变差了，加上预测得分后才能提升效果</li><li>pretrain的规模越大，对效果也是有提升的。</li></ul><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/54-MixFormer/20220524235732.png" alt=""></p><p>Attention可视化</p><ul><li>背景中的干扰物逐层受到抑制</li><li>在线模板更适应外观变化并有助于区分目标</li><li>多个模板的前景可以通过交叉注意力来增强</li><li>某个位置倾向于与周围的局部块相互作用。</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/54-MixFormer/20220524172159.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.11082&quot;&gt;论文&lt;/a&gt; &lt;a href=&quot;https://github.com/MCG-NJU/MixFormer&quot;&gt;代码&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心：用transfrom架构整合特征提取和特征融合&lt;/p&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="transform" scheme="https://www.zjp97.top/tags/transform/"/>
    
    <category term="CVPR2022" scheme="https://www.zjp97.top/tags/CVPR2022/"/>
    
  </entry>
  
  <entry>
    <title>SBT: Correlation-Aware Deep Tracking</title>
    <link href="https://www.zjp97.top/tracking/53-SBT/"/>
    <id>https://www.zjp97.top/tracking/53-SBT/</id>
    <published>2022-05-24T02:41:22.000Z</published>
    <updated>2022-06-11T03:11:30.078Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524105019.png" alt=""></p><p><a href="https://arxiv.org/abs/2203.01666">论文</a></p><p>写在开头：本文的写作值得学习，实验极其详尽。本篇博文按照作者的写作思路过一遍摘要和引言，对于我们大多数人写文章按照这个套路都没什么问题。</p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>切入点：鲁棒性和判别性都很重要</p><p>现有问题：孪生网络无法判别性的建模目标和干扰</p><p>提出新方法：target-dependent feature network </p><p>做法：通过attention，将跨图像的特征相关性嵌入特征网络的多个层中。</p><p>好处：</p><ol><li>在多个层进行匹配，压制非目标特征，得到实例感知的特征提取；</li><li>输出的搜索特征可以直接用于预测定位，无需互相关操作；</li><li>可以在大量不成对数据上预训练，加速收敛</li></ol><span id="more"></span><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p><strong>指出跟踪中两个竞争的需求：</strong>1. 在剧烈形变下识别目标（鲁棒性）；2. 区分目标和背景干扰物（判别性）</p><p><strong>当前方法：</strong>提取更具有表达力的Siamese特征；设计更鲁棒的相关操作。</p><p><strong>当前方法的问题：</strong>很少注意到两个竞争的需求会使得网络陷入目标-干扰物困境，主要体现在三个方面</p><ol><li><p>Siamese特征编码过程无法感知template和search中的的实例级信息</p></li><li><p>backbone没有明确建模区分两个竞争的决策边界，陷入次优结果</p></li><li><p>每个训练视频只标注一个目标，但是跟踪过程中为包含干扰物的任意目标。这个差距进一步扩大了问题2</p></li></ol><p><strong>本文观点：</strong>特征提取应该具有动态感知实例变化的能力，为跟踪生成适当的embeddings。具体包括：为视频中的同一对象生成连贯的特征，以及对具有相似外观的目标和干扰物生成对比特征</p><p><strong>本文提出方法：</strong></p><p>一个基于注意力的动态特征网络 Single Branch Transformer (SBT) network，允许模板和搜索图像的特征在特征提取的每个阶段进行深度交互。</p><p>如图2所示，传统的孪生网络堆叠卷积，最后接一个互相关层进行特征交互。而SBT堆叠注意力模块Extract-or-Correlation (EoC) blocks，其中EoC-SA融合同一图像的特征，EoC-CA交互模板和搜索图的特征。最后将搜索图像特征直接输出给预测头。</p><p>直观地，交叉注意力<strong>逐层</strong>过滤掉与目标无关的特征，自注意力增强了目标的特征表示。由于特征提取过程是target-dependent且非对称的，因此SBT将目标和干扰物进行区分的同时保持不同目标之间的连贯特征。如图2d所示，属于目标的特征(绿色)与背景(粉色)和干扰物(蓝色)越来越分离，而孪生网络提取的搜索特征是target-unaware的。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524111925.png" alt=""></p><p><strong>关键创新</strong></p><p>为模板和搜索图像对的处理引入一个单一流，通过注意力模块联合特征提取和特征关联。</p><p>SBT本质上还是一个特征提取的backbone，因此可以在ImageNet等非成对数据上进行预训练，在跟踪数据中微调快速收敛。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524112053.png" alt=""></p><p>整体结构如图3所示，包含三个stage。前两个stage通过patch embedding降维，后面接自注意力EoC-SA；第三个satge交替使用自注意力EoC-SA和交叉注意力EoC-CA。该框架没有额外的互相关层，通过在在网络的不同阶段使用交叉注意力实现了信息交互融合。最后将融合后的搜索特征直接输入预测头做分类和回归。</p><h2 id="Patch-Embedding"><a href="#Patch-Embedding" class="headerlink" title="Patch Embedding"></a>Patch Embedding</h2><p>使用$7 \times 7$步长为4的卷积+LN层，降低空间分辨率。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524112939.png" alt=""></p><h2 id="Extract-or-Correlation-Block"><a href="#Extract-or-Correlation-Block" class="headerlink" title="Extract-or-Correlation Block"></a>Extract-or-Correlation Block</h2><p>EoC模块如图3b，c所示，包括两个LN层一个MLP层以及一个注意力，注意力包含自注意力(SA)和交叉注意力(CA).。SA输入的ij是同源的，CA输入的ij分别来自模板z和搜索图x。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524114016.png" alt=""></p><p>作者为了计算高效，尝试了 Vanilla Global attention (VG)；Spatial-Reduction Global attention(SRG)，对k，v降分辨率；Vanilla Local window attention (VL) ，计算局部块的注意力；Shift window vanilla Local attention (SL)，即Swin Transformer。最后使用SRG。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524114738.png" alt=""></p><center>Spatial-Reduction Global attention(SRG)</center><h2 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h2><p>尝试了绝对/相对位置编码，和CVPT中的条件位置编码，即一个$3 \times 3$的depth-wise卷积。</p><h2 id="Direct-Prediction"><a href="#Direct-Prediction" class="headerlink" title="Direct Prediction"></a>Direct Prediction</h2><p>分类和回归分支均由两个MLP层构成，分别在空间维度$\varphi_{sp}$和通道维度$\varphi_{cn}$做MLP，其中RS表示reshape。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524115757.png" alt=""></p><h1 id="SBT设计准则"><a href="#SBT设计准则" class="headerlink" title="SBT设计准则"></a>SBT设计准则</h1><p>作者做了大量实验来探究如何设计网络框架，每张实验图里都包含大量信息，微软有卡任性。这里只总结结论，具体实验可以参考原文。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524120305.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524120323.png" alt=""></p><ol><li>多尺度分层结构的性能比单级结构好 （表1 setting A）</li><li>不同位置编码影响不大 （表1 setting A）</li><li>卷积得到的patch embedding比手工划分的更具表现力 （表1 setting A）</li><li>全局注意力SRG效果和效率综合最佳 （表1 setting A）</li><li>EoC-CA越多一定范围上可以提升性能  （图4d）</li><li>越早使用EoC-CA生成与目标相关的特征越好，但是放在太前面如stage1，2的计算开销较大，而带来的提升较小（图4d，b）</li><li>交替使用EoC-SA和EoC-CA （图4f）</li><li>浅层stage1和2不要设置过多参数和计算量（表1 setting B）</li><li>网络步长一定，3个stage比4个stage好（表1 setting B）</li><li>空间分辨率越大，性能越高，但计算量大（表1 setting B）</li><li>平衡block的数量和channel维度（表1 setting B）</li><li>SBT可以通过预训练中受益，比基于transformer的跟踪器（如transt，stark）收敛得更快（图4e）</li></ol><h1 id="SBT跟踪的理论分析"><a href="#SBT跟踪的理论分析" class="headerlink" title="SBT跟踪的理论分析"></a>SBT跟踪的理论分析</h1><ul><li>SBT克服了孪生跟踪器平移不变性的限制</li></ul><p>padding破坏平移不变性，而对于SBT，padding只存在于PaE中，attention没有引入padding。此外，attention的token本身就具有排列不变性，即目标在图像上移动多少，则其所在的token也移动同样的距离。</p><ul><li>交叉注意力相当于执行了两次互相关</li></ul><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524163539.png" alt=""></p><ul><li>在串行管道中嵌入了分层特征利用</li></ul><p>经典的孪生跟踪器如SiamRPN++是并行的的分层聚合，即对多个特征分别做互相关再叠加。SBT探索了串行的多层次的特征相关性，即将前一层相关（交叉注意力）的输出送给下一次相关的输入。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524164926.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524164541.png" alt=""></p><h1 id="4种SBT网络"><a href="#4种SBT网络" class="headerlink" title="4种SBT网络"></a>4种SBT网络</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524165005.png" alt=""></p><p>Imagenet预训练用4个stage，跟踪微调只用前3个stage。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="Comparison-to-State-of-the-Art-Trackers"><a href="#Comparison-to-State-of-the-Art-Trackers" class="headerlink" title="Comparison to State-of-the-Art Trackers"></a>Comparison to State-of-the-Art Trackers</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524165305.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524165320.png" alt=""></p><p>作者将本文提出的Correlation-Aware backbone 用在几个主流的跟踪器上，得到SiamFCpp-CA. DiMP-CA.STARK-CA. STM-CA，观察性能提升巨大，甚至部分计算量还下降了。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524165514.png" alt=""></p><h2 id="Exploration-Study"><a href="#Exploration-Study" class="headerlink" title="Exploration Study"></a>Exploration Study</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524165758.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524170547.png" alt=""></p><p><strong>Correlation-embedded structure</strong> 将特征交互嵌入特征提取中的效果比单独在最后使用互相关效果要好。并且本文使用的交叉注意力的交互形式比DCF和DW-Corr要好。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524170333.png" alt=""></p><p><strong>Target-dependent feature embedding</strong> (a) Correlation-embedded tracker. (b) Siamese correlation with SBT. (c) Siamese correlation with ResNet-50 </p><p>本文提出的方法最具有 target-dependent的性质，即目标和干扰分的最开。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524170608.png" alt=""></p><p><strong>Benefits from pre-training</strong> 本文的方法可以预训练，提升性能加速收敛。</p><p>补充材料里还有一些关于计算量和结构设计方面的实验，就不一一介绍了。作者团队是真的有耐心做了这么多对比实验。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524171043.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524171151.png" alt=""></p><p>最后提下本文方法的局限性，就是对严重遮挡或者出视野不友好，这也是所有孪生跟踪器的通病了。虽然attention的逐点匹配一定程度上可以克服遮挡，但是对于图13中这些有相似属性的遮挡物还是存在很多误判。另一个局限在于本文使用了很多快速注意力，这些并不能直接通过直接调包实现。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524171328.png" alt=""></p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>本文首次提出了一种target-dependent的跟踪特征网络。SBT大大简化了跟踪管道，比最近基于Transformer的跟踪器收敛更快。从实验和理论两方面对SBT跟踪进行了系统的研究。此外，实现了四个版本的SBT网络，可以改进其他流行的VOT和VOS跟踪器。大量实验表明，该方法取得了良好的效果，可以作为动态特征网络应用于其他跟踪管道。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/53-SBT/20220524105019.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.01666&quot;&gt;论文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;写在开头：本文的写作值得学习，实验极其详尽。本篇博文按照作者的写作思路过一遍摘要和引言，对于我们大多数人写文章按照这个套路都没什么问题。&lt;/p&gt;
&lt;h1 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h1&gt;&lt;p&gt;切入点：鲁棒性和判别性都很重要&lt;/p&gt;
&lt;p&gt;现有问题：孪生网络无法判别性的建模目标和干扰&lt;/p&gt;
&lt;p&gt;提出新方法：target-dependent feature network &lt;/p&gt;
&lt;p&gt;做法：通过attention，将跨图像的特征相关性嵌入特征网络的多个层中。&lt;/p&gt;
&lt;p&gt;好处：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在多个层进行匹配，压制非目标特征，得到实例感知的特征提取；&lt;/li&gt;
&lt;li&gt;输出的搜索特征可以直接用于预测定位，无需互相关操作；&lt;/li&gt;
&lt;li&gt;可以在大量不成对数据上预训练，加速收敛&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="transform" scheme="https://www.zjp97.top/tags/transform/"/>
    
    <category term="CVPR2022" scheme="https://www.zjp97.top/tags/CVPR2022/"/>
    
  </entry>
  
  <entry>
    <title>相关滤波和孪生网络目标跟踪综述（Martin团队）</title>
    <link href="https://www.zjp97.top/tracking/52-survey/"/>
    <id>https://www.zjp97.top/tracking/52-survey/</id>
    <published>2021-12-22T03:10:16.000Z</published>
    <updated>2022-06-11T03:00:51.623Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211222231118.png" alt=""></p><p>Visual Object Tracking with Discriminative Filters and Siamese Networks: A Survey and Outlook 马丁团队的新作，强烈推荐。</p><p><a href="https://arxiv.org/abs/2112.02838">论文</a></p><p>精确和鲁棒的视觉目标跟踪是计算机视觉中最具挑战性和最基本的问题之一。它需要在只给定目标初始状态的条件下，准确估计图像序列中目标的轨迹及状态。相关滤波和孪生网络已经成为当下的主流跟踪算法，本文选取了90多个DCF和Siamese跟踪器进行系统和全面的回顾。首先，介绍了DCF和Siamese跟踪核心公式的背景理论。然后，区分和全面回顾了这两种跟踪范式中共享的和各自特定的挑战。此外，深入分析了DCF和Siamese跟踪器在9个benchmark上的性能，涵盖了视觉跟踪的不同方面的实验：数据集、评估指标、性能和速度比较。在此分析的基础上，提出了对视觉跟踪开放挑战的建议。</p><span id="more"></span><h1 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h1><p>这项工作系统的回顾流行的DCF和Siamese的跟踪范式。两种模型都有一个共同的目标，即学习一个准确的目标外观模型，能够有效地从背景中区分出目标物体。尽管在解决上述目的方面出现了不同的潜在范式，但深度学习的出现给这两种范式带来了一些重要的相似之处和共同的挑战。</p><p>相同的挑战包括：</p><ol><li><strong>特征表达</strong> 从预训练网络中提取深度特征表示是两种范式共同的趋势。然而，深度网络架构和特征层次结构的选择仍然是一个开放的问题；</li><li><strong>目标状态估计</strong> 两种范式的核心公式只解决了如何估计目标对象的平移，但都没有提供一个显式的方法来估计完整的目标状态（边界框参数）；</li><li><strong>离线训练</strong> 最初只有Siamese跟踪器可以端到端离线训练，但最近的DCF也可以利用大规模离线学习，将其与高效、可微的在线学习模块集成，以实现鲁棒和准确的跟踪。</li></ol><p>各自特定的问题包括：</p><ol><li><strong>边界效应</strong> DCF通常利用训练样本的周期性循环位移来学习在线分类器，这引入了不良的边界效应，严重降低了目标模型的质量；</li><li><strong>优化问题</strong> DCF的损失函数优化是一个挑战，特别是在岭回归中加入了空间或时间正则化等目标约束条件时变得更加困难；</li><li><strong>模型在线自适应</strong> 当目标外观发生变化时，模型需要能够应对这些变化。DCF可以通过损失函数更新外观模型，但Siamese跟踪器并没有固有的在线模型更新机制。因此，在线适应性是Siamese跟踪器的一个重要问题。</li></ol><p>下面将分别介绍这两种跟踪范式的背景理论以及回顾它们针对上述挑战做出的相关工作。图4整体展现了两种跟踪范式的发展过程。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211225112755.png" alt=""></p><h1 id="DCF"><a href="#DCF" class="headerlink" title="DCF"></a>DCF</h1><p>DCF是一种有监督的线性回归技术。DCF成功的关键是通过循环移动训练样本实现稠密采样，这允许在学习和应用相关滤波器时使用快速傅里叶变换(FFT)，大幅提升计算效率。通过利用傅里叶变换的特性，DCF在线学习相关滤波器，有效地最小化岭回归误差来定位连续帧中的目标对象。为了估计下一帧的目标位置，将学习的滤波器应用到感兴趣的区域，其中最大响应的位置估计目标位置，然后以迭代的方式更新滤波器。</p><h2 id="DCF基本公式"><a href="#DCF基本公式" class="headerlink" title="DCF基本公式"></a>DCF基本公式</h2><p>早期的DCF如MOOSE和CSK均采用单通道的灰度特征，从KCF之后基本都是应用多通道特征，所以这里直接列出多通道的形式，关于单通道的详细推导可以查看原论文。多通道DCF优化目标如下：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211223103910.png" alt=""></p><p>其中$d \in \{1,…,D\}$表示特征维度，$m$是样本个数，$x, y$分别对应训练样本和标签，$\omega$表示是滤波器参数。通过利用FFT转到频域求解：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211223104338.png" alt=""></p><p>详细推导可以参考<a href="https://blog.csdn.net/u011285477/article/details/53861850">KCF论文阅读笔记_图像研究猿的专栏</a></p><h2 id="DCF跟踪流程"><a href="#DCF跟踪流程" class="headerlink" title="DCF跟踪流程"></a>DCF跟踪流程</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211223104623.png" alt=""></p><p>DCF跟踪流程如图2所示，首先在第一帧学习滤波器$\omega$，然后在后续帧进行检测，并更新滤波器参数。第m帧的检测公式计算如下：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211223105745.png" alt=""></p><p>其中$z$为根据上一帧的检测结果裁剪的patch，$\omega_{m-1}$为从初始帧一直递归更新到上一帧的滤波器，二者通过卷积运算得到每个位置的目标分数$s$。其中$s$中响应最大的位置为当前检测的目标位置，并在此位置裁剪新的patch用于更新滤波器参数。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211223114556.png" alt=""></p><p>其中$\omega_m^{num}, \omega_m^{den}$分别为分子和分母，即滤波器$\hat{\omega}=\frac{\omega_m^{num}}{\omega_m^{den}}$。</p><h2 id="DCF存在的挑战"><a href="#DCF存在的挑战" class="headerlink" title="DCF存在的挑战"></a>DCF存在的挑战</h2><p>DCF框架中存在的问题包括特征表达、边界效应、损失优化和目标状态估计，下面分别进行讨论。</p><h3 id="特征表达"><a href="#特征表达" class="headerlink" title="特征表达"></a>特征表达</h3><p><strong>Handcrafted  Features</strong> 手工特征包括：灰度/RGB/LAB等颜色强度特征，Color  Names  (CN)  特征以及HOG特征。</p><p><em>由于其速度和有效性，这些特性已经成为手工方法中的首选。此外，HOG特征也被有效地与CN特征结合起来，以利用形状和颜色信息。</em></p><p><strong>Deep Features</strong> 随着CNN的发展，许多DCF方法将高维非线性的卷积特征用于跟踪，如HCF、HDT、CCOT、ECO、ASRCF和RPCF等。它们均使用在ImageNet上离线预训练的特征，虽然是用于分类任务的特征，但这种深度表示方法适用于广泛的视觉任务。</p><p>一些流行的预训练深度网络，如VGG-19、imagenet-vgg-m-2048、VGG-16、ResNet50和googlenet被用来提取深度特征表示。其中浅层特征包含高分辨率的低层信息，这对精确定位目标非常重要。更深层的特征对复杂的形变具有较高的不变性，可以提高跟踪鲁棒性，同时在很大程度上不受小的平移和尺度变化的影响。因此，在DCF框架中融合浅卷积层和深卷积层的精确策略一直是人们感兴趣的话题。CCOT中提出了DCF框架的连续域公式，实现了多分辨率特征的融合。ECO研究降低CCOT计算成本的策略，并降低过拟合的风险。其他跟踪器如HDT、HCFTs、MCCT、MCPF、MCPF、LMCF、STRCF、TRACA、DRT、UPDT和GFS-DCF使用后期融合策略集成深度特征。该策略是在每个单独的特征表示上训练一个分类器，然后聚合特征响应图。</p><p><strong>End-to-End Features Learning</strong> 上述方法依赖离线预训练，而后续研究关注如何在跟踪数据集上端到端优化DCF框架，这样可以学到任务特定的深度特征表示来改善跟踪性能。CFNet以离线方式对相关滤波器进行端到端学习，CREST和ACFN也采用了相同的在线策略。最近，ATOM额外加入了端到端的尺度估计分支，DiMP和PrDiMP则改进了经典的DCF模型，提升了判别能力。</p><p><em>最近DCF跟踪器（ATOM, DiMP, PrDiMP）中端到端特征学习的趋势导致了在多个基准上的优秀跟踪性能，为探索DCF范式中更复杂的端到端特征学习铺平了道路。</em></p><h3 id="边界效应"><a href="#边界效应" class="headerlink" title="边界效应"></a>边界效应</h3><p>DCF的循环位移操作引入了对跟踪不利的边界效应。具体来说，由于循环位移，训练样本中的负样本并不是真实的背景内容，而是一个较小图像块的不断位移合成的重复。因此，模型在训练过程中看到的背景样本较少，严重限制了其判别能力。此外，由于周期性重复所造成的失真，预测的目标分数只在图像块的中心附近是准确的，搜索区域的大小因此受到限制。尽管可以通过窗口函数相乘来对其进行预处理。然而，这种技术并不试图解决上述问题，而只是为了消除边界区域的不连续性。针对这一问题，许多方法在DCF的目标公式中加入了各种目标特定的空间、时空和平滑约束。</p><p><strong>空间正则化</strong> SRDCF提出了一种空间正则化来控制滤波器的空间扩展，以缓解边界问题，公式如下：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211224163041.png" alt=""></p><p>其中空间权重函数$f(n) &gt; 0$，在背景像素处取大值，在目标区域内取小值，对背景滤波系数进行惩罚。这样可以在一个更大的搜索图像上学到一个聚焦于目标区域的紧凑型滤波器。空间正则化策略已被应用于各种跟踪器中，包括ARCF、ASRCF和AutoTrack。为了提高SRDCF的效率，Li等人提出了STRCF，它只使用单一的训练样本，而引入了时间正则化项来整合历史信息。</p><p><strong>约束优化</strong> SRDCF的目的是惩罚目标区域外的滤波系数，而Kiani等人提出引入硬约束（BACF）。该策略强制滤波系数$\omega$在目标区域外为零。优化公式可以通过引入二进制掩码P来表示：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211224165451.png" alt=""></p><p>其中$P \cdot \omega$ 掩盖了背景特征对于滤波器参数的影响，目标函数采用ADMM迭代优化。公式(13)和(14)中所研究的特定于目标的约束通常针对不同的对象是固定的，并且在跟踪过程中不会发生变化。最近，Dai等人通过引入自适应正则化项来扩展BACF和SRDCF。</p><p><strong>隐式方法</strong> GFS-DCF提出了一种联合特征选择模型，该模型同时学习三个正则化项:空间正则化用于特征选择，通道正则化用于特征通道选择，低秩时间正则化项用于增强滤波权值的平滑性。Mueller等人提出CACF对每个目标patch的上下文信息进行正则化。在每一帧中，CACF对几个上下文补丁进行采样，作为负样本。</p><p><strong>空域形式</strong> 最近，ATOM和DiMP采用低分辨率（16倍降采样）的深度特征，可以以小尺寸卷积核（$4 \times 4$）的形式在空间域中直接学习滤波器。这种方法完全绕开了边界效应，因为不需要周期性地扩展训练样本。</p><p><em>基于正则化的(SRDCF， STRCF)和基于约束的(CFLB/BACF)方法都取得了巨大成功，并被广泛应用于跟踪器中。最近的深度学习方法(ATOM/DiMP)通过直接在空域中优化滤波器，已经完全避开了边界效应问题。因此，虽然傅里叶域对高分辨率特征的计算具有吸引力，但在使用强大的低分辨率深度特征时，高效的空域优化方法在在线学习中占优。因此，目前基于DCF的SOTA方法研究采用了空域优化的形式，不需要额外的策略来缓解边界效应。</em></p><h3 id="优化问题"><a href="#优化问题" class="headerlink" title="优化问题"></a>优化问题</h3><p>标准的DCF优化利用循环矩阵在傅里叶域可对角化的性质来求解岭回归的闭式解。这套方法无法处理有额外正则项或约束条件的优化问题，下面总结几种有效的模型优化方法。</p><p><strong>Gauss-Seidel Method</strong> 代表方法：DeepSRDCF，采用Gauss-Seidel迭代速度非常慢，大概只有每秒几帧。</p><p><strong>Conjugate Gradient Based Method</strong> 共轭梯度最早用于CCOT，可作用于任意一组满秩的正规方程$A\tilde{\omega}=b$。它通过寻找共轭方向$p^{i}$和最优步长$\beta^i$ 来更新滤波器$\tilde{\omega}^i=\tilde{\omega}^{i-1}+\beta^ip^{i}$。该算法在有限次迭代次数中收敛到解，但在实际应用中，算法在固定次数的迭代后或当误差降低到令人满意的水平时停止。CG在处理D维特征时，复杂度从Gauss-Seidel的二次复杂度$O(D^2)$降为线性的$O(D)$，因此可以用于高维的深度特征优化。</p><p>针对非线性最小二乘问题，Gauss-Newton法也被用于许多跟踪器，包括ECO、ATOM和UPDT。该方法对误差使用泰勒级数展开，找到一个二次逼近的目标。由此产生的二次问题可以用迭代方法处理，例如上面描述的CG方法。在ECO和ATOM中，采用Gauss-Newton结合CG对滤波器$\omega$和降维矩阵进行联合优化。DiMP使用Gauss-Newton和Steepest Descent迭代来学习滤波器。优化步骤本身是可微分的，这进一步支持端到端的学习。PrDiMP进一步使用更一般的牛顿近似来处理凸且非线性KL散度目标函数。</p><p><strong>Alternating Direction Method of Multipliers (ADMM) Method</strong> ADMM方法近年来被广泛应用于DCF中，特别是引入额外正则项的优化上。ADMM将大的全局问题分解为多个较小、较容易求解的局部子问题，并通过协调子问题的解而得到大的全局问题的解。基于ADMM的优化方法为每个子问题提供了闭式解，并且在非常少的迭代内收敛。BACF、DRT、AutoTrack、ARCF和RPCF等跟踪器都采用ADMM来实现高效的求解。</p><h3 id="目标状态估计"><a href="#目标状态估计" class="headerlink" title="目标状态估计"></a>目标状态估计</h3><p><strong>Multiple Resolution Scale Search Method</strong> 多尺度金字塔搜索是最简单粗暴的方法，首先按不同的比例因子调整图像大小，然后在每个尺度进行估计，选择分数最大的位置和尺度作为最终结果。在一定程度上可以提升尺度估计的准确性，但是多尺度采样带来较大的计算负担，且只能等比例缩放边界框。</p><p><strong>Discriminative Scale Space Search Method</strong> 即DSST为代表的方法，将目标状态估计分两步进行。由于两帧之间的尺度变化通常较小或中等，首先通过在当前的尺度估计上应用通常的平移滤波器来找到目标平移。然后，在尺度维度上应用单独的一维滤波器来更新目标尺寸。好处有两方面：1）通过减小搜索空间来提高计算效率；2）对尺度滤波器进行训练，区分不同尺度下目标的外观，从而得到更准确的估计。后续的fDSST跟踪器通过应用PCA和子网格插值[27]降低了DSST的计算开销。</p><p><strong>Deep Bounding Box Regression Method</strong> 上述方法依赖于比例因子参数和在线精确相关滤波器响应，没有以离线方式利用强大的深度特征表示。此外，这些在线方法不执行任何边界框回归。因此，这些方法在尺度突然变化的情况下表现出性能下降。精确估计目标边框是一项复杂的任务，需要高层次的先验知识，不能被建模为一个简单的图像变换(例如统一的图像缩放)。</p><p>边界框回归在目标检测中有广泛的应用，ATOM就借鉴了检测中的IOUNet来进行状态估计。考虑到检测网络具有class-specific的特性，不适合target-specific的跟踪任务，Martin设计了一个调制网络嵌入参考帧中的目标外观信息从而得到target-specific的IOU预测。在跟踪过程中，通过简单地最大化每帧的预测IOU来找到目标框。结果表明，与传统的多尺度搜索方法相比，该方法的性能有了显著提高。后续的DiMP, PrDiMP和KYS都采用了这种策略，其中PrDiMP使用基于能量的模型来预测边界框的非归一化概率密度，而不是预测IOU，通过最小化KL散度和高斯标签来训练的。</p><h2 id="基于分割的DCF"><a href="#基于分割的DCF" class="headerlink" title="基于分割的DCF"></a>基于分割的DCF</h2><p>目标分割为跟踪提供了可靠的目标观测，解决了旋转目标框、遮挡、变形、缩放等跟踪问题。Bertinetto等人使用了一种基于颜色直方图的分割方法来改进在不同光照变化、运动模糊和目标变形下的跟踪。Lukezic等人提出了一种使用基于颜色的分割方法来正则化滤波学习的空间可靠性图。Kart等人将CSR-DCF跟踪器扩展到基于颜色和深度分割的RGB-D跟踪，深度线索提供了更可靠的分割图。Lukezic等人提出了一种single shot分割跟踪器来解决联合框架内的VOT和VOS问题，采用两种判别模型进行编码，用于联合跟踪和分割任务。最近，Robinson等人利用ATOM的快速优化方案，将一种功能强大的判别模型用于视频对象分割任务。Bhat等人也使用了目标模型区分能力来实现更鲁棒的视频对象分割。</p><h1 id="SIAMESE-TRACKERS"><a href="#SIAMESE-TRACKERS" class="headerlink" title="SIAMESE  TRACKERS"></a>SIAMESE  TRACKERS</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211225121835.png" alt=""></p><p>深度学习模型成功的关键是特征在大量数据上的离线学习能力，能够从大量标注的数据中学习复杂而丰富的关系。孪生网络将目标跟踪看成一个相似性学习问题，通过端到端的离线训练来学习目标图像和搜索区域之间的相似性。Siamese跟踪器由模板分支和检测分支组成，模板分支输入初始目标图像块，检测分支输入当前帧图像块。这两个分支共享CNN参数，使得两个图像块编码了适合跟踪的相同变换。Siamese跟踪框架如图3所示，其主要目标是克服预训练CNN的局限性，充分利用端到端学习。离线训练视频用于指导跟踪器处理旋转、视点变化、光照变化和其他复杂的挑战。Siamese跟踪器能够学习物体运动和外观之间的一般关系，并可以用来定位训练中未见过的目标。</p><p><strong>Training Pipeline</strong> 以SiamFC为例，输入一对训练图像(x,  z)，x表示感兴趣的对象（第一帧裁剪的图像块）和z表示后续帧的搜索图像区域。将这些图像对输入CNN中，以获得两个特征图，然后使用互相关进行匹配，</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211226102603.png" alt=""></p><p>其中星号表示互相关，$f_{\rho}(.)$表示CNN，如AlexNet，模型参数为$\rho$。$g_{\rho}(x,z)$表示x和z之间相似性的响应映射，$b$是常数标量。</p><p>训练目标是使得响应图$g_{\rho}(x,z)$的最大值与目标位置相对应，因此采用logistic loss：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211226103514.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211226103550.png" alt=""></p><p>其中$v$是预测值，$c \in \{-1,1\}$是标签。</p><p>同期的其他一些工作，如：SINT使用欧式距离作为相似度量，而不是互相关；GOTURN预测的是边框回归的结果；CFNET将相关滤波器作为匹配函数中的一个独立模块加入到x中，使该网络更浅更高效。</p><p><strong>Testing Pipeline</strong> 测试过程就是利用学到的匹配函数度量x在z上每个匹配区域的相似度，用得分最高的位置预测目标的新位置。最初的SiamFC只是将每一帧与目标的初始外观进行比较，并在GPU上以140FPS的速度实时跟踪。Siamese跟踪器在推理和离线学习中都具有极高的计算效率，并且跟踪性能卓越，因此在跟踪社区中受到了很多关注。</p><h2 id="Siamese跟踪存在的挑战"><a href="#Siamese跟踪存在的挑战" class="headerlink" title="Siamese跟踪存在的挑战"></a>Siamese跟踪存在的挑战</h2><p>经典的SN在准确性和效率上都优于DCF跟踪器。然而，SN在backbone提取网络、离线训练时需要大量的标注图像对、缺乏在线适应性、损失函数、目标状态估计等方面也存在一定的局限性。下面将讨论这些问题并概述近年来发展的解决方案。</p><h3 id="Backbone结构"><a href="#Backbone结构" class="headerlink" title="Backbone结构"></a>Backbone结构</h3><p>早期的Siamese跟踪器均使用AlexNet作为特征提取，包括SiamFC, GOTURN, SINT, FlowTrack, MemTrack, EAST和SiamRPN。然而，这些跟踪器在性能上仍然有限，因为AlexNet是一个相对较浅的网络，并没有产生非常强的特征表示。直接替换更深的网络并不能取得性能上的提升。</p><p>为了解决这一问题，SiamRPN++研究发现，在SNs中，网络中的padding使得学习到的特征表示不满足空间平移不变性约束。因此，提出了一种有效的采样技术来满足这种空间不变性约束。利用强大的深层ResNet架构，许多Siamese跟踪器的性能得到了改善。Zhang等人也研究了同样的问题，并提出了SiamDW，其中浅骨干AlexNet被包括Inception、VGG-19和ResNet在内的深网络所取代。研究发现，除了padding外，感受野和网络步长也是深层网络不能直接替代浅层网络的主要原因。有了这些基础，最近的跟踪器包括SiamCAR、Ocean和SiamBAN等都使用了更深的网络结构。</p><p><em>由于ResNet的简单性和强大的性能，ResNet已经成为Siamese跟踪的首选方案。然而，Transformer的最新进展预计将在未来几年对跟踪社区产生重大影响。</em></p><h3 id="离线训练"><a href="#离线训练" class="headerlink" title="离线训练"></a>离线训练</h3><p>当前流行的跟踪训练数据集包括：ImageNet ILSVRC2014、ILSVRC2015、COCO、YouTube-BB、YouTube-VOS、LaSOT、GOT-10k和TrackingNet。这些数据集充分覆盖了大量的语义，并且不关注特定的对象，否则在Siamese训练中，调优后的网络参数会过拟合到特定的对象类别。</p><p>与DCF范式不同，标准的Siamese范式不能利用跟踪过程中的已知干扰物。因此，当与目标本身相似的物体出现时，Siamese跟踪器通常不能很好地处理。早期的方法只在训练过程中对同一视频中的训练图像进行采样，这种抽样策略不关注具有语义相似干扰物的情况。为了解决这个问题，Zhu等人在DaSiamRPN中引入了难负样本挖掘技术，通过在训练过程中引入更多的语义负样本对来克服数据不平衡。构建的负样本对由相同和不同类别的标记目标组成。该技术通过更多地关注细粒度表示，帮助DaSiamRPN克服漂移。Voigtlaender (SiamRCNN)等人利用嵌入网络和最近邻近似提出了另一种难负样本挖掘技术。对于每个真实目标框，使用预训练网络为相似的目标外观提取嵌入向量。 然后使用索引结构估计近似最近邻，并使用它们估计嵌入空间中目标对象的最近邻。</p><p><em>近年来，利用更多的训练数据和设计数据挖掘技术的趋势在多个基准上显示出了良好的跟踪性能。</em></p><h3 id="在线模型更新"><a href="#在线模型更新" class="headerlink" title="在线模型更新"></a>在线模型更新</h3><p>在SiamFC中，目标模板在第一帧中初始化，然后在视频的剩余部分中保持固定。跟踪器不进行任何模型更新，因此其性能完全依赖于SN的泛化能力。然而当外观变化很大时，不更新模型往往导致跟踪失败。下面将介绍模型更新方向的潜在解决方案。</p><p><strong>Moving Average Update Method</strong> 最简单的线性更新策略，使用固定学习率的对模板滑动平均更新。虽然它提供了一种集成新信息的简单方法，但由于恒定的更新速率和简单的线性模板组合，导致跟踪器无法从漂移中恢复。</p><p><strong>Learning Dynamic SN Method</strong> DSiam设计了两个动态变换矩阵，包括目标外观变化和背景抑制。这两个矩阵都在傅里叶域中用闭式解进行求解。DSiam提供了有效的在线学习，但它忽略了历史目标变化，这对于更平滑地适应模板非常重要。 </p><p><strong>Dynamic Memory Network Method</strong> MemTrack可以动态写入和读取以前的模板，以应对目标外观的变化。使用LSTM作为存储器控制器，输入是搜索特征图，输出存储器读写过程的控制信号。这种方法使跟踪器能够记忆长期目标外观。然而，该算法只关注目标特征，忽略了背景杂波中的鉴别信息，在目标变化剧烈的情况下，会导致精度下降。 </p><p><strong>Gradient-Guided Method</strong> Li等人提出了GradNet，对梯度信息进行编码，通过前向和后向操作更新目标模板。跟踪器利用梯度信息更新当前帧中的模板，然后加入自适应过程，简化基于梯度的优化过程。与上述方法不同的是，该方法充分利用了反向传播梯度中的判别信息，而不是仅仅集成之前的模板。这样可以提高算法性能，但代价是以反向传播的方式计算梯度引入了计算负担。</p><p><strong>UpdateNet Method</strong> UpdateNet利用一个CNN整合了初始模板、历史累计模板、以及当前帧模板，进行自适应更新。基于现有模板与累积模板的差异，可以适应现有框架的具体更新要求。此外，在每帧中还考虑了初始模板，提供了高度可靠的信息，增加了对模型漂移的鲁棒性。结果表明，与SiamFC和DaSiamRPN相比，该方法具有优异的性能。</p><p><em>虽然已经提出了许多模型更新的技术，但简单地不使用更新仍然是一种鲁棒且流行的选择。在这个方向上的进一步研究需要开发简单的、通用的、端到端可训练的技术，从而进一步提高Siamese跟踪的鲁棒性。</em></p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>损失函数主要包括分类和回归两种任务。</p><p><strong>Logistic Loss</strong> 如上述公式16-17所示，早期方法均采用逻辑损失，包括SiamFC, DSiam, RASNET, SA-SIAM, CFNET, SiamDW和GradNet。该训练方法利用图像对上的成对关系，在正样本对上相似性分数最大化，在负样本对上相似性分数最小化。</p><p><strong>Contrastive Loss</strong> </p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211227102746.png" alt=""></p><p>其中$\epsilon$为距离阈值，$D$代表两个样本特征的欧氏距离，$y_{x z} \in \{0,1\}$表示$x$和$z$是否属于同一个目标。SINT采用了contrastive loss，而GOTURN采用了预测和真实目标框之间的L1 loss。</p><p><strong>Triplet Loss</strong> 上面的损失只利用了图像之间的两两关系，忽略了正样本对和负样本对之间的结构联系。Yan等人提出了SPLT跟踪器，在训练过程中使用Triplet Loss，定义如下：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211227103412.png" alt=""></p><p>其中$x^p$是正例，$x^n$是负例。Triplet Loss可以进一步挖掘目标、正实例和负实例之间的潜在关系，而且包含了更鲁棒的相似结构。SiamFC-Tri，SiamRCNN都利用了该损失函数。</p><p><strong>Cross Entropy Loss</strong> SNs中的分类分支借鉴了目标检测中常用的交叉熵损失，定义为：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211227110305.png" alt=""></p><p>其中$p_o$是预测值，$p_o^*$是标签。SiamRPN最早将检测思想引入SNs并使用了交叉熵损失。后续在其基础上，SiamRPN++, SiamAttn, Ocean, CLNET, SPM, C-RPN均使用交叉熵损失用于训练分类分支。</p><p><strong>Regression Loss</strong> 回归损失使用较多的为smooth L1 loss (SiamRPN, SiamRPN++, SiamAttn, CLNET, SPM, C-RPN)和IOU loss (SiamBAN, Ocean, SiamFC++)。</p><p><strong>Multi-Task Loss</strong> 对于分类分支和回归分支的联合训练，需要多任务损失。如交叉熵+smooth L1 loss或交叉熵+IOU loss。</p><p><strong>Regularized Linear Regression</strong> 为了将相关滤波作为一个单独的层加入SNs，需要使用公式9-10的线性回归损失。将岭回归问题通过闭式解的形式解决，并以端到端方式嵌入整个训练框架，代表方法有：CFNET, TADT, RTINET, DSiam, FlowTrack, UDT, UDT++。</p><p><em>目前，关于损失函数的研究还没有普遍的共识。相反，最近的SOTA方法采用了不同的替代方法。在上述方法中，交叉熵损失仍然是最近的跟踪器一个普遍的选择</em></p><h3 id="目标状态估计-1"><a href="#目标状态估计-1" class="headerlink" title="目标状态估计"></a>目标状态估计</h3><p>与DCF类似，SNs也面临尺度变化的挑战。相似函数只能学习图像之间深层结构关系，没有考虑尺度变化问题。下面将讨论这一方面的发展。</p><p><strong>Multiple Resolution Scale Search Method</strong> 最早期的方法仍然是简单粗暴的多尺度搜索，如如RASNET、SA-Siam、StructSiam、UDT、UDT++、TADT、GradeNet、RTINET、FlowTrack。</p><p><strong>Deep Anchor-based Bounding Box Regression Method</strong> 基于锚框的边框回归利用目标检测中的RPN网络来预测具有多种尺度和宽高比的proposal。RPN是一个全卷积网络，它同时预测每个位置的分类分数和边框回归。RPN以端到端方式进行训练，以生成高质量的proposal。最早运用RPN的跟踪器是SiamRPN，包括一个分类分支和回归分支，相比传统方法取得了显著的进步。之后的DaSiamRPN， SiamRPN++， SiamDW ， SPLT， C-RPN， SiamAttn， CSA， SPM等也建立在相同的概念上。</p><p><strong>Deep Anchor-free Bounding Box Regression Method</strong> 上述基于锚框的方法需要启发式知识精心设计锚箱，引入了许多超参数和计算复杂度。因此目标检测中提出了无锚框的方法，避免了设计与锚框相关的超参数，更加灵活通用。无锚框方法分为keypoint-based和center-based两种，前者首先定位预定义的关键点，然后在目标上执行边框回归；后者则预测目标中心正样本区域到边界的四个距离。无锚检测器能够消除与锚框相关的超参数，性能与基于锚框的检测器相似，具有更强的泛化能力。典型的无锚框孪生跟踪算法包括SiamBAN, Ocean, SiamCAR等。</p><p><em>目标检测技术在目标状态估计方面取得了显著的进展。最近使用RPN和无锚框回归结构的趋势表明，可以在端到端范例中进一步探索这些技术</em></p><h2 id="基于分割的Siamese跟踪"><a href="#基于分割的Siamese跟踪" class="headerlink" title="基于分割的Siamese跟踪"></a>基于分割的Siamese跟踪</h2><p>Siamese跟踪也可以和分割结合起来，处理一些形变的问题（比如手掌张开的人、旋转或轴对齐的目标框）。Wang等人提出SiamMask，可以同时估计二值蒙版、边界框和相应的背景-前景得分。该网络能够联合处理视觉跟踪和目标分割，以提高鲁棒性。Lu等人采用无监督视频对象分割任务，该任务基于SN内的联合注意机制提出了一种新颖的体系结构。</p><h1 id="EXPERIMENTAL-COMPARISON"><a href="#EXPERIMENTAL-COMPARISON" class="headerlink" title="EXPERIMENTAL COMPARISON"></a>EXPERIMENTAL COMPARISON</h1><p>实验比较了59个DCF和33个Siamese trackers在9个跟踪数据集上的性能，包括OTB100, TC128, UAV, VOT2014, VOT2016, VOT2018, TrackingNet, LaSOT和GOT-10k。图5显示了来自不同跟踪数据集的示例帧。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211227153554.png" alt=""></p><h2 id="跟踪数据集"><a href="#跟踪数据集" class="headerlink" title="跟踪数据集"></a>跟踪数据集</h2><p>为了提供一个标准和公平的目标跟踪器性能评估，许多benchmarks被提出。除了短期跟踪外，一些最近的数据集提供了短期和长期跟踪序列。公开的基准数据集包含各种跟踪挑战，包括尺度变化(SV)，出视野(OV)，形变(DEF)，低分辨率(LR)，光照变化(IV)，平面外旋转(OPR)，遮挡(OCC)，背景杂波(BC)，快速运动(FM)，平面内旋转(IPR)，运动模糊(MB)、部分遮挡(POC)、摄像机突然运动(CM)、长宽比变化(ARC)、全遮挡(FOC)、视点变化(VC)、相似物体(SOB)、物体颜色变化(OCC)、绝对运动(SOB)、目标旋转(ROT)、场景复杂度(SCO)、快速摄像机运动(FCM)、低分辨率物体(LRO)、运动变化(MOC)。表1给出了我们在实验比较中使用的每个数据集的描述。接下来，简要描述每个跟踪数据集。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211227154029.png" alt=""></p><p><strong>OTB100</strong> 22个对象类别的100个视频组成，与OTB50相同的11个跟踪属性。OTB100数据集的平均分辨率为356 × 530，视频长度在71 - 3872帧之间。</p><p><strong>TC128</strong> TC128用于评价颜色对视觉跟踪的影响。它包含27个对象类别的128个完整标注的彩色视频序列。在128个序列中，有78个序列不同于OTB100，而剩下的50个序列在两个数据集中是共同的。TC128也包含11个跟踪属性，类似于OTB100(表1)，平均分辨率为461  × 737，最小帧数71帧，最大帧数3872帧。</p><p><strong>UAV123</strong> UAV是低空无人机捕获的真实和合成高清视频序列，分为两个子集，UAV123和UAV20L。UAV123包含9种对象类别的123个短序列，最小帧数109帧，最大帧数3085帧。UAV20L由飞行模拟器生成的5个对象类的20个长视频组成。这些序列包含最小1717帧和最大5527帧。两个数据集子集都包含1231 ×  699的平均分辨率，并带有12个跟踪属性。</p><p><strong>VOT2016</strong> 包含60个序列，每个序列每帧都由不同的属性标注，包括OCC、IV、MOC、ARC、SCO和FCM。序列的平均分辨率为757×480，最小帧数为48，最大帧数为1507。</p><p><strong>VOT2018</strong> 该数据集由短期和长期挑战组成。VOT2018 ShortTerm (VOT2018-st)挑战由24个对象类别的60个序列组成。短期挑战序列的平均分辨率为758 ×  465，最小帧数为41帧，最大帧数为1500帧。长期分割由35个长期序列组成。序列的长期平均分辨率为896 ×  468，最小帧数为1389，最大帧数为29700。</p><p><strong>VOT2020</strong> 由五个子集组成，我们使用VOT2020短期(VOT2020-st)数据集来评估跟踪器的性能。VOT2020-ST与VOT2018-ST在视频数量、类数量和属性数量方面相同。区别在于标注由mask编码，并且重新定义了A,R,EAO。可以参考我之前写的<a href="https://www.bilibili.com/read/cv7654043?from=search&amp;spm_id_from=333.337.0.0">VOT2020 测评指标</a>。</p><p><strong>TrackingNet</strong> 由60643个序列和超过1400万个密集的包围框注释组成。它涵盖了27个不同的对象类。序列也由15个跟踪属性表示。数据集被划分为训练、验证和测试部分。训练集包含30643个序列，而测试集包含511个视频。在测试集中，序列的平均分辨率为591  × 1013，最小帧数为96，最大帧数为2368，帧数为30fps。</p><p><strong>LaSOT</strong> 由1120个训练序列(2.8M帧)和280个测试序列(685K帧)组成。在每一帧中，所有序列都用边界框标注。对象类别是从ImageNet中选择的，包含70个不同的对象类别，每个类别包含20个目标序列。根据ARC、BC、FCM、DEF、POC、ROT和VC等14个属性对序列进行分类。序列的平均分辨率为632  × 1089。此外，该数据集包含非常长的序列，范围在1000到11,397帧之间。</p><p><strong>GOT -10K</strong> 由WordNet语义层次结构中的10,000个视频组成。目的是为开发具有丰富运动轨迹的类无关跟踪器提供统一的训练和测试平台。这些序列被分类为563类运动物体、6种跟踪属性和87类运动，以覆盖现实世界中尽可能多的具有挑战性的模式。GOT-10K分为训练、验证和测试三部分。训练集包含9340个序列，480个物体类别；测试集包含420个视频，83个物体类别，每个序列平均长度为127帧。在测试集中，序列的平均分辨率为929  × 1638，最小帧数为51，最大帧数为920，帧数为10fps。</p><h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><p><strong>Precision Plot</strong> 精度曲线基于中心位置误差，中心位置误差定义为目标物体的预测中心与真实目标框中心之间的平均欧氏距离。距离精度，定义为目标物体的中心定位误差在T像素范围内的帧数所占的百分比。通过绘制阈值范围内的距离精度来生成精度曲线，使用T=20的精度对跟踪器进行排名。</p><p><strong>Success Plot</strong> 精度只度量跟踪器的定位性能，对于测量目标尺度变化不准确。因此考虑用IOU来衡量这一指标，成功率是预测IOU大于阈值T的百分比。通过将IOU阈值从0改变到1来生成成功率曲线，使用曲线下的面积对跟踪器进行排名。</p><p><strong>Normalized Precision Plot</strong> 由于距离精度对目标尺度敏感，因此引入归一化精度，它计算相对于目标大小的误差，而不是考虑绝对距离。然后在0到0.5的范围内绘制相对误差曲线，这条曲线下的面积称为归一化精度，用于对跟踪器进行排序。</p><p><strong>Average Overlap</strong> 度量预测和真实框的重叠率的平均值。</p><p><strong>$SR_{0.50}$和$SR_{0.75}$</strong> 阈值为0.50和0.75时的成功率。</p><p>在OTB100、TC128、UAV123和LaSOT数据集上，使用One pass评估标准，从精度和成功率方面衡量跟踪性能。这些数据集上的跟踪器是通过在第一帧上初始化并让它运行到序列的末尾来计算的。</p><p>在VOT系列中，跟踪器一旦偏离目标就会被重置。VOT评估协议根据准确性(A)，鲁棒性(R)和期望平均重叠(EAO)对跟踪器进行比较。A是成功跟踪期间预测和真值的平均重叠。R表示跟踪器在跟踪过程中丢失目标(失败)的次数。一旦跟踪器丢失目标对象，复位机制将在某些帧后启动。EAO是一种估计跟踪器期望在与给定数据集具有相同视觉特性的大量短期序列上获得的平均重叠。</p><h2 id="定量分析"><a href="#定量分析" class="headerlink" title="定量分析"></a>定量分析</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211227162203.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211227162224.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211227162351.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211227162405.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211227162415.png" alt=""></p><p>LaSOT和UAV123包含长序列和多个干扰，在这些数据集上性能较高的跟踪器具有较强的鲁棒性和重检测能力。我们观察到，最近的跟踪器DiMP和PrDiMP取得了很强的效果，SiamR-CNN中的干扰感知跟踪提高了此类场景中的鲁棒性。与LaSOT相比，TrackingNet和GOT10k包含短序列，鲁棒性和重检测能力的重要性要小得多。相反，这些数据集更看重高精度的预测框，比如SiamR-CNN和PrDiMP。此外，SiamR-CNN和SiamAttn在几个数据集均实现了良好的结果。除了SiamAttn在LaSOT和SiamRCNN在VOT上效果欠佳。在基于DCF的方法中，PrDiMP在所有评估的数据集中一致地实现SOTA结果。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211227162909.png" alt=""></p><p>图6展示了近年来不同benchmark上跟踪性能的提升趋势。可以看到OTB几乎饱和了，而最近推出的LaSOT、GOT10K和TrackingNet都显示了类似的趋势，性能还在持续上涨，且仍有较大空间。这表明，这些新的具有挑战性的benchmark对于SOTA跟踪器来说仍然是非常具有挑战性的，它们的引入显著地推动了视觉跟踪研究的上界。</p><h2 id="速度比较"><a href="#速度比较" class="headerlink" title="速度比较"></a>速度比较</h2><p>KCF和STAPLE追踪器的速度明显最好，而DeepSRDCF和HCF等借助离线深度学习特征的跟踪器速度较慢。</p><h1 id="DISCUSSION-AND-CONCLUSIONS"><a href="#DISCUSSION-AND-CONCLUSIONS" class="headerlink" title="DISCUSSION  AND CONCLUSIONS"></a>DISCUSSION  AND CONCLUSIONS</h1><p><strong>Importance of end-to-end tracking framework</strong> 端到端训练不管对DCF还是Siamese跟踪都非常管用。随着大规模训练数据集的引入，这在过去几年才成为可能。</p><p><strong>Importance of robust target modeling</strong> 虽然基于Siamese的方法在许多领域都表现出色，但基于端到端DCF的方法在挑战长期跟踪场景(如LaSOT)方面仍然显示出优势。这说明了通过在网络结构中嵌入判别学习模块来实现鲁棒在线目标外观建模的重要性。这些方法有效地整合了背景外观线索，并且在使用在线学习的跟踪过程中易于更新。</p><p><strong>Target state estimation</strong> 基于Siamese的方法通过利用目标检测技术，推动了更精确的边框回归的进步。最近的基于单阶段(无锚框)的方法，如Ocean，实现了简单、准确和高效的边框回归。此外，这些策略是通用的，可以很容易地集成到任何视觉跟踪架构中。</p><p><strong>Role of segmentation</strong> 分割可以提供更精确的像素级预测，此外，分割还可以改善跟踪本身的潜力，例如辅助目标模型更新。因此，未来的工作应着眼于将准确的分割集成到跟踪框架中。</p><p><strong>Backbone architectures</strong> ResNet在视觉跟踪中仍然是最常用的特征提取方法，但在计算资源受限的平台ResNet的计算成本仍然较高。因此未来研究移动平台的高效backbone仍然是一个有趣的方向。</p><p><strong>Estimating geometry</strong> 将DCF和Siamese方法往3D方向扩展。</p><p><strong>Role of Transformers</strong> Transformer最近在各种视觉任务上都取得了成功，也被应用到了跟踪。未来还需要做更多的工作来进一步分析Transformer的有效性，以及它与DCF和Siamese的联系。</p><p><strong>Future directions</strong> 1）与分割结合；2）与SLAM结合；3）与多目标跟踪结合。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/52-survey/20211222231118.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Visual Object Tracking with Discriminative Filters and Siamese Networks: A Survey and Outlook 马丁团队的新作，强烈推荐。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.02838&quot;&gt;论文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;精确和鲁棒的视觉目标跟踪是计算机视觉中最具挑战性和最基本的问题之一。它需要在只给定目标初始状态的条件下，准确估计图像序列中目标的轨迹及状态。相关滤波和孪生网络已经成为当下的主流跟踪算法，本文选取了90多个DCF和Siamese跟踪器进行系统和全面的回顾。首先，介绍了DCF和Siamese跟踪核心公式的背景理论。然后，区分和全面回顾了这两种跟踪范式中共享的和各自特定的挑战。此外，深入分析了DCF和Siamese跟踪器在9个benchmark上的性能，涵盖了视觉跟踪的不同方面的实验：数据集、评估指标、性能和速度比较。在此分析的基础上，提出了对视觉跟踪开放挑战的建议。&lt;/p&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="综述" scheme="https://www.zjp97.top/tags/%E7%BB%BC%E8%BF%B0/"/>
    
    <category term="DCF" scheme="https://www.zjp97.top/tags/DCF/"/>
    
    <category term="孪生网络" scheme="https://www.zjp97.top/tags/%E5%AD%AA%E7%94%9F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>轻量化目标跟踪</title>
    <link href="https://www.zjp97.top/tracking/51-lighttrack/"/>
    <id>https://www.zjp97.top/tracking/51-lighttrack/</id>
    <published>2021-12-22T03:10:04.000Z</published>
    <updated>2022-06-11T03:00:51.623Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>近些年的目标跟踪算法都在往做大做强的方向发展，比如更深的网络和更复杂的模块。尽管性能越刷越高，但是却很少考虑效率问题，以至于几乎无法在边缘设备上实时运行部署，实用性较低，因此研究轻量化的目标跟踪算法是非常必要的（另外一个原因也可能是做大做强上能水论文的点越来越不好找了 /狗头保命）。本篇博客总结了三篇最近研究跟踪模型轻量化的工作。</p><ul><li><a href="https://arxiv.org/abs/2104.14545">LightTrack: Finding Lightweight Neural Networks for Object Tracking via One-Shot Architecture Search</a></li><li><a href="https://arxiv.org/abs/2112.09686">Efficient Visual Tracking with Exemplar Transformers</a></li><li><a href="https://arxiv.org/abs/2112.07957">FEAR: Fast, Efficient, Accurate and Robust Visual Tracker</a></li></ul><span id="more"></span><h1 id="LightTrack"><a href="#LightTrack" class="headerlink" title="LightTrack"></a>LightTrack</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222211440.png" alt=""></p><p>详细解读：<a href="https://www.bilibili.com/video/BV1r54y1V7EC?from=search&amp;seid=5643544047018487388&amp;spm_id_from=333.337.0.0">【极市直播】严彬：CVPR 2021-LightTrack：基于网络结构搜索的超轻量级跟踪模型设计</a></p><p>LightTrack使用神经架构搜索（NAS）来设计更轻量级和高效的目标追踪器。实验表明，LightTrack与手工设计的 SOTA 跟踪器（如SiamRPN++和Ocean）相比，可以实现更优越的性能，而需要的计算量和参数要少得多。此外，当部署在资源受限的移动芯片上时，也能以更快的速度运行。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222114947.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222115004.png" alt=""></p><p>LightTrack采用one-shot NAS的方法搜索结构，流程如图2所示。整个过程训练与搜索是解耦的，首先训练超网（随机采样路径进行训练），然后用进化算法从超网中寻找最优子结构。</p><p>最后搜索到的LightTrack-Mobile结构如图3所示，具有如下特点：</p><ol><li>backbone中有将近一半使用$7 \times 7$的卷积核大小，可能是因为这样能在较浅的backbone中尽量提升感受野；</li><li>搜索架构选择了倒数第二个block作为特征输出，可能是因为跟踪网络并不倾向太高级的语义特征；</li><li>分类分支需要的网络层数比回归分支要少，可能是因为粗目标定位比精确的边框回归更容易。</li></ol><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222120151.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222120211.png" alt=""><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222120235.png" alt=""></p><p>实验可以看到三个版本mobile，largeA，largeB在性能、计算量和参数量上都具有优势。在骁龙845中，LightTrack运行速度比Ocean快12倍，参数量减少13倍，计算量减少38倍。作者称这种改进可能会缩小学术模型和工业部署在物体跟踪任务中的差距。</p><h1 id="Exemplar-Transformer"><a href="#Exemplar-Transformer" class="headerlink" title="Exemplar Transformer"></a>Exemplar Transformer</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222211457.png" alt=""></p><p>本文对transformer架构进行轻量化，提出了一种高效的Exemplar Transformer来替代卷积。E.T.Track在CPU上速度达到47FPS，比其他基于transformer的跟踪器快8倍，作者称这是目前唯一的实时transformer-based的跟踪器。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222213402.png" alt=""></p><h2 id="Exemplar-Transformers"><a href="#Exemplar-Transformers" class="headerlink" title="Exemplar Transformers"></a>Exemplar Transformers</h2><p>transformer中self-attention计算如公式2：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222160932.png" alt=""></p><p>Q,K的序列长度均为图像尺寸，公式2计算复杂度为图像尺寸的平方，这样带来较大计算负担。作者认为，对所有特征之间的关联在机器翻译中是必要的，但是在视觉任务中是不必要的。因为机器翻译中每个特征都代表一个特定的单词或标记，而视觉任务中相邻的空间通常表示相同的物体。因此在视觉任务中，可以减少特征向量的数量，构建一个更粗略更具描述性的视觉表达，从而显著降低计算复杂度。</p><p>作者首先提出了两个假设：</p><ol><li>一个小的exemplar value集合可以在一个数据集之间共享；</li><li>一个粗略的查询具有足够的描述性来利用这些exemplar value。</li></ol><p>为此，作者在构建query时首先将输入特征图$X \in \mathbb{R}^{H \times W \times C}$通过平均池化压缩成空间维度为S的大小，再经过线性映射得到Q。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222203001.png" alt=""></p><p>其中S=1，即将query映射成一个向量，这样可以使效率最大化。作者认为对于单目标跟踪，一个查询就足够了。</p><p>对于Key，作者学习了一小组捕获数据集信息的范例表示，而不是一个细粒度的特征映射和仅仅依赖于样本内部的关系。 Exemplar keys可以表示成$K=\hat{W}_K \in \mathbb{R}^{E \times D} $，数量从HW降为E。这个Key与输入特征是无关的，是从整个训练数据集中学习出来的一个变量。</p><p>对于Value，采用卷积操作在局部层面进行操作。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222203750.png" alt=""></p><p>其中$W_V \in \mathbb{R}^{E \times K \times K}$。整个exemplar attention可以表示为：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222203939.png" alt=""></p><p>exemplar attention和传统的self-attention对比如图2和3所示，本文方法利用卷积处理局部特征，利用相似性度量处理全局特征。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222204310.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222204324.png" alt=""></p><p>计算复杂度分析如表1所示，其中Key的数量E=4，所以Exemplar Attention的计算量和卷积是同一个量级的。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222210355.png" alt=""></p><h2 id="E-T-Track-Architecture"><a href="#E-T-Track-Architecture" class="headerlink" title="E.T.Track Architecture"></a>E.T.Track Architecture</h2><p>上述提出的Exemplar Transformer layer可以作为卷积的替代，作者将LightTrack的预测头分支所有卷积换成了Exemplar Transformer，构建新的跟踪器E.T.Track如图4所示。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222211042.png" alt=""></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222211237.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222211248.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222211303.png" alt=""></p><h1 id="FEAR"><a href="#FEAR" class="headerlink" title="FEAR"></a>FEAR</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222213350.png" alt=""></p><p>本文的目的是设计快速、高效、准确、鲁棒的跟踪器，提出两个轻量化模型，dual-template module和pixel-wise fusion block。前者使用一个可学习的参数集成了时域信息，而后者使用更少的参数编码了更有判别性的特征。使用复杂的backbone，本文方法FEAR-M和FEAR-L在速度和精度上超过大多数算法；而使用轻量backbone的版本FEAR-XS比目前的Siamese跟踪器快10倍以上的跟踪速度，同时保持接近的精度。FEAR-XS比LightTrack小2.4倍，快4.3倍，且具有更高的精度。此外，本文引入能耗和速度来扩展模型效率的定义。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222214236.png" alt=""></p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222214256.png" alt=""></p><p>整体方法如图2所示，跟经典siamese方法的差别在于输入增加了一个动态模板，将静态和动态模板进行线性插值后再与搜索特征进行融合。</p><p>特征提取部分使用轻量的FBNet，特征融合部分设计了像素级别的融合，如图3所示，这个和PGNet，CGACD等方法的操作是一样的。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222214656.png" alt=""></p><p><strong>Dynamic Template Update</strong></p><p>初始模板$F_T$和动态模板$F_d$通过可学习的参数$\omega$进行线性融合。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222215132.png" alt=""></p><p>动态模板的选择如图4所示</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222215336.png" alt=""></p><p>将搜索特征与分类分数相乘后池化得到向量$e_s$，动态模板进行池化得到向量$e_t$，计算二者的余弦相似度。推理阶段从每N个历史搜索帧中选择相似度最大的帧裁剪更新动态模板。训练时还额外增加了负样本$e_T$构建三元损失。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222220117.png" alt=""></p><h2 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222220456.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222220504.png" alt=""></p><p>实验除了常规的benchmark性能测试还专门做了效率分析。作者引入了FEAR Benchmark来评估跟踪算法对移动设备电池和热状态的影响，以及随着时间的推移对处理速度的影响。如图5所示，随着运行时间的增加，其他算法均出现了速度下降、电量下降，温度升高的现象，但本文的FEAR-XS在这些指标上均能保持稳定（高速、耗电少、温度低）。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222220712.png" alt=""></p><p>速度上在多款手机处理器上均大幅超过了LightTrack。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/51-lighttrack/20211222220858.png" alt=""></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;近些年的目标跟踪算法都在往做大做强的方向发展，比如更深的网络和更复杂的模块。尽管性能越刷越高，但是却很少考虑效率问题，以至于几乎无法在边缘设备上实时运行部署，实用性较低，因此研究轻量化的目标跟踪算法是非常必要的（另外一个原因也可能是做大做强上能水论文的点越来越不好找了 /狗头保命）。本篇博客总结了三篇最近研究跟踪模型轻量化的工作。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2104.14545&quot;&gt;LightTrack: Finding Lightweight Neural Networks for Object Tracking via One-Shot Architecture Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.09686&quot;&gt;Efficient Visual Tracking with Exemplar Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.07957&quot;&gt;FEAR: Fast, Efficient, Accurate and Robust Visual Tracker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="综述" scheme="https://www.zjp97.top/tags/%E7%BB%BC%E8%BF%B0/"/>
    
    <category term="轻量化" scheme="https://www.zjp97.top/tags/%E8%BD%BB%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Explicitly Modeling the Discriminability for Instance-Aware Visual Object Tracking</title>
    <link href="https://www.zjp97.top/tracking/50-IAT/"/>
    <id>https://www.zjp97.top/tracking/50-IAT/</id>
    <published>2021-11-24T02:40:56.000Z</published>
    <updated>2022-06-11T03:00:51.623Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211124104316.png" alt=""></p><p><a href="https://arxiv.org/abs/2110.15030">论文</a></p><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>作者认为当前方法的局限在于，跟踪目标的特征仅被表达学习，而没有做判别性的建模（即网络只学会了如何从表观上描述一个目标的特征，但并未学到不同目标特征之间的差异）。为了解决这一问题，本文引入对比学习（contrastive learning）构建实例级的跟踪器 Instance-Aware Tracker (IAT)，确保每个训练样本都能被唯一建模，并与其他大量样本高度区分。提出的IAT包括video-level和object-level两种形式，前者提高了从背景中识别目标的能力，后者提高了区分目标和干扰物的判别能力。</p><span id="more"></span><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>首先举一个例子，当人类在跟踪目标时，比如一只猫，我们不但会尽力记住它的眼睛、颜色、耳朵的形状等几个显著特征；同时利用大脑中的强先验知识还会知道这是一只特定的猫，知道这只猫和其他猫有什么不同，甚至知道和我们脑海中所有其他的东西有什么不同。这就是作者的出发点，一个强大的表征不仅应该<strong>表达性</strong>(expressively)地描述目标的特征，还需要<strong>区分性</strong>(discriminatively)地描述目标的特征。</p><p>对于判别性建模当前已经有一些工作：DaSiamRPN增加语义负样本来提升判别性，但一个模板一次只能看到一个负样本，因此效果有限。SINT++提出生成难正样本进行鲁棒跟踪，但没有考虑负样本的重要性。<a href="https://ieeexplore.ieee.org/document/9324950">Yao</a>在在线更新时选择难负样本来提高模型的适应性，但会降低推理时的速度。DiMP充分利用目标-背景差异达到了较强的性能，但同样面对负样本数量有限的问题。这些方法均不完全或隐式地模拟了判别性建模，因此阻碍了性能提升。</p><p>因此，本文提出实例感知跟踪器 (Instance-Aware Tracker, IAT) 显式地建模视觉跟踪的判别性，首次将基于对比学习的实例级分类任务集成到跟踪中。作者借鉴了经典的MoCo算法（算法结构如下图），可以参考<a href="https://www.bilibili.com/video/BV1av411874e?spm_id_from=333.999.0.0">MoCo论文简析</a>。将每个目标看成一个特定的类别，在memory bank中存储大量的类内和类间负样本。然后，对网络进行训练，使其从大量的负样本中对比区分目标对象，从而提升判别能力。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211124162015.png" alt=""></p><p>作者提出了两种不同级别的实例感知，视频级和对象级。前者将整个视频看成一个样本，从全局的视角提高了从背景中识别目标的能力；后者将每个目标看成一个实例，强化了目标和干扰物的区分能力。提出的方法仅在训练过程中使用，因此不会增加推理时间，作者在PrDiMP上进行实验，速度可以达到30 FPS。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211125165645.png" alt=""></p><p>整体框架如图2所示，包括特征提取、分类分支、回归分支和实例分类分支。整个上半部分就是PrDiMP的结构，增加的实例分类分支将每个目标与大量其他样本进行分类，以提高跟踪器的判别力。下面对实例分类分支进行详细介绍。</p><h2 id="Instance-Classification-Branch"><a href="#Instance-Classification-Branch" class="headerlink" title="Instance Classification Branch"></a>Instance Classification Branch</h2><p>为了引入大量负样本，作者借鉴了自监督学习方法MoCo，将该分支定义为字典查找任务，将来自同一视频的帧作为正样本，将来自其他视频的帧作为负样本。如图2所示，该分支包括memory encoder $\omega$ 和instance boosting module $\psi$，其中$\omega$的网络<strong>结构</strong>和f1/f2相同用于提取字典中的特征，$\psi$调整来自$\omega$和f1的特征用于后续对比学习。</p><p>给定query q和字典$\{k_+, \{k_i\}^K_{i=1} \}$，字典中包含一个正样本$k_+$和K个负样本，对比学习的任务是从字典中查找到对应q的正样本$k_+$。在本文的跟踪任务中，q为模板特征，$k_+$为搜索特征，而$\{k_i\}^K_{i=1}$是来自其他视频的历史搜索特征。训练时，给定模板帧$I_t$和搜索帧$I_s$，实例分类分支的前向传播如下：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211125095342.png" alt=""></p><p>而字典是一个大的队列，每次迭代都会将当前的$k_+$送入队列，把历史最早的特征移出队列，这样无需重复计算就能得到大量历史搜索特征作为负样本，这其中的负样本同时包含了类内和类间的负样本。memory encoder $\omega$ 采用动量更新的方式逐步逼近query的encoder f1/f2：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211125103154.png" alt=""></p><p>样本query和key的相似性通过点积来衡量，最终的对比损失类似InfoNCE：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211125103535.png" alt=""></p><h2 id="Two-Variants-of-IAT"><a href="#Two-Variants-of-IAT" class="headerlink" title="Two Variants of IAT"></a>Two Variants of IAT</h2><p>上面介绍的对比学习框架中的loss是比较不同实例之间的相似性，但输入是完整的图片，因此下面我们来看如何将输入的图片转化成实例的概念，即instance boosting module $\psi$。作者提出了两种不同粒度的实例的概念，包括视频级别的实例(IAT-V)和对象级别的实例(IAT-O)，如图3所示。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211125112935.png" alt=""></p><p>IAT-V将每个视频视为一个实例，它使用了包含目标和背景的整个输入帧的特征，结构如图3左半部分所示。经过一层卷积后全局池化成$1 \times 1$大小，再过两层FC得到了对应的特征向量。IAT-V可以学习视频之间的差异，某一视频中的目标将有机会从其他负样本视频的目标和背景中学习目标和干扰物之间的差异。因此，IAT-V可以从全局的角度提高从包括语义对象和非语义杂波的各种背景中识别目标的能力。</p><p>IAT-O将每个目标边界框视为一个实例，如图3右半部分所示。相比IAT-V增加了一个ROI Pooling层来提取目标框内的特征。因此，IAT-O中的所有样本均为语义样本，可以从更具体的角度来区分不同物体之间的特征。</p><h2 id="Training-Loss"><a href="#Training-Loss" class="headerlink" title="Training Loss"></a>Training Loss</h2><p>训练损失包含三部分：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211125114405.png" alt=""></p><p>其中$L_{cls}, L_{reg}$来自PrDiMP的分类和回归损失，$L_{ins}$是公式5的实例分类损失。整个训练流程如算法1所示，和MoCo的流程是一样的。注意该过程只用在训练中，推理时只有分类和回归分支在工作。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211125115134.png" alt=""></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>实验选择了8个数据集进行验证：OTB50, OTB100, GOT-10k, LaSOT, NFS, UAV123, TrackingNet, VOT2019。负样本数量K=1000，数量远超DaSiamRPN和DiMP中的负样本个数。</p><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211125115114.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211125115232.png" alt=""></p><p>表1分析了memory bank中负样本K的数量，可以看到，增加负样本哪怕只有10个都是有益的，差不多到1000性能饱和了，再增加反而加大了区分的难度。</p><p>表2分析了IAT-O中不同的ROI池化尺寸，3最佳。</p><p>表3对比了IAT-V，IAT-O以及将它们结合的效果。shared表示二者共享同一个instance classification branch并且将得到的特征向量相加，separated表示不共享instance classification branch但同样相加特征向量。这两种结合方式均未带来明显提升，因此作者认为单独使用即可。</p><h2 id="SOTA-Comparison"><a href="#SOTA-Comparison" class="headerlink" title="SOTA Comparison"></a>SOTA Comparison</h2><p>在大多数数据集上，IAT-O结果优于IAT-V。可能是该模型更细粒度的区分，进一步提高了克服干扰的能力。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211125120055.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211125120135.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211125120202.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211125120217.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211125120230.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211125120242.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211125120254.png" alt=""></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/50-IAT/20211124104316.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.15030&quot;&gt;论文&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h1&gt;&lt;p&gt;作者认为当前方法的局限在于，跟踪目标的特征仅被表达学习，而没有做判别性的建模（即网络只学会了如何从表观上描述一个目标的特征，但并未学到不同目标特征之间的差异）。为了解决这一问题，本文引入对比学习（contrastive learning）构建实例级的跟踪器 Instance-Aware Tracker (IAT)，确保每个训练样本都能被唯一建模，并与其他大量样本高度区分。提出的IAT包括video-level和object-level两种形式，前者提高了从背景中识别目标的能力，后者提高了区分目标和干扰物的判别能力。&lt;/p&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="arxiv" scheme="https://www.zjp97.top/tags/arxiv/"/>
    
    <category term="contrastive learning" scheme="https://www.zjp97.top/tags/contrastive-learning/"/>
    
  </entry>
  
  <entry>
    <title>Siamese Transformer Pyramid Networks for Real-Time UAV Tracking</title>
    <link href="https://www.zjp97.top/tracking/49-SiamTPN/"/>
    <id>https://www.zjp97.top/tracking/49-SiamTPN/</id>
    <published>2021-11-23T07:53:36.000Z</published>
    <updated>2022-06-11T03:00:51.623Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/49-SiamTPN/20211123155750.png" alt=""></p><p><a href="https://arxiv.org/abs/2110.08822">论文</a> <a href="https://github.com/RISC-NYUAD/SiamTPNTracker">代码</a></p><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>本文着眼于移动平台的无人机目标跟踪，融合了CNN和Transformer的优点。具体来说，通过轻量的shufflenet v2来构建特征金字塔，并使用Transformer对其进行强化（特征融合），以构建一个鲁棒的目标外观模型。开发了一种具有横向交叉注意力的集中式架构，用于构建增强的高级特征图。此外，作者设计了pooling attention module减少key和value的数量进一步降低了Transformer的内存消耗和时间复杂度。提出的方法在CPU端运行速度可超过30 FPS。</p><span id="more"></span><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/49-SiamTPN/20211123164050.png" alt=""></p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/49-SiamTPN/20211123171117.png" alt=""></p><p>图2c为本文提出的框架，注意作者没有直接迁移复杂的transformer编码器和解码器结构，而是利用编码器设计了基于注意力的特征金字塔融合网络来更有效地学习target-specific的模型。下面分别介绍各个模块。</p><h2 id="Feature-Extraction-Network"><a href="#Feature-Extraction-Network" class="headerlink" title="Feature Extraction Network"></a>Feature Extraction Network</h2><p>特征提取网络输出stage 3，4，5降采样倍数分别为8，16，32倍的特征，然后将模板和搜索特征分别送入Transformer Pyramid Network(TPN)进行特征融合，将融合后的特征进行互相关。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/49-SiamTPN/20211123173713.png" alt=""></p><p>其中，$\Gamma$表示TPN模块，M表示互相关结果。</p><h2 id="Feature-Fusion-Network"><a href="#Feature-Fusion-Network" class="headerlink" title="Feature Fusion Network"></a>Feature Fusion Network</h2><p><strong>Multi-head Attention</strong> 经典的MHA公式如下：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/49-SiamTPN/20211123173945.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/49-SiamTPN/20211123173954.png" alt=""></p><p><strong>Pooling Attention</strong> MHA的计算量如下</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/49-SiamTPN/20211123180328.png" alt=""></p><p>降低计算量的方法有三种：（1）减少query的数量，（2）减少维度C，（3）减少key和value数量。（1）和（2）都会减少输入到后续预测头的特征维度（包括空间和通道），影响跟踪精度。因此我们选择（3），通过池化操作来降低K和V的空间尺寸。</p><p>为了进一步降低计算量，作者去掉了位置编码，原因包括：1）输入token的排列会受到最终互相关的约束；2）位置编码会占用额外的计算和存储资源。最终的pooling attention block(PAB)可以写成</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/49-SiamTPN/20211123210427.png" alt=""></p><p>图3对比了PAB和传统MHA的区别</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/49-SiamTPN/20211123210646.png" alt=""></p><h2 id="Transformer-Pyramid-Network"><a href="#Transformer-Pyramid-Network" class="headerlink" title="Transformer Pyramid Network"></a>Transformer Pyramid Network</h2><p>为了利用同时具有低级信息和高级语义的特征金字塔，作者提出Transformer Pyramid Network (TPN)来构建具有高级语义的混合特征。TPN如图4所示，输入特征金字塔$\{P_3, P_4, P_5\}$，输出融合特征$\{P_3’, P_4’, P_5’\}$，中间包含若干个TPN block。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/49-SiamTPN/20211123211856.png" alt=""></p><p>TPN block使用$P_4$作为所有特征层次的query，产生3个具有不同池化尺度的组合，这些组合由3个并行的PAB模块处理。其中$P_3, P_4, P_5$的池化尺寸分别为4，2，1（图4中似乎标反了）。三个尺度的输出直接相加然后送入两个自注意力的PAB中，得到最终的语义特征。整个过程用公式表示：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/49-SiamTPN/20211123212739.png" alt=""></p><p>$P_3, P_5$直接恒等映射，以减少计算开销。PA block可以有效地提高层次特征之间的相关性。TPN Block重复B次，生成的特征用于后续互相关。预测头部分就是简单的无锚框分类回归结构，分类损失为交叉熵，回归损失为GIOU loss和L1 loss。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>实现细节上，搜索特征和模板大小分别为256和80，是目标大小的4倍和1.5倍，对应的特征金字塔尺寸为$\{h_3^x=32, h_4^x=16, h_5^x=8 \}$, $\{h_3^z=10, h_4^z=5, h_5^z=3 \}$。</p><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/49-SiamTPN/20211123215218.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/49-SiamTPN/20211123215233.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/49-SiamTPN/20211123215552.png" alt=""></p><p>Attention可视化。图6中的第二、三列对比了有无TPN的响应映射。如果没有TPN来学习区分特征，相关结果将变得分散，并且更容易漂移到干扰项。最后三列说明了金字塔特征之间的注意力图。低层级(P3到P4，P4到P4)之间的注意力在整个搜索区域提取了更多的局部信息，而高层级(P5到P4)的注意力更集中在目标的语义上。</p><h2 id="SOTA-Comparison"><a href="#SOTA-Comparison" class="headerlink" title="SOTA Comparison"></a>SOTA Comparison</h2><p>表3中shufflenet版本的速度在CPU上达到32.1 FPS，alexnet版本在gpu上速度105FPS，且精度超过了使用resnet50的SiamRPN++和HiFT。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/49-SiamTPN/20211123220034.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/49-SiamTPN/20211123220047.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/49-SiamTPN/20211123220108.png" alt=""></p><p>最后作者在真实场景中进行测试，不是很了解就不介绍了，感兴趣可以阅读原文。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/49-SiamTPN/20211123155750.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.08822&quot;&gt;论文&lt;/a&gt; &lt;a href=&quot;https://github.com/RISC-NYUAD/SiamTPNTracker&quot;&gt;代码&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h1&gt;&lt;p&gt;本文着眼于移动平台的无人机目标跟踪，融合了CNN和Transformer的优点。具体来说，通过轻量的shufflenet v2来构建特征金字塔，并使用Transformer对其进行强化（特征融合），以构建一个鲁棒的目标外观模型。开发了一种具有横向交叉注意力的集中式架构，用于构建增强的高级特征图。此外，作者设计了pooling attention module减少key和value的数量进一步降低了Transformer的内存消耗和时间复杂度。提出的方法在CPU端运行速度可超过30 FPS。&lt;/p&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="transform" scheme="https://www.zjp97.top/tags/transform/"/>
    
    <category term="UAV" scheme="https://www.zjp97.top/tags/UAV/"/>
    
    <category term="WACV2022" scheme="https://www.zjp97.top/tags/WACV2022/"/>
    
  </entry>
  
  <entry>
    <title>RPT++: Customized Feature Representation for Siamese Visual Tracking</title>
    <link href="https://www.zjp97.top/tracking/48-RPT/"/>
    <id>https://www.zjp97.top/tracking/48-RPT/</id>
    <published>2021-11-22T03:06:07.000Z</published>
    <updated>2022-06-11T03:00:51.623Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211122111651.png" alt=""><a href="https://arxiv.org/abs/2110.12194">论文</a></p><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>本文是去年RPT方法的改进，作者来自同一团队。出发点是现有的跟踪方法对于分类和回归使用的是同一套特征，而作者指出这两个任务是有差异的，因此对特征的需求也是不同的。如图1所示，分类需要的是显著区域中更具有判别力的特征，而回归需要边界附近的特征来精确定位。针对这一问题，作者提出了两种定制化的特征提取，用于捕获特定任务的视觉模式。其中Polar Pooling从语义关键点收集丰富的信息，以进行更强的分类；而Extreme Pooling捕获目标边界的清晰视觉模式，实现目标状态的精确估计。</p><span id="more"></span><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211122115832.png" alt=""></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>首先分析一下现有pipeline的局限。主流的anchor和anchor-free的方法都是在一个特征点上进行分类和回归，这种基于点的特征表示方法传递显式语义和边界信息的能力较弱。另一些方法通过提取目标框内的ROI特征来提升表征目标的能力，但矩形框可能包含多余的背景像素，且缺乏对物体几何结构的建模能力。</p><p>为了解决这些问题，作者先cue了一下之前的工作RPT。RPT用若干个有代表性的点集来表示目标，通过可变形卷积自动学习语义显著性和边界区域的特征。这种方法比单点特征包含更多可识别的信息，有助于真正理解对象的视觉模式。<strong>但是</strong>，RPT提取的关键点真的都可靠吗？事实上，RPT经常提取一些位于背景显著区域的错误关键点。  并且RPT用于分类和回归的特征均取自相同的点集，忽略了这两个任务之间的不对齐。如上面图1介绍的，从语义关键点提取的特征为分类提供了更具判别力的视觉模式，而边界附近的特征编码了关于空间范围的先验知识，有助于准确估计目标状态。这两个任务在特征表示中的差异极大地限制了跟踪器的性能。</p><p>因此，本文在RPT的基础上定制了两个特征提取器，分别从对应的关键点中获得语义显著信息和边界极值信息。其中，Polar Pooling通过<strong>计算从中心到每个语义关键点的径向最大响应</strong>来捕获目标区域内更精确的视觉模式。Extreme Pooling通过一个额外的<strong>不确定性分支</strong>来消除边界极值点估计中的模糊性。最后将这两个增强后的特征分别送入分类和回归分支。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h2><p>首先回顾一下baseline的RPT方法。在模板和搜索特征做互相关后，相关图上的每个特征都可以看成一个目标候选，用一系列代表性点集表示。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211122213117.png" alt=""></p><p>其中n表示点的个数，默认为9个。RPT通过两步来细化这些点的分布:</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211122213232.png" alt=""></p><p>公式2可以通过下图直观理解，对于一个初始中心点$(x_k, y_k)$，先通过回归偏移$\{(x_k^c, y_k^c)\}^n_{k=1}$得到一个粗略的点集，然后对每个点再进行一次微调$\{(x_k^r, y_k^r)\}^n_{k=1}$得到最终的目标状态。RPT通过可变形卷积得到更强大的特征表示，能够对物体的几何变换进行建模，生成的特征同时用于分类和回归。而下面介绍的改进就是通过两个不同的模块生成不同的特征进行分类和回归。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211122212826.png" alt=""></p><h2 id="Framework-of-RPT"><a href="#Framework-of-RPT" class="headerlink" title="Framework of RPT++"></a>Framework of RPT++</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211122214512.png" alt=""></p><p>RPT++整体框架如图2所示，首先预测相关特征图上每个位置的粗略关键点集，包括四个边界极值关键点（左上右下）和五个语义关键点。将四个边界点转换为伪框如下：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211122223816.png" alt=""></p><p>其中$(x_{init}, y_{init})$表示相关特征图上每个点，$\Delta x^c_{leftmost}, \Delta x^c_{topmost}, \Delta x^c_{rightmost}, \Delta x^c_{bottommost}$对应左、上、右、下的极值关键点的伪框偏移量。与RPT不同的是，本文还额外估计了每个偏移量的不确定性，用一个高斯分布表示。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211123103507.png" alt=""></p><p>其中均值$o_e \in \{ \Delta x^c_{leftmost}, \Delta x^c_{topmost}, \Delta x^c_{rightmost}, \Delta x^c_{bottommost}\}$表示预测偏移量，方差$\sigma$表示不确定性。这种方法常用于目标检测中，用一个概率分布来表示回归预测而不是仅仅估计一个硬值，从而预测回归的不确定性。</p><p>总结一下，粗略关键点集的预测对相关特征图上的每个位置，在回归分支输出n个2D偏移量，以逐点的方式细化样本点的分布；在不确定性分支输出4个伪框偏移量的不确定性。接下来，我们将语义关键点送入polar pooling得到更利于分类的视觉表示，将边界极值关键点和不确定性预测送入extreme pooling得到更精确的边界框。</p><h2 id="Customized-Feature-Extraction"><a href="#Customized-Feature-Extraction" class="headerlink" title="Customized Feature Extraction"></a>Customized Feature Extraction</h2><p><strong>Extreme Pooling</strong> 在某些情况下，目标的极值点是不明确的(例如，沿着车辆边界顶部的任何一点都可能被视为极值点)。这些模糊性使得有效的极值特征难以提取，直接限制了定位精度。Extreme pooling就是为了解决这一问题，具体来说，既然我们已经得到了每个边界极值点的不确定性，那么提取边界特征时就不用局限仅提取该点的特征，而是可以提取该点附近的一个不确定性区域的特征，这样可以更好地描述目标边界。整个过程如图3所示。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211123110412.png" alt=""></p><p>Extreme pooling首先在每个边界极值点$(x_e,y_e)$上根据其对应的不确定性$\sigma$裁剪一个区域</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211123110843.png" alt=""></p><p>其中$(x_1,y_1), (x_2,y_2)$表示裁剪区域的角点，$\mu$是缩放因子，这个不确定性区域的大小为$2\mu\sigma \times 2\mu\sigma \times C$。然后将该区域的特征送入ROIAlign+MaxPool，还原成$1 \times 1$的特征。将4个极值点都做一样的操作并与原始特征拼接，就得到了通道维度为5C的特征，其中包含了边界的不确定性。</p><p><strong>Polar Pooling</strong> 伪框中的语义关键点经常落在目标之外的背景上，这传达了不准确的视觉模式。因此作者设计了polar pooling，如图4所示。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211123112846.png" alt=""></p><p>将原始点到每个语义关键点的径向路径平均分为N个点，取这N个点的最大值特征作为输出，用公式7表示。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211123113127.png" alt=""></p><p>其中$(x_c,y_c), (x_s,y_s)$分别表示原始点和语义点的坐标，括号里的坐标可能是小数，采用双线性插值进行估计。最后，将5个语义点的输出特征值$F_s$与原始特征拼接起来，构造语义增强的特征，通道维度为6C。</p><h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>损失函数包括三部分</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211123115234.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211123115247.png" alt=""></p><p>其中$L_{reg}^C$表示学习粗略点集的KL loss，参考这篇<a href="https://blog.csdn.net/qq_14845119/article/details/102753188">博客</a>；$L_{cls}^R, L_{reg}^R$是refine过程中的Focal loss和IOU loss。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Comparison-with-SOTA"><a href="#Comparison-with-SOTA" class="headerlink" title="Comparison with SOTA"></a>Comparison with SOTA</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211123115730.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211123115837.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211123115816.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211123120046.png" alt=""></p><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211123120206.png" alt=""></p><p>表5 RPT++各个组件的消融实验</p><p>表6 polar pooling径向采样个数N分析</p><p>表7 extreme pooling缩放因子$\mu$ 分析</p><p>表8 extreme pooling 采样窗口的影响，对比了固定大小的crop以及根据不确定性crop。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/48-RPT++/20211122111651.png&quot; alt=&quot;&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.12194&quot;&gt;论文&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h1&gt;&lt;p&gt;本文是去年RPT方法的改进，作者来自同一团队。出发点是现有的跟踪方法对于分类和回归使用的是同一套特征，而作者指出这两个任务是有差异的，因此对特征的需求也是不同的。如图1所示，分类需要的是显著区域中更具有判别力的特征，而回归需要边界附近的特征来精确定位。针对这一问题，作者提出了两种定制化的特征提取，用于捕获特定任务的视觉模式。其中Polar Pooling从语义关键点收集丰富的信息，以进行更强的分类；而Extreme Pooling捕获目标边界的清晰视觉模式，实现目标状态的精确估计。&lt;/p&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="arxiv" scheme="https://www.zjp97.top/tags/arxiv/"/>
    
  </entry>
  
  <entry>
    <title>Saliency-Associated Object Tracking</title>
    <link href="https://www.zjp97.top/tracking/47-SAOT/"/>
    <id>https://www.zjp97.top/tracking/47-SAOT/</id>
    <published>2021-08-17T08:54:55.000Z</published>
    <updated>2022-06-11T03:00:51.624Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210817170638.png" alt=""></p><p><a href="https://arxiv.org/abs/2108.03637">论文</a> <a href="https://github.com/ZikunZhou/SAOT.git">代码</a></p><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>和上一篇一样是研究跟踪的匹配问题。目前主流的跟踪策略分为全局和局部两种，全局策略（如DW-Corr）学习目标的整体表示，当目标发生较大变化时表现不佳。局部策略（如PG-Corr）将目标分割成多个patch，并行跟踪所有patch，通过聚合这些patch的跟踪结果，推断出目标状态。而局部策略的局限在于并不是所有patch都包含丰富的信息，一些没有判别能力的patch难以跟踪，可能对推断目标状态产生不利影响。因此，本文提出只跟踪目标的显著局部区域而不是简单跟踪所有局部块，具体提出了细粒度的显著性挖掘模块(fine-grained saliency mining module)，用于捕获局部显著性；以及显著性关联模块(saliency-association modeling module) 将捕获的显著区域关联在一起，学习目标模板与搜索图像之间的全局相关性，以进行状态估计。</p><span id="more"></span><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210817211327.png" alt=""></p><h2 id="Overall-Framework"><a href="#Overall-Framework" class="headerlink" title="Overall Framework"></a>Overall Framework</h2><p>整体框架如图2所示，模板特征$F_x \in \mathbb{R}^{h_x \times w_x \times c } $ 和搜索特征 $F_s \in \mathbb{R}^{h_s \times w_s \times c } $ 首先送入Saliency Mining module用于提取模板的局部显著性，该模块计算模板和搜索特征的pixel-to-pixel相似性，选择局部最大且最锐利的点(local sharp maximum points) 作为显著点，这些显著点表示模板中最具判别性的区域。</p><p>然后，Saliency-Association Modeling module将捕获的显著点进行关联，以学习模板和搜索图像之间的有效全局相关性表示。最后再对相关性结果进行状态估计。</p><h2 id="Saliency-Mining"><a href="#Saliency-Mining" class="headerlink" title="Saliency Mining"></a>Saliency Mining</h2><p>显著性挖掘模块用于捕获模板中具有判别能力的局部显著区域，包括两个步骤：</p><ol><li>为模板$F_x$中的每个点构建与搜索特征$F_s$的相似图；</li><li>根据相似图衡量$F_x$中每个点的显著性，从而进行选择。</li></ol><p><strong>Construction of similarity maps</strong> </p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210818103912.png" alt=""></p><p>pixel-to-pixel相似图的构建如图3所示，将模板特征中的每个点$(u,v)$与搜索特征计算余弦距离，得到单通道的相似图$S_(u,v) \in  \mathbb{R}^{h_s \times w_s}$，总共会生成$h_x \times w_x$这样的相似图。</p><p><strong>Saliency evaluation</strong> 接下来根据每个相似性图中最大值点附近的峰值分布（包括强度和集中度）来评估对应模板特征点的显著性。强度使用峰值旁瓣比PSR进行衡量</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210818112612.png" style="zoom:80%;" /></p><p>其中，$\Phi$表示旁瓣，$\mu_{\Phi}, \sigma_{\Phi}$分别表示旁瓣的均值和方差。在原始的PSR中，主瓣$\Psi$的大小是预定义的固定值，作者认为这样定义是不合理的，因为没有考虑响应图的分布。图4展示了两种不同峰分布的相似图，这两个相似图显然对应着主瓣的大小不同。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210818115217.png" alt=""></p><p>为了消除这一影响，作者将主瓣的边界定义为峰附近最靠近的轮廓，其高度等于整个相似图的平均值。至此，我们可以得到相似图$S_{ (u,v) }$的峰强度$\gamma$：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210818160143.png" style="zoom:80%;" /></p><p>用于显著性评价的另一个度量是峰分布的集中度(concentration)，它与主瓣覆盖面积$A_{\Psi}$成反比。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210818160322.png" style="zoom:80%;" /></p><p>将上面的强度和集中度结合起来就得到了相似图$S_{ (u,v) }$的显著性$s( S_{ (u,v) } )$：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210818160635.png" style="zoom:80%;" /></p><p>考虑到应该尽可能跟踪目标模板的中心，因此在计算显著性时额外增加一个高斯正则项，公式7中的高斯项$g$是与模板中心对齐的。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210818161803.png" style="zoom:80%;" /></p><p>根据公式7的显著性评价方式，我们可以计算模板特征$F_x$中每个点的显著性，并选取K个最大的点构成显著性集合 $P_x = \{ p_x^k \}_{k=1}^K$，这个显著性结合在搜索特征$F_s$的对应匹配点集合为$P_s = \{ p_s^k \}_{k=1}^K$。</p><h2 id="Saliency-Association-Modeling"><a href="#Saliency-Association-Modeling" class="headerlink" title="Saliency-Association Modeling"></a>Saliency-Association Modeling</h2><p>显著性关联模块利用捕获的显著性来学习模板和搜索特征之间的全局关联，作者利用图的方式进行构建，同样包括两步：</p><ol><li>在捕获的显著性之间构建一个图，以建模这些显著性之间的交互关系；</li><li>基于构造的图聚合显著性。</li></ol><p><strong>Construction of the saliency graph</strong>  构建显著性图时同时考虑了相似图$S$和搜索特征$F_s$，将二者拼接得到 $F_g$，共有$h_s \times w_s$ 个节点，每个节点的维度是 $h_x w_x + c$。然后构建图的边，如图5所示，包括两种类型，一种是对显著点之间进行连接，另一种是对每个点和其邻域点进行连接，边的集合定义为$C$。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210818164636.png" alt=""></p><p>本文用两层全连接层来学习边的权重，定义权重的邻接矩阵为 $A \in \mathbb{R}^{N \times N}, N=h_s w_s $</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210818170326.png" alt=""></p><p>其中 $v1, v2$是图的两个节点，$\phi_1, \phi_2$是两层全连接层，用sigmoid函数约束权值范围在0到1之间。只有当边$\langle i, j \rangle$符合图5中的两种情况才有权值。</p><p><strong>Aggregation of the captured saliencies</strong>  使用两层的GCN进行显著性聚合</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210818172505.png" alt=""></p><p>其中m和M表示多项式的阶数和总阶数，$\omega_m$是可学习权重。$\hat{A} = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{\frac{1}{2}}$是normalized邻接矩阵，$\tilde{A}=A+I$，$ \tilde{D}$是$\tilde{A}$的对角阵，这些均为图卷积知识。$X^{l} \in \mathbb{R}^{N \times d_l}$和$X^{l+1} \in \mathbb{R}^{N \times d_{l+1} }$是第 l 层上所有节点的输入和输出特征，其中 $d_l, d_{l+1}$是对应的特征维度，$d_0 = h_x w_x + c$，$ \Theta_m^l \in \mathbb{R}^{d_l \times d_{l+1}} $是第l层第m阶的参数矩阵。</p><p>通过构造显著性图并进一步进行显著性聚合，Saliency-Association Modeling module能够学习目标模板与搜索图像之间的全局相关表示，进而用于预测搜索图像中的目标状态。</p><h2 id="Tracking-Framework"><a href="#Tracking-Framework" class="headerlink" title="Tracking Framework"></a>Tracking Framework</h2><p>跟踪框架如图2所示，预测头采用FCOS，并增加DiMP的在线跟踪。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210818212315.png" alt=""></p><p><strong>Base model</strong> 只有特征提取，DiMP的在线分类，以及预测头；</p><p><strong>PPFM</strong> 只做图3的Pixel-to-Pixel匹配，然后将拼接后的$S$和$F_s$直接过两层卷积得到相关结果；</p><p><strong>PAM</strong> 没有提取显著性，将所有局部平等的进行关联（类似self-attention）；</p><p>PPFM和base model的对比证明建模模板和搜索图像之间的相似性是有益的，PAM和PPFM的对比证明将匹配的局部进行关联是有益的；最后SAOT和PAM的对比证明了局部显著性是有益的。最后还与经典的DW-Corr和PG-Corr进行比较。</p><h2 id="SOTA-Comparison"><a href="#SOTA-Comparison" class="headerlink" title="SOTA Comparison"></a>SOTA Comparison</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210818213939.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210818213952.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210818214010.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210818214021.png" alt=""></p><h2 id="Qualitative-Study"><a href="#Qualitative-Study" class="headerlink" title="Qualitative Study"></a>Qualitative Study</h2><p>可视化结果有点牛逼，对于形变和干扰场景的响应看上去非常好。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210818214434.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210818214104.png" alt=""></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/47-SAOT/20210817170638.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2108.03637&quot;&gt;论文&lt;/a&gt; &lt;a href=&quot;https://github.com/ZikunZhou/SAOT.git&quot;&gt;代码&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h1&gt;&lt;p&gt;和上一篇一样是研究跟踪的匹配问题。目前主流的跟踪策略分为全局和局部两种，全局策略（如DW-Corr）学习目标的整体表示，当目标发生较大变化时表现不佳。局部策略（如PG-Corr）将目标分割成多个patch，并行跟踪所有patch，通过聚合这些patch的跟踪结果，推断出目标状态。而局部策略的局限在于并不是所有patch都包含丰富的信息，一些没有判别能力的patch难以跟踪，可能对推断目标状态产生不利影响。因此，本文提出只跟踪目标的显著局部区域而不是简单跟踪所有局部块，具体提出了细粒度的显著性挖掘模块(fine-grained saliency mining module)，用于捕获局部显著性；以及显著性关联模块(saliency-association modeling module) 将捕获的显著区域关联在一起，学习目标模板与搜索图像之间的全局相关性，以进行状态估计。&lt;/p&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="ICCV2021" scheme="https://www.zjp97.top/tags/ICCV2021/"/>
    
    <category term="matching" scheme="https://www.zjp97.top/tags/matching/"/>
    
  </entry>
  
  <entry>
    <title>Learn to Match: Automatic Matching Network Design for Visual Tracking</title>
    <link href="https://www.zjp97.top/tracking/46-AutoMatch/"/>
    <id>https://www.zjp97.top/tracking/46-AutoMatch/</id>
    <published>2021-08-07T03:33:08.000Z</published>
    <updated>2022-06-11T03:00:51.626Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210808103150.png" alt=""></p><p><a href="https://arxiv.org/abs/2108.00803">论文</a> <a href="https://github.com/JudasDie/SOTS">代码</a></p><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>本文聚焦于孪生跟踪算法的匹配过程，目前主流的互相关操作是启发式设计的，严重依赖人工经验，并且单一的匹配方法无法适应各种复杂的跟踪场景。因此，本文引入了6种新的匹配算子来替代互相关。通过分析这些算子在不同跟踪挑战场景下的适应性，作者发现可以将它们结合起来进行互补，并借鉴NAS思想提出一种搜索方法 binary channel manipulation (BCM) 探索这些匹配算子的最优组合。</p><span id="more"></span><h1 id="Analysis-of-Matching-Operators"><a href="#Analysis-of-Matching-Operators" class="headerlink" title="Analysis of Matching Operators"></a>Analysis of Matching Operators</h1><p>首先介绍本文采用的6种匹配算子，Concatenation, Pointwise-Addition , Pairwise-Relation , FiLM , Simple-Transformer 和Transductive-Guidance。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210808161330.png" alt=""></p><p><strong>Concatenation</strong> 图2(a)，将模板特征$F_z$ pool成 $f_z \in 1 \times 1 \times C$，再与搜索特征$F_x$拼接。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210808161411.png" style="zoom:80%;" /></p><p><strong>Pointwise-Addition</strong> 图2(b)，将上面的拼接换成了对应元素相加。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210808161701.png" style="zoom:80%;" /></p><p><strong>Pairwise-Relation</strong> 图2(c)，类似non-local，将$F_x, F_z$分别reshape成 $H_x W_x \times C$ 和 $C \times H_z W_z $​，然后做矩阵乘法。相当于将模板中的每个元素都与搜索特征的所有元素衡量相似性。​</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210808192301.png" style="zoom:80%;" /></p><p><strong>FiLM</strong> 图2(d)，这个思路源于视觉推理，通过对神经网络的“中间特征”应用仿射变换来自适应地影响网络的输出。如公式5所示，对$f_Z$卷积变换后得到系数$\gamma$和偏置$\beta$，将其作用于搜索特征$F_x$​</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210808202649.png" style="zoom:80%;" /></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210808202908.png" style="zoom:80%;" /></p><p><strong>Simple-Transformer</strong> 图3(a)，就是multi head attention。</p><p><strong>Transductive-Guidance</strong>  图3(b)，源于VOS中的掩码传播机制，即用前一帧的掩码引导当前帧的预测。首先像Pairwise-Relation一样计算模板和搜索特征之间的对应关系(affinity)（公式7），然后用第一帧的伪掩码进行调制（公式8），生成的$G$中的每个位置表示前景的概率，最后将$G$与原始搜索特征$F_x$相加（公式9）。整个过程可以参考该作者另一篇文章<a href="https://arxiv.org/abs/2008.02745v2">OceanPlus</a>。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210808221525.png" style="zoom:80%;" /></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210808222703.png" style="zoom:80%;" /></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210808222743.png" style="zoom:80%;" /></p><p><strong>Analysis</strong></p><p>作者将上述6种算子与传统的互相关匹配分别在OTB100上进行测试，比较其性能，如表1所示。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210808223451.png" alt=""></p><p>对于单一的匹配算子，除了6和7，其他的均取得了和dw-corr相当的性能，甚至2（直接拼接）的性能更好。</p><p>虽然2的性能最优，但并不能保证其在所有跟踪挑战中都是最好的，比如在SV，OPR，OV和LR上就被其他算子超过。作者在图4进一步可视化了响应图，可以看到depthwise cross-correlation (a), Pairwise-relation (d), and Transductive-Guidance (g)更关注目标本身；而the concatenation (b), Pointwise-Addition (c), Simple-Transformer (e), and FiLM (e)则包含了更多上下文信息。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210808225804.png" style="zoom:80%;" /></p><p>上述结果表明不同的匹配算子在不同跟踪场景下具有不同的可靠性，由此想到是否可以将它们结合起来，利用这些特征进行互补。因此，作者接下来提出一种自适应学习的自动选择和组合匹配算子的方法。</p><h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210809101847.png" alt=""></p><p>图5为整体框架，首先将模板和搜索特征送入多个匹配算子构成的搜索空间，生成m个响应特征 $\{ r_1, r_2, … , r_m \}$​，给每个响应特征的每个通道都赋予一个可学习的调制器(manipulator) $\omega_i^j$​​​，表示该通道的贡献。然后引入二元Gumbel-Softmax来离散化这些调制器进行二元决策，训练过程参考DARTS的两层优化。最后根据学好的调制器保留两个匹配算子构建新的跟踪器再次训练。</p><p>下面分别介绍二元的通道调制器(Binary Channel Manipulation)和两层优化(Bilevel Optimization)</p><h2 id="Binary-Channel-Manipulation"><a href="#Binary-Channel-Manipulation" class="headerlink" title="Binary Channel Manipulation"></a>Binary Channel Manipulation</h2><p>Binary Channel Manipulation (BCM) 用于决定匹配算子对目标状态预测的贡献，它给每个匹配响应特征的每个通道都赋予一个可学习的调制器(manipulator) $\omega_i^j$​​，将所有调制后的特征拼接聚合成一个大的特征（有点类似NAS里的supernet）。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210809111736.png" style="zoom:80%;" /></p><p>其中$r_i^j$​ 表示第 i 个响应特征的第 j 个通道。对于每个匹配算子，我们将其对应的所有通道调制器相加得到该算子的potential $p_i$​，这个在后面选择匹配算子时会用到。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210809112703.png" style="zoom:80%;" /></p><p>接下来，将连续的$\omega_i^j$​​转换为离散的，离散后的类别是二值分类，概率向量为$\pi = \{ \pi_1 = \sigma(\omega_i^j), \pi_2 = 1 - \sigma(\omega_i^j) \}$​​。为了能够可微的反向传播，这里利用<a href="https://www.cnblogs.com/initial-h/p/9468974.html">Gumbel-Softmax</a>进行训练。对概率向量$\pi$​​对应的随机变量添加Gumbel噪声$g_k$​​后随机采样</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210809114815.png" style="zoom:80%;" /></p><p>然后将argmax替换为Softmax定义一个连续可微的近似</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210809114947.png" style="zoom:80%;" /></p><p>因为这只是一个二值概率分布，公式14带入$\pi$的表达式后可以被简化成</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210809115114.png" style="zoom:80%;" /></p><p>推导如下：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210809120120.png" alt=""></p><p>作者这里令$\tau=1, g_k=0$​​，对于离散采样的样本d，前向传播时使用硬值(0或1)，反向传播采用软值获得梯度。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210809120823.png" style="zoom:80%;" /></p><h2 id="Bilevel-Optimization"><a href="#Bilevel-Optimization" class="headerlink" title="Bilevel Optimization"></a>Bilevel Optimization</h2><p>优化的目标有两部分，一个是调制器参数$\omega$，另一个是匹配网络的卷积层参数$\theta$。这里借鉴DARTS的优化方法，利用训练集优化$\theta$，利用验证集优化$\omega$，记作</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210809154925.png" style="zoom:80%;" /></p><p>为了加速收敛，将其简化成</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210809155027.png" style="zoom:80%;" /></p><p>按上述方式训练后，保留potential $p_i$最高的两个匹配算子构成新的跟踪器重新训练。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h2><p>训练过程分成匹配网络的搜索和新跟踪器的训练两个阶段，第一阶段用Bilevel Optimization搜索最优的匹配网络组合，第二阶段用优化的匹配网络构建一个新的跟踪器进行常规的训练。搜索算法为分类和回归分支确定不同的匹配网络。经过第一阶段的训练后，分类分支使用Simple-Transformer和FiLM，而回归分支使用FiLM 和 Pairwise-Relation。</p><h2 id="State-of-the-art-Comparison"><a href="#State-of-the-art-Comparison" class="headerlink" title="State-of-the-art Comparison"></a>State-of-the-art Comparison</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210809163533.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210809163718.png" style="zoom:80%;" /></p><h2 id="Ablation-and-Analysis"><a href="#Ablation-and-Analysis" class="headerlink" title="Ablation and Analysis"></a>Ablation and Analysis</h2><p><strong>One or Many Manipulators</strong> 本文给匹配算子的每个通道都赋予一个调制器，作者也尝试了给每个匹配算子只赋予一个标量的调制器，性能会有所下降 OTB100 69.5，LaSOT 54.7。</p><p><strong>Random Search</strong> 随机选择匹配算子同样会使性能下降，OTB100 69.1，LaSOT 53.2。</p><p><strong>NAS-like Matching Cell</strong> 用DARTS的方法构建一个有向无环图进行搜索，如图7所示，同样不如本文的结果，且速度会大幅下降。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210809165018.png" style="zoom:80%;" /></p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><ol><li>DW-Corr并不是孪生跟踪算法的最优选择，本文设计了6种新的匹配算子；</li><li>本文用一种自动搜索的方式组合出最优的匹配网络。</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/46-AutoMatch/20210808103150.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2108.00803&quot;&gt;论文&lt;/a&gt; &lt;a href=&quot;https://github.com/JudasDie/SOTS&quot;&gt;代码&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h1&gt;&lt;p&gt;本文聚焦于孪生跟踪算法的匹配过程，目前主流的互相关操作是启发式设计的，严重依赖人工经验，并且单一的匹配方法无法适应各种复杂的跟踪场景。因此，本文引入了6种新的匹配算子来替代互相关。通过分析这些算子在不同跟踪挑战场景下的适应性，作者发现可以将它们结合起来进行互补，并借鉴NAS思想提出一种搜索方法 binary channel manipulation (BCM) 探索这些匹配算子的最优组合。&lt;/p&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="ICCV2021" scheme="https://www.zjp97.top/tags/ICCV2021/"/>
    
    <category term="matching" scheme="https://www.zjp97.top/tags/matching/"/>
    
  </entry>
  
  <entry>
    <title>HiFT: Hierarchical Feature Transformer for Aerial Tracking</title>
    <link href="https://www.zjp97.top/tracking/45-HiFT/"/>
    <id>https://www.zjp97.top/tracking/45-HiFT/</id>
    <published>2021-08-07T03:32:22.000Z</published>
    <updated>2022-06-11T03:00:51.628Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/45-HiFT/20210807114323.png" alt=""></p><p><a href="https://arxiv.org/abs/2108.00202">论文</a> <a href="https://github.com/vision4robotics/HiFT">代码</a></p><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>现有的孪生跟踪算法大多是基于相似得分图对目标对象进行分类和回归，使用单一的相似图会降低复杂场景下的定位精度，而像SiamRPN++那样单独使用多个相似图分别进行预测又会引入较大计算负担，不适用于移动设备。因此，本文提出一种 hierarchical feature transformer (HiFT) 对多个层级的相似图进行融合，既可以捕获全局的依赖关系，又可以高效地学习多层级特征之间的依赖关系。</p><p>在介绍本文方法前，我们先分析经典的transformer架构应用于目标跟踪任务中的难点。</p><ol><li>预定义的(或学习的)解码query在面对任意跟踪对象时很难保持有效性；</li><li>transformer难以处理小目标（参考deform DETR）。</li></ol><span id="more"></span><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/45-HiFT/20210807215745.png" alt=""></p><p>图2为整体框架，分成特征提取，transformer和预测头三部分。特征提取采用高效的alexnet，最后三层特征输入到transformer中，预测头采用类似FCOS的三分支预测（分类、回归、定位质量）。下面详细介绍本文提出的Hierarchical Transformer。</p><h2 id="Hierarchical-Feature-Transformer"><a href="#Hierarchical-Feature-Transformer" class="headerlink" title="Hierarchical Feature Transformer"></a>Hierarchical Feature Transformer</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/45-HiFT/20210807221543.png" style="zoom:80%;" /></p><p>HiFT包含高分辨率特征编码和低分辨率特征解码，前者学习不同特征层和空间信息之间的相互依赖关系，以提高对不同尺度(特别是低分辨率)目标的关注；而后者聚合了来自低分辨率深层特征的语义信息。这种全局上下文和层次特征之间的相互依赖大大提升了对复杂跟踪场景的适应能力。</p><p>transform的输入是三层不同尺度的互相关相似图，如公式1所示。图3中的$M_3’$ 和 $M_4’$ 则是加上了位置编码。​</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/45-HiFT/20210807225118.png" style="zoom:80%;" /></p><p><strong>Feature Encoding</strong></p><p>首先对$M_3’$ 和 $M_4’$ 进行相加和归一化的融合，得到$M_E^1 = Norm(M_3’+M_4’)$；然后经过multi-head attention得到 $M_E^2 \in WH \times C$，attention矩阵中同时包含了$M_3’$ 和 $M_4’$的多尺度信息，注意这里MHA中Q, K, V的输入差异；</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/45-HiFT/20210808094324.png" style="zoom:80%;" /></p><p>此外还额外增加了一个调制层 (modulation layer)，探索$M_4’$ 和 $M_E^3$ 之间的空间信息。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/45-HiFT/20210808095158.png" style="zoom:80%;" /></p><p><strong>Feature Decoding</strong></p><p>decoder部分和标准的transformer类似，差别在于输入的查询向量不是预定义的query，而是低分辨率的特征$M_5 \in WH \times C$​​，并且无需位置编码。</p><p>作者在实验中堆叠了一个编码和两个解码结构。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Evaluation-on-Aerial-Benchmarks"><a href="#Evaluation-on-Aerial-Benchmarks" class="headerlink" title="Evaluation on Aerial Benchmarks"></a>Evaluation on Aerial Benchmarks</h2><p>本文的应用环境是无人机跟踪，所以测试数据集均在无人机数据集测试。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/45-HiFT/20210808100142.png" style="zoom:80%;" /></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/45-HiFT/20210808100159.png" style="zoom:80%;" /></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/45-HiFT/20210808100237.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/45-HiFT/20210808100251.png" style="zoom:80%;" /></p><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/45-HiFT/20210808100338.png" style="zoom:80%;" /></p><p>图5中OT表示标准transformer结构，FT表示用特征取代解码器中的object query，PE表示在解码输入中加入位置编码，RL表示在GT的矩形框内采样正样本（本文用的椭圆采样策略）。可以看到，OT使得性能下降，证明预定义的object query不适用于目标任意的跟踪任务；增加了PE后相比不用PE性能大幅下降；使用RL性能同样大幅下降，这样看上去似乎label assign策略的影响都要大于HFT了。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/45-HiFT/20210808102502.png" style="zoom:80%;" /></p><p>图5展示了本文方法在快速运动、低分辨率和遮挡等场景均可以更聚焦目标。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/45-HiFT/20210808102632.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/45-HiFT/20210808102731.png" alt=""></p><p>速度一骑绝尘，大于100FPS，并且使用alexnet的性能超过了很多resnet50的算法。并且作者在嵌入式平台NVIDIA AGX Xavier中实验也达到了31.2FPS(未使用tensorrt)，非常适合应用。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/45-HiFT/20210807114323.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2108.00202&quot;&gt;论文&lt;/a&gt; &lt;a href=&quot;https://github.com/vision4robotics/HiFT&quot;&gt;代码&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h1&gt;&lt;p&gt;现有的孪生跟踪算法大多是基于相似得分图对目标对象进行分类和回归，使用单一的相似图会降低复杂场景下的定位精度，而像SiamRPN++那样单独使用多个相似图分别进行预测又会引入较大计算负担，不适用于移动设备。因此，本文提出一种 hierarchical feature transformer (HiFT) 对多个层级的相似图进行融合，既可以捕获全局的依赖关系，又可以高效地学习多层级特征之间的依赖关系。&lt;/p&gt;
&lt;p&gt;在介绍本文方法前，我们先分析经典的transformer架构应用于目标跟踪任务中的难点。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;预定义的(或学习的)解码query在面对任意跟踪对象时很难保持有效性；&lt;/li&gt;
&lt;li&gt;transformer难以处理小目标（参考deform DETR）。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="transform" scheme="https://www.zjp97.top/tags/transform/"/>
    
    <category term="ICCV2021" scheme="https://www.zjp97.top/tags/ICCV2021/"/>
    
    <category term="UAV" scheme="https://www.zjp97.top/tags/UAV/"/>
    
  </entry>
  
  <entry>
    <title>Domain Adaptive SiamRPN++ for Object Tracking in the Wild</title>
    <link href="https://www.zjp97.top/tracking/44-DASiamRPN/"/>
    <id>https://www.zjp97.top/tracking/44-DASiamRPN/</id>
    <published>2021-07-01T08:54:14.000Z</published>
    <updated>2022-06-11T03:00:51.628Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210701165637.png" alt=""></p><p><a href="https://arxiv.org/abs/2106.07862">论文</a></p><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>基于孪生网络的跟踪算法均假定训练和测试数据遵循相同的分布，然而在正常图像上训练的跟踪器并不能保证在其他领域的数据上（如雨雾天气的序列）也表现良好，即存在域偏移（domain shift）问题，如图1和图2。作者称本文是首次将域分布差异问题引入视觉跟踪领域。</p><p>针对这一问题，本文提出一种域自适应方法，包括Pixel Domain Adaptation (PDA) 和 Semantic Domain Adaptation (SDA)。PDA分别对（不同域的）模板和搜索图像的特征对齐，消除天气、光照等引起的像素级域偏移；SDA将（不同域的）跟踪目标的特征表达对齐，以消除语义级的域偏移。二者均通过对抗训练的方式学习域分类器，域分类器强制网络学习域不变的特征表达，从而实现域自适应。</p><span id="more"></span><p>最后作者在有雾和红外序列两个不同域的数据集上进行了验证。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702104303.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702104337.png" alt=""></p><h1 id="Theoretical-Preliminaries"><a href="#Theoretical-Preliminaries" class="headerlink" title="Theoretical Preliminaries"></a>Theoretical Preliminaries</h1><p>最简单粗暴的方法就是搜集许多具有不同域的标注训练数据，但这显然不现实。因此我们的目标是针对<strong>无监督域自适应场景</strong>（即源域有标记而目标域未标记），使跟踪器在源域和目标域上都表现良好，而不需要额外的标注成本。一种通用的方案就是学习域不变（domain-invariant）的特征表达来缩小不同域之间的差异。作者利用 <em>A</em>-distance理论和概率分析来实现这一目的，下面先简单介绍这些概念。</p><h2 id="A-distance"><a href="#A-distance" class="headerlink" title="A-distance"></a><em>A</em>-distance</h2><p>给定源域 $S$ 和目标域 $T$， <em>A</em>-distance可以用于衡量两个域样本分布的差异，定义如下：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702113837.png" alt=""></p><p>其中h表示域分类器，$h(x)\rightarrow 0$表示样本x属于源域，$h(x)\rightarrow 1$表示样本x属于目标域。$min \ error(h(x))$表示理想域分类器的预测误差，显然，误差越小（越容易区分）表示域差异越大。现在要最小化域差异$d_A(S,T)$以实现特征对齐，等价于要最大化理想域分类器误差，即</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702114945.png" alt=""></p><p>其中 f 表示样本 x 的特征表达。公式(3)是特征提取器 f 和域分类器 h 之间的minimax优化问题。这个怎么理解呢？其实类似GAN，域分类器 h 需要尽可能区分不同域的样本，而特征提取 f 需要欺骗分类器让其难以区分不同域，即让 f 提取到域不变特征。</p><p>作者在优化这个问题时采用 Gradient Reversed Layer (GRL)，如下图所示，在梯度从域分类器传到特征提取之前将其取负号反转，希望粉色部分的参数向$L_d$减小的方向优化，绿色部分的参数向$L_d$增大的方向优化，用一个网络一个优化器就实现了两部分有不一样的优化目标，形成<strong>对抗</strong>的关系。（参考<a href="https://www.zhihu.com/question/266710153/answer/1338864403">Gradient Reversal Layer指什么？ - Just4Fan的回答 - 知乎</a> )</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702115615.png" alt=""></p><h2 id="Probabilistic-Analysis-for-Object-Tracker"><a href="#Probabilistic-Analysis-for-Object-Tracker" class="headerlink" title="Probabilistic Analysis for Object Tracker"></a>Probabilistic Analysis for Object Tracker</h2><p>作者将跟踪问题看成一个后验概率 $P(S,B|Z,X)$，即给定模板Z和搜索区域X，预测分类得分S和目标框B。由于域偏移的存在，源域的联合概率分布$P_S(S,B,Z,X)$与目标域的联合概率分布$P_T(S,B,Z,X)$是不同的。</p><p><strong>Pixel Domain Adaptation</strong> 根据贝叶斯公式，可以将联合概率分布分解成：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702150940.png" alt=""></p><p>其中$i \in \{S,T\}$。条件概率$P(S,B|Z,X)$相当于跟踪器的分类回归分支，我们假设这部分对于不同域是一样，那么域偏移主要来自模板和搜索图像的特征提取$P(Z,X)$。为了消除域偏移，需要另Siamese网络提取域不变的特征映射，即$P_S(Z,X) = P_T(Z,X)$</p><p><strong>Semantic Domain Adaptation</strong> 上面PDA解决天气或光照引起的全局域偏移，但不同域的目标还存在外观和类别的变化，因此还需要考虑目标语义的域偏移。类似的，可以将联合概率分解成：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702151847.png" alt=""></p><p>同样假设条件概率$P(S | B,Z,X)$对于不同域是一样的，那么域偏移主要来自$P(B,Z,X)$。为了消除偏移，需要$P_S(B,Z,X) = P_T(B,Z,X)$，表示给定了模板、搜索区域以及对应的目标框，跟踪目标的特征表达要是一样的。考虑到目标域是没有真实框标注的，因此这里统一采用RPN的预测框表示B。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702152654.png" alt=""></p><p>图3是整体方法框架，根据上一节的<em>A</em>-distance理论以及概率分析，作者提出了PDA和SDA两个模块。其中PDA针对的是孪生网络的整体特征，SDA针对的是预测框内的目标特征。</p><h2 id="Pixel-Domain-Adaptation"><a href="#Pixel-Domain-Adaptation" class="headerlink" title="Pixel Domain Adaptation"></a>Pixel Domain Adaptation</h2><p>PDA包括模板对齐和搜索区域对齐，目的是通过域分类器和Siamese网络之间的minimax优化来混淆跨域的特征映射。域分类器由Conv+MaxPool+FC组成，FC层对每个像素进行二值分类，损失函数为：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702154430.png" alt=""></p><p>m,n为像素位置，D是标签，p是预测结果。然后按照公式3的minimax优化，需要对域分类器参数最小化该损失，对siamese特征提取参数最大化该损失，即</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702154740.png" alt=""></p><p>$\omega_{pda}$表示PDA域分类器参数，$\varphi$表示孪生网络参数。域分类器的参数更新方向与减少域分类损失的方向相同，这与普通的训练方法相同；而Siamese网络的参数更新方向被反转（GRL），这正是增加域分类损失的方向，二者形成对抗。</p><h2 id="Semantic-Domain-Adaptation"><a href="#Semantic-Domain-Adaptation" class="headerlink" title="Semantic Domain Adaptation"></a>Semantic Domain Adaptation</h2><p>由于不同域的类别、视角和姿态的变化，跟踪目标会发生明显的变化，SDA强制跟踪目标的特征表示在语义上是域不变的。具体过程为，通过ROI Align提取预测框内的multi-layer的ROI特征，域分类器（两层FC）对其进行分类，GRL放在域分类器和ROI Align之间。域分类损失为：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702155824.png" alt=""></p><p>同样以对抗的方式训练SDA</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702155913.png" alt=""></p><p>$\omega_{sda}$表示SDA域分类器参数，$\varphi$表示孪生网络参数。无论跟踪目标来自源域还是目标域，目标的域不变特征都能在分数图中获得较高的响应。</p><p>最后总的训练损失包括孪生跟踪器的损失和域自适应损失</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702160122.png" alt=""></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>训练时使用LaSOT作为源域数据，Foggy GOT-10k和LSOTB-TIR作为目标域数据。Foggy GOT-10k是作者生成的有雾数据集，LSOTB-TIR是红外数据集，注意二者作为目标域数据训练时是没有标注的。模板和搜索图像的裁剪通过运行现有的SiamRPN++对目标域数据集获取伪标签得到的。</p><p>表1-4展示了正常天气到有雾的跨域和RGB到红外的跨域的跟踪结果。这里的比较方式有点迷，作者列出每个epoch的结果证明性能的提升，但如果只关注最好的结果发现的性能提升其实不明显。比如Foggy VOT2018 0.211 v.s. 0.218，LSOTB-TIR 0.543 v.s. 0.547。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702162344.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702162404.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702162413.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702162420.png" alt=""></p><p>消融实验也呈现一样的结果，如果只比较最好的性能，单独的PDA和SDA甚至不如baseline。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702161634.png" alt=""></p><p>其他的一些可视化结果。图6将特征压缩到平面证明了源域和目标域的特征混淆在一起，证明了域不变特征。图7证明了提出的方法在跨域性能表现良好的同时，不会损失在源域上的性能。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702161754.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702161803.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210702161810.png" alt=""></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/44-DASiamRPN++/20210701165637.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.07862&quot;&gt;论文&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h1&gt;&lt;p&gt;基于孪生网络的跟踪算法均假定训练和测试数据遵循相同的分布，然而在正常图像上训练的跟踪器并不能保证在其他领域的数据上（如雨雾天气的序列）也表现良好，即存在域偏移（domain shift）问题，如图1和图2。作者称本文是首次将域分布差异问题引入视觉跟踪领域。&lt;/p&gt;
&lt;p&gt;针对这一问题，本文提出一种域自适应方法，包括Pixel Domain Adaptation (PDA) 和 Semantic Domain Adaptation (SDA)。PDA分别对（不同域的）模板和搜索图像的特征对齐，消除天气、光照等引起的像素级域偏移；SDA将（不同域的）跟踪目标的特征表达对齐，以消除语义级的域偏移。二者均通过对抗训练的方式学习域分类器，域分类器强制网络学习域不变的特征表达，从而实现域自适应。&lt;/p&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="Domain adaption" scheme="https://www.zjp97.top/tags/Domain-adaption/"/>
    
  </entry>
  
  <entry>
    <title>Crop-Transform-Paste: Self-Supervised Learning for Visual Tracking</title>
    <link href="https://www.zjp97.top/tracking/43-Crop-Transform-Paste/"/>
    <id>https://www.zjp97.top/tracking/43-Crop-Transform-Paste/</id>
    <published>2021-06-30T12:27:54.000Z</published>
    <updated>2022-06-11T03:00:51.628Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/43-Crop-Transform-Paste/20210630203150.png" alt=""></p><p><a href="https://arxiv.org/abs/2106.10900">论文</a></p><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>现有的跟踪方法依赖大量高质量标注数据集训练，但很多复杂挑战场景并未出现在训练数据中。为了避免昂贵的人工标注，本文提出一种<strong>自监督</strong>学习方式。作者设计了Crop-Transform-Paste，通过模拟跟踪过程中各种场景变化，合成足够的训练数据。在合成数据中目标状态是已知的，因此无需人工标注。</p><p>本文提出的自监督方法可以无缝集成到任何现有的跟踪框架中进行训练，实验证明提出的方法：1）在<strong>少样本</strong>跟踪场景中取得比监督学习更好的性能；2）能够处理目标形变、遮挡、背景干扰等各种挑战；3）可以与监督学习相结合，进一步提高性能。</p><span id="more"></span><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/43-Crop-Transform-Paste/20210701002056.png" alt=""></p><p>整体框架如图2所示，作者利用Crop-Transform-Paste来合成训练样本。给定一个只有第一帧有标注的跟踪序列，首先裁剪目标区域（crop）；然后对目标进行跟踪相关的变换，模拟跟踪过程中的目标变化（transform）；最后将变换后的目标粘贴到不同帧的不同位置上，模拟目标周围的背景(上下文)变化（paste）。</p><p><strong>Crop</strong></p><p>根据给定的bbox裁剪目标patch。其实也可裁剪任意的patch，但这样缺乏有效的语义特征和边界。</p><p><strong>Transform</strong></p><p>如图2中间部分所示，采用shear, blur, cutout, color jittering, rescale 来模拟跟踪过程中的形变、运动模糊、遮挡、光照和尺度变换。其中跟踪场景中出现的模糊通常是由相机抖动或快速运动引起的，因此采用shaking blur（滤波器核仅在水平和垂直中心线非零）。shaking blur不和cutout同时使用，因为cutout填充的黑块会影响blur，而且现实中又模糊又遮挡的场景也比较少见。</p><p><strong>Paste</strong></p><p>如图2右边部分所示，把变换后的目标块随机粘贴到其他帧的不同位置上。为了减轻直接粘贴在边界处产生的尖锐突变，作者在目标patch外增加了pad，并在pad区域使用混合相加以平滑过渡。此外，还可以从其他视频中选择相似目标块粘贴到背景上，模拟背景（相似物）干扰，作者将其称为Similar-Patch Paste (SPatchP)。</p><p><strong>Discussions</strong></p><p>利用上述Crop-Transform-Paste可以生成各种训练样本而无需人工标注，因此属于自监督学习。作者讨论了它与典型的关注表征学习的自监督学习（SSL-RL）之间的差异。</p><ol><li>不同的框架。SSL-RL按照contrastive learning框架训练一个独立的特征学习模块，而本文的方法聚焦在生成无需人工标注的训练数据，因此可以嵌入到任何现有跟踪框架中；</li><li>不同的data transformation技术。SSL-RL通过data transformation构造训练对来训练其特征学习模块，而本文设计了特定的跟踪相关的transformations来模拟跟踪过程中的场景变化。由于目标不同，采用了不同的transformation技术。比如后续实验中发现翻转(flip)会产生不利的影响。</li></ol><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>本文的实验非常充分，对比了不同的跟踪器，不同的数据变换方式，训练的数据量，以及和有监督方式的结合等。</p><h2 id="Analysis-of-Crop-Transform-Paste"><a href="#Analysis-of-Crop-Transform-Paste" class="headerlink" title="Analysis of Crop-Transform-Paste"></a>Analysis of Crop-Transform-Paste</h2><p><strong>Selection of base target patch</strong> 表1对比了不同的目标块生成方式，SS-random表示随机裁剪，SS-annotated表示根据标注裁剪。显然后者优于前者，因为标注目标包含更有效的语义特征和更多的边界信息。并且自监督学习的性能和监督学习（Su）也比较接近。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/43-Crop-Transform-Paste/20210701114700.png" alt=""></p><p><strong>Selection of images to paste onto</strong> 表2对比了不同的粘贴方式。裁剪的目标粘贴到相同的视频序列中优于粘贴到其他视频序列中。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/43-Crop-Transform-Paste/20210701115051.png" alt=""></p><h2 id="Ablation-of-Each-Transformation"><a href="#Ablation-of-Each-Transformation" class="headerlink" title="Ablation of Each Transformation"></a>Ablation of Each Transformation</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/43-Crop-Transform-Paste/20210701151326.png" alt=""></p><p>作者做了两组实验分析每种变换的作用，第一组在只有shift变换的baseline1上添加一个变换来显示性能增益，第二组从使用所有变换的baseline2中删除一个变换来呈现性能下降。表3显示rescale和shift的作用最大，说明尺度和平移变换是最常见的。blur和color jitter单独使用不够好，可能过拟合，而与其他变换结合使用时有益的。flip不管单独使用还是结合其他变换一起均带来负面影响，作者认为它破坏了目标的边界信息（为什么？）。</p><h2 id="Advantages-of-the-Proposed-Method"><a href="#Advantages-of-the-Proposed-Method" class="headerlink" title="Advantages of the Proposed Method"></a>Advantages of the Proposed Method</h2><p><strong>Challenge-oriented transformations</strong> Crop-Transform-Paste的一个明显优势在于可以针对跟踪挑战定制变换方法，比如1) rescale for Scale Variation (SV); 2) Cutout for Occlusion (OCC); 3) Blur for Motion Blur (MB) and 4) Similar-Patch Paste (SPatchP) for Background Clutter (BC)。如图3所示，针对OTB100中的各种挑战，使用对应的变换方法能够有效提升性能。在遮挡、运动模糊和背景杂波的测试序列上，cutout, blur, 和 similar-patch paste 在SS+Su模式下的提升甚至超过单独SS模式下的提升，说明这些场景的标注数据很少。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/43-Crop-Transform-Paste/20210701153651.png" alt=""></p><p><strong>Data efficiency</strong> 图1和图4对比了自监督学习（SS）、监督学习（Su）和二者结合（SS+Su）在使用不同数量的标注样本训练时的性能。可以看到，SS仅需要很少的标注样本0.6K就能收敛到一个不错的结果。并且二者结合可以进一步提升监督学习的性能。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/43-Crop-Transform-Paste/20210701155122.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/43-Crop-Transform-Paste/20210701155137.png" alt=""></p><p><strong>Integration with supervised methods</strong> </p><p>表4用SiamRPN++和DiMP证明提出的自监督方法可以嵌入到现有的有监督框架中进一步提升性能。DiMP提升较小是因为它本身有个在线训练模块能够一定程度 处理跟踪目标或场景的变化。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/43-Crop-Transform-Paste/20210701155221.png" alt=""></p><h2 id="Evaluation-against-Unsupervised-Methods"><a href="#Evaluation-against-Unsupervised-Methods" class="headerlink" title="Evaluation against Unsupervised Methods"></a>Evaluation against Unsupervised Methods</h2><p>最后是和无监督方法的对比，同样是更优的。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/43-Crop-Transform-Paste/20210701155442.png" alt=""></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>本文提出了一种基于Crop-Transform-Paste的自监督学习方法，用于在没有人工标注的场景下训练跟踪模型。方法简洁，实验充分。个人认为这篇文章的重点在于自监督而不是数据增广，毕竟增广那部分只是单纯做了些常见的变换，核心还是利用这些变换有针对地合成各类样本。</p><p>但就带来一个新的思考：如果我不用合成样本的自监督学习，直接把所有的变换全部作为数据增广加到有监督学习的训练中效果如何？以一个视频序列举例，本文的合成样本只利用视频的某一帧进行变换再粘贴到其他帧上生成N个样本；而传统的监督方法是获取视频中任意N帧得到N个样本。那假如对这N个不同的真实样本直接做数据增广，理论上能够覆盖的样本变换范围应该更大，效果是不是会更好？</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/43-Crop-Transform-Paste/20210630203150.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.10900&quot;&gt;论文&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h1&gt;&lt;p&gt;现有的跟踪方法依赖大量高质量标注数据集训练，但很多复杂挑战场景并未出现在训练数据中。为了避免昂贵的人工标注，本文提出一种&lt;strong&gt;自监督&lt;/strong&gt;学习方式。作者设计了Crop-Transform-Paste，通过模拟跟踪过程中各种场景变化，合成足够的训练数据。在合成数据中目标状态是已知的，因此无需人工标注。&lt;/p&gt;
&lt;p&gt;本文提出的自监督方法可以无缝集成到任何现有的跟踪框架中进行训练，实验证明提出的方法：1）在&lt;strong&gt;少样本&lt;/strong&gt;跟踪场景中取得比监督学习更好的性能；2）能够处理目标形变、遮挡、背景干扰等各种挑战；3）可以与监督学习相结合，进一步提高性能。&lt;/p&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="Data augmentation" scheme="https://www.zjp97.top/tags/Data-augmentation/"/>
    
    <category term="Self-supervised" scheme="https://www.zjp97.top/tags/Self-supervised/"/>
    
  </entry>
  
  <entry>
    <title>强化学习在目标跟踪中的应用</title>
    <link href="https://www.zjp97.top/tracking/42-rl-tracking/"/>
    <id>https://www.zjp97.top/tracking/42-rl-tracking/</id>
    <published>2021-06-08T08:28:12.000Z</published>
    <updated>2022-06-11T03:00:51.630Z</updated>
    
    <content type="html"><![CDATA[<p>强化学习讨论的问题是智能体(agent) 如何在一个复杂不确定的环境(environment) 里去最大化它能获得的奖励。 今天介绍三篇关于强化学习在目标跟踪中的工作，分别利用强化学习来决策使用的特征，多个跟踪器的切换以及是否更新模板。</p><p>论文列表：</p><ul><li><a href="https://arxiv.org/abs/1708.02973">Learning Policies for Adaptive Tracking with Deep Feature Cascades</a></li><li><a href="https://proceedings.neurips.cc//paper/2020/file/885b2c7a6deb4fea10f319c4ce993e02-Paper.pdf">Online Decision Based Visual Tracking via Reinforcement Learning</a></li><li><a href="https://arxiv.org/abs/2004.07538">Fast Template Matching and Update for Video Object Tracking and Segmentation</a></li></ul><span id="more"></span><h1 id="强化学习方法"><a href="#强化学习方法" class="headerlink" title="强化学习方法"></a>强化学习方法</h1><p>强化学习包含环境, 动作和奖励三部分, 其本质是agent 通过与环境的交互, 使得其作出的action所得到的总奖励达到最大, 或者说是期望最大。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210608202931.png" style="zoom: 50%;" /></p><p>强化学习方法主要可以分为Value-Based，Policy-Based以及二者结合的Actor-Critic方法。Value-Based方法通过Temporal Difference (TD) Learning学习动作价值函数；Policy-Based方法通过Policy Gradient学习策略函数；而Actor-Critic方法将二者结合，actor学习一个策略来得到尽量高的回报，critic对当前策略的值函数进行估计，即评估actor的好坏。</p><p>关于强化学习的更多细节可以参考王树森老师的视频课程<a href="https://www.bilibili.com/video/BV12o4y197US">【王树森】深度强化学习(DRL)</a></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210608203415.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210608204616.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210608204711.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210608204741.png" alt=""></p><hr><h1 id="Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades"><a href="#Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades" class="headerlink" title="Learning Policies for Adaptive Tracking with Deep Feature Cascades"></a>Learning Policies for Adaptive Tracking with Deep Feature Cascades</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>第一篇来自ICCV2017，出发点是不同复杂程度的跟踪场景对特征的需求是不同的，对于简单场景使用浅层特征（甚至像素特征）就能处理，而对于一些复杂场景才需要具有更强语义信息的深度特征。这个<strong>自适应决策</strong>的问题可以通过基于Q-learning的强化学习完成，如图1所示，学习一个agent来判断当前特征是否已经可以以较高的置信度定位目标，还是需要继续计算更深层的特征来寻找目标。</p><p>这样对简单目标提前终止的策略可以大幅提升推理速度，相比baseline平均速度提升了大约10倍，GPU速度158.9FPS，并且在cpu上也能以23.2FPS的速度接近实时运行。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210629104958.png" alt=""></p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>首先定义一些公式符号，孪生网络每一层的互相关层定义：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210629121458.png" alt=""></p><p>其中$\varphi_l$ 表示第 $l$ 层的特征，$F_l$ 表示第 $l$ 层的互相关结果。</p><p>整体框架如图2所示，在每一层互相关结果$F_l$后面接一个Q-Net，用于判断是否在该层停止，或者调整预测框的形状并继续使用下一层特征。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210629121328.png" alt=""></p><p>agent采用强化学习的方式训练，基本元素包括状态(state S) ，动作(action A)和奖励(reward R)。在每一个step即第l层中，agent根据当前状态$S_l$ 采取动作$A_l$来决定是否调整预测框或者在该层停止并输出结果，动作$A_l$的目的是减少预测的框的不确定性。训练时根据预测框与GT的IOU给出相应的奖励$R_l$（有正有负），通过最大化期望奖励，agent能学到最好的决策来采取行动，在精度和效率上取得平衡。</p><p><strong>Actions:</strong> 包括7个各向异性的的尺度变换和一个stop动作。7个尺度变换里包括2个全局的缩放和4个局部缩放，缩放比例为0.2。还有一个不缩放(no scaling)的动作，这一操作用于在当前响应图不明确或无法做出决策时推迟决策。</p><p><strong>States:</strong> 状态是一个包含响应图$F’_l$和历史动作$h_l$的二元组 $(F’_l,h_l)$。$F’_l$使用的是当前层和之前所有层响应图的平均，相当于结合了浅层的细节和深层的语义。$h_l$包含历史4个动作的向量，每个动作是8维的one-hot的向量，所以$h_l$总共是32维。</p><p><strong>Rewards:</strong> 奖励函数$R(S_{l-1},S_l)$反应了采取动作$A_l$后，从状态$S_{l-1}$到状态$S_l$的定位精度提升（或下降），精度采用IOU衡量，奖励函数计算如下：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210629174619.png" style="zoom:80%;" /></p><p>当动作不为stop时，若该动作能使IOU增大，则奖励+1，否则惩罚-1。若采取任何尺度变换都不能进一步提升IOU或者已经到达最后一层了，则采取stop动作，此时以IOU阈值0.6来决定奖惩。</p><p><strong>Deep Q-learning</strong>：本文使用value-based的DQN来选择动作，该方法需要学习一个动作-价值函数$Q(S_l,A_l)$, 选择能够使得Q最大的动作A。Q函数用网络模拟，如图2虚线框所示，包含两个128维的FC层，输出对应8维动作的回报。训练时采用TD learning进行迭代。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210629181015.png" style="zoom:80%;" /></p><p>其中R表示当前奖励，$Q(S‘,A’)$表示未来总的回报，$\gamma$是折扣因子。</p><p>测试阶段无需奖励，只根据Q函数调整预测框直到输出stop动作。作者在OTB50上验证平均只需要2.1步输出结果，即只需要两层网络，因此可以大幅提速。</p><p>此外，这套策略还可以集成一些简单的特征，比如像素特征和hog特征，计算更快。</p><p>图3展示了一些early stop的例子，如跟踪清晰的人脸时只需C1-C2的特征，但跟踪一个模糊的人脸则需要更深层的C5特征。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210629194612.png" alt=""></p><hr><h1 id="Online-Decision-Based-Visual-Tracking-via-Reinforcement-Learning"><a href="#Online-Decision-Based-Visual-Tracking-via-Reinforcement-Learning" class="headerlink" title="Online Decision Based Visual Tracking via Reinforcement Learning"></a>Online Decision Based Visual Tracking via Reinforcement Learning</h1><h2 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h2><p>第二篇来自NIPS2020。目前主流的跟踪方法有基于检测的和基于模板匹配的，二者各有优劣。基于检测的方法容易受遮挡等影响错误更新网络，但是能适应形变；而基于模板匹配的方法只利用第一帧模板，与上述情况刚好相反。很自然会想到将二者结合，但这是两套完全不同的跟踪原理，直接融合并不能同时收敛到各自的最优解。因此本文提出了一个基于分层强化学习(HRL)的在线决策机制。决策机制实现了一种智能切换策略，其中检测器和模板跟踪器必须相互竞争，以便在它们擅长的不同场景中进行跟踪。</p><h2 id="Method-1"><a href="#Method-1" class="headerlink" title="Method"></a>Method</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210629205511.png" alt=""></p><p>整体框架如图2所示，包括决策模块和跟踪模块。决策模块是一个Actor-Critic（or Option-Critic？）结构，包括switch network和termination network。首先将初始帧模板和上一帧跟踪结果送入switch network，输出一个二元信号选择跟踪器。跟踪器结果送入termination network，输出终止当前跟踪器的概率。注意这里终止之后并不一定切换到另一个跟踪器，因为并不能保证另一个就更好，而是要经过switch network重新选择。</p><p><strong>Decision Module</strong></p><p>给定一组状态$S$和动作$A$，马尔可夫选项$\omega \in \Omega$ 包括三部分：intra-option policy $\pi: S \times A \rightarrow [0,1]$, termination condition $\beta: S^{+}  \rightarrow [0,1]$, initiation set $I \subseteq  S$。当option $\omega$选定后，根据$\pi_{\omega}$选择相应的动作，直到终止函数 $\beta_{\omega}$ 判断终止。</p><p>这是一个标准的Option-Critic结构，一大堆公式就省略了。但是最后作者却用Actor-Critic去解释图2，即switch network是option-value函数，作为Critic来评价option，并且为termination network提供更新梯度（参考上面Actor-Critic的ppt）。termination network作为Actor评估正在使用的跟踪器性能，以决定它是否应该在当前帧终止。</p><p>switch network的奖励函数定义如下：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210629214943.png" style="zoom:80%;" /></p><p>其中$P$表示被选中的跟踪器的跟踪框与GT的IOU，$P^*$表示未被选中的跟踪器的跟踪框与GT的IOU，$D_{IoU}$表示两者的差。按照公式7总共有3种情况：一个成功一个失败，两个均成功，两个均失败。</p><p>训练按照Actor-Critic训练，Critic使用贝尔曼方程（TD learning）更新，Actor使用策略梯度更新。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210629215908.png" alt=""></p><p>图5是一个可视化结果，其中终止概率1表示终止，0表示保持不变。可以看到初始是SiamFC（黄框）表现较好；当发生形变后，FCT的价值函数更大，终止概率趋近1，跟踪器切换；之后一直都是FCT表现更好，因此终止概率始终在0附近。</p><hr><h1 id="Fast-Template-Matching-and-Update-for-Video-Object-Tracking-and-Segmentation"><a href="#Fast-Template-Matching-and-Update-for-Video-Object-Tracking-and-Segmentation" class="headerlink" title="Fast Template Matching and Update for Video Object Tracking and Segmentation"></a>Fast Template Matching and Update for Video Object Tracking and Segmentation</h1><h2 id="Motivation-2"><a href="#Motivation-2" class="headerlink" title="Motivation"></a>Motivation</h2><p>第三篇来自CVPR2020。本文针对的任务是<strong>多实例半监督视频目标分割</strong>(VOS)。基于检测的算法被广泛应用于这一任务，难点在于选择匹配方法来预测结果，以及是否更新目标模板。 本文利用强化学习来同时做出这两个决策。具体来说，<strong>agent根据预测结果的质量来决定是否更新目标模板</strong>。 匹配方法的选择则基于agent的动作历史来确定。</p><p>目前大部分VOT或VOS方法主要分为三步：</p><ol><li>对当前帧进行实例分割，生成一系列候选proposal；</li><li>将目标模板和所有proposal进行匹配，找到正确的proposal作为最终结果；</li><li>使用当前帧的预测结果替换目标模板。</li></ol><p>针对步骤2，基于外观的匹配方法（siamese）准确但非常耗时，而直接利用候选框与前一帧预测框的IOU进行快速匹配只适用于目标缓慢移动或变化。针对步骤3，现有方法简单地直接用当前结果替换模板，不考虑结果的正确性，会导致误差逐渐累积。因此需要利用强化学习智能切换。</p><h2 id="Method-2"><a href="#Method-2" class="headerlink" title="Method"></a>Method</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210630113658.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210630171122.png" alt=""></p><center>将上一帧中所有目标框的最小外接矩形bm扩大一定倍率得到搜素区域bs。</center><p>整体方法如图2所示，分为三个步骤：</p><ol><li>按照图3的方式确定目标搜索区域，采用实例分割网络（如YOLACT, Mask-RCNN）生成候选预测，然后利用基于IOU的匹配方法得到初步匹配结果；</li><li>通过agent判断初步结果的正确性和质量，决定是否更新模板；</li><li>确定是否需要切换到基于外观的匹配的方法。若连续N帧初步结果都不好（即第二步预测不更新模板），则切换到基于外观的匹配。此时会将整个图像送入网络。</li></ol><p>下面介绍将Actor-Critic的框架嵌入上述模型</p><p><strong>Action</strong></p><p>首先定义相关的符号，如图4所示，目标模板包括边界框 $T_{box}$，mask $T_{mask}$，$T_{box}$中的图像内容$T’_{box}$，$T_{mask}$中的图像内容$T’_{mask}$。而预测结果则是类似的$P_{box}, P_{mask}, P’_{box}, P’_{mask}$。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210630173712.png" style="zoom:80%;" /></p><p>第一个决策是是否更新模板，agent的动作 $a_i \in A$ 有两种情况，$a_0$表示用当前结果更新模板，$a_1$表示不更新。</p><p>第二个决策是匹配方法选择快速的基于IOU匹配还是精确的基于外观匹配，在精度和速度间取得平衡。前者分别计算模板和候选预测的box IOU和mask IOU，选择IOU最大的作为匹配结果；后者则是计算模板图像块$T’_{box}$和候选预测图像块$P’_{box}$的相似性，选择最像的作为匹配结果。<strong>注意这里没有另外增加一个agent</strong>，而是根据第一个agent的历史决策来决定。若agent连续N帧预测$a_1$，表示目标很可能丢失，此时需要切换到基于外观的匹配方法。</p><p><strong>State</strong> </p><p>输入agent的状态$s_t$包括两部分，如下式。第一部分$S_T$是模板图像，其中边框$T’_{box}$之内的内容保持不变，之外的内容填充黑色；第二部分$S_P$是搜索图像，同样将mask之内的内容保持不变，之外的内容填充黑色。提取这两种图像的特征并相加得到输入状态。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210630175114.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210630175123.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210630175138.png" alt=""></p><p><strong>Reward</strong></p><p>奖励函数定义：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210630175755.png" alt=""></p><p>$J_t$表示$P_{mask}$和GT mask之间的IOU。</p><p><strong>Actor-Critic Training</strong></p><p>基本元素确定后，按照Actor-Critic框架训练。</p><p>critic网络用value-based的方式训练：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210630180416.png" alt=""></p><p>$\delta_t$ 表示TD error，公式8中梯度下降用加号是因为公式9减法顺序和常规的是反过来的。</p><p>actor用policy-based方法训练：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210630192837.png" alt=""></p><p>公式11减$V(s_t)$表示是带baseline的策略梯度。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210630193309.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/42-rl-tracking/20210630193324.png" alt=""></p><p>图1和图6表面本文的方法在VOT和VOS任务上均能在速度和精度上取得一个较好的平衡。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上三种方法分别介绍了利用强化学习来决策使用的跟踪特征，多个跟踪器的切换以及是否更新模板。可以发现，应用的方向基本都是把跟踪方法中某些需要启发式设计的模块换成了强化学习进行智能决策。此外，第一篇和第三篇均提到了引入强化学习可以在一定程度上提速，对于某些简单的情况，agent可以决策使用简单的方法进行跟踪。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;强化学习讨论的问题是智能体(agent) 如何在一个复杂不确定的环境(environment) 里去最大化它能获得的奖励。 今天介绍三篇关于强化学习在目标跟踪中的工作，分别利用强化学习来决策使用的特征，多个跟踪器的切换以及是否更新模板。&lt;/p&gt;
&lt;p&gt;论文列表：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1708.02973&quot;&gt;Learning Policies for Adaptive Tracking with Deep Feature Cascades&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://proceedings.neurips.cc//paper/2020/file/885b2c7a6deb4fea10f319c4ce993e02-Paper.pdf&quot;&gt;Online Decision Based Visual Tracking via Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.07538&quot;&gt;Fast Template Matching and Update for Video Object Tracking and Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="综述" scheme="https://www.zjp97.top/tags/%E7%BB%BC%E8%BF%B0/"/>
    
    <category term="强化学习" scheme="https://www.zjp97.top/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking</title>
    <link href="https://www.zjp97.top/tracking/41-siamrcr/"/>
    <id>https://www.zjp97.top/tracking/41-siamrcr/</id>
    <published>2021-05-26T13:03:49.000Z</published>
    <updated>2022-06-11T03:00:51.631Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/41-siamrcr/20210526210529.png" alt=""></p><p><a href="https://arxiv.org/abs/2105.11237">论文</a></p><p>本文解决的是老生常谈的分类和回归不匹配的问题。作者提出在分类和回归之间建立<strong>双向</strong>的连接，可以动态地重新加权每个正样本的损失。此外，增加了一个定位分支用于预测定位精度，可以在推理过程中替代回归辅助连接(regression assistance link)，使得训练和测试更加一致。最终运行速度为65FPS。</p><span id="more"></span><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/41-siamrcr/20210526212918.png" alt=""></p><p>首先点出问题，即孪生跟踪架构中分类和回归是分开独立优化的，导致二者不匹配。如图1所示，分类得分最高的位置生成的预测框不一定是最好的，或者预测比较好的框分类得分很低。这个图和IOUNet中的图很相似，事实上，这个问题在检测任务中已经被很多学者研究，并且有些成果也被应用到跟踪中。比如SiamFC++借鉴FCOS架构增加了一个衡量定位精度的分支，ATOM/DiMP系列使用IOUNet进行回归。作者指出这些方法仍然存在不匹配，因为并没有解决分类和回归独立优化的问题。</p><p>因此，本文提出在分类和回归之间建立一个互惠关系 (reciprocal relationship)，使它们同步优化，以生成精度一致的输出。整体框架如图2所示，在分类和回归分支之间增加了两个连接 classification / regression assistance link。classification assistance 用分类置信度给回归损失加权，使得回归可以更关注高置信度的位置；regression assistance 用定位精度（预测框和gt的IOU）给分类损失加权，迫使分类分数与回归精度更加一致。</p><p>而在推理阶段，gt是未知的，无法通过计算IOU得到定位精度，因此额外增加一个定位分支专门用于预测定位精度。将分类置信度与定位预测置信度相乘，在推理阶段生成新的跟踪评分/置信度图，保证了与训练过程的一致性（类似FCOS / SiamFC++）。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/41-siamrcr/20210526221151.png" alt=""></p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>图2整体是一个anchor-free的孪生框架，预测头部分采用的是CenterNet的中心、中心偏移量、宽高的形式。这里将中心偏移量和宽高放在同一个分支输出，所以回归分支输出维度是4  $t^<em>_{x,y} = (w^</em>, h^<em>, \Delta x^</em>, \Delta y^*)$ 。</p><h2 id="Reciprocal-Classification-and-Regression"><a href="#Reciprocal-Classification-and-Regression" class="headerlink" title="Reciprocal Classification and Regression"></a>Reciprocal Classification and Regression</h2><p>下面开始介绍两种辅助连接，设计原则就是：</p><ol><li>当回归框的定位精度较低时，相应的分类得分不应该很高，因为如果该位置成为分类置信度的赢家，差的回归结果将导致跟踪性能较差；</li><li>当回归框的分类分数较低时，提高其定位精度是没有意义的，因为这个框一定不会是最后的输出。</li></ol><p><strong>Regression Assistance Link</strong> 针对准则1，将回归分支生成的框与gt计算IOU，看成一种动态的样本重加权作用于分类损失：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/41-siamrcr/20210526223907.png" alt=""></p><p>$N_{pos}$表示正样本个数，$B,B^*$分别表示预测框和gt。</p><p><strong>Classification Assistance Link</strong> 针对准则2，将分类置信度 $p^{cls}_{x,y}$ 动态地重新加权回归损失：</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/41-siamrcr/20210526225051.png" alt=""></p><p><strong>Localization Score Branch</strong> 在训练时，regression assistance link使得分类分支考虑了回归精度，但这需要借助gt。而在推理阶段没有gt，为了保证训练和推理的一致性，使得推理阶段的分类分支也能考虑定位精度，作者额外增加了一个定位分支专门用于预测定位精度，作用类似FCOS / SiamFC++的centerness。其训练损失就是计算定位分支输出与IOU之间的交叉熵损失。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/41-siamrcr/20210526225941.png" alt=""></p><p>推理阶段示意图如图3，将分类分支和定位分支的得分相乘生成最后的跟踪分数图。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/41-siamrcr/20210527094602.png" alt=""></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/41-siamrcr/20210526230610.png" alt=""></p><p>表1中这两个组件的性能提升几乎是正交的，联合使用的提升(5.05%)几乎等于单独使用时各自性能提升的和(3.54%+2.86%)。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/41-siamrcr/20210526234418.png" alt=""></p><p>图4展示了不同方法的跟踪分数和回归框IOU之间的相关性，(a)是baseline，(b)是centerness，(c)是basline+定位分支，(d)是SiamRCR。其中(b)和(c)类似，区别是分别使用IOU和centerness来衡量定位精度。</p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/41-siamrcr/20210526234837.png" alt=""></p><p>最后展示了划分正负样本时不同半径对结果的影响。</p><h2 id="Comparison-with-the-State-of-the-Art"><a href="#Comparison-with-the-State-of-the-Art" class="headerlink" title="Comparison with the State-of-the-Art"></a>Comparison with the State-of-the-Art</h2><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/41-siamrcr/20210526234927.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/41-siamrcr/20210526234937.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/41-siamrcr/20210526234948.png" alt=""></p><p><img src="https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/41-siamrcr/20210526234956.png" alt=""></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文解决了孪生跟踪框架中分类和回归不一致的问题。印象中最先研究这个问题的是检测领域的PISA (Prime Sample Attention in Object Detection)，根据IOU对样本进行排序然后选择prime sample。之后也有许多相关文章，包括本文的Regression Assistance Link也能看到一些检测里面的影子（吐槽一下我19年投了一篇文章有个点和这个一毛一样然而到现在都还没中hhh）。但目前的研究其实基本上都是在用回归矫正分类，而本文做了一个双向的矫正，（显式地）增加了用分类矫正回归的过程（为什么说是显式，因为其实只要把分类和回归乘到一起，这个作用就是相互的，比如公式2中，我可以把focal loss当成权重，IOU当成优化变量，那就变成了分类矫正回归）。这种显式的矫正似乎监督能力更强，如果有个消融实验对比一下单独使用公式3和公式4的效果就更直观了。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://hexo-zjp97-1300900922.cos.ap-nanjing.myqcloud.com/picgo/blog/41-siamrcr/20210526210529.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2105.11237&quot;&gt;论文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文解决的是老生常谈的分类和回归不匹配的问题。作者提出在分类和回归之间建立&lt;strong&gt;双向&lt;/strong&gt;的连接，可以动态地重新加权每个正样本的损失。此外，增加了一个定位分支用于预测定位精度，可以在推理过程中替代回归辅助连接(regression assistance link)，使得训练和测试更加一致。最终运行速度为65FPS。&lt;/p&gt;</summary>
    
    
    
    <category term="目标跟踪" scheme="https://www.zjp97.top/categories/tracking/"/>
    
    
    <category term="IJCAI2021" scheme="https://www.zjp97.top/tags/IJCAI2021/"/>
    
  </entry>
  
</feed>
