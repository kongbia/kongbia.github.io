<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Siam R-CNN: Visual Tracking by Re-Detection</title>
    <url>/tracking/1-siamrcnn/</url>
    <content><![CDATA[<p>Siam R-CNN是亚琛工业大学&amp;牛津大学联合推出的，核心是通过重检测进行视觉跟踪，并构建了基于轨迹的动态规划算法，建模被跟踪对象和潜在干扰对象的完整历史。效率方面，该方法可以在 ResNet-101 上达到 4.7 FPS，在 ResNet-50 上达到 15 FPS 。</p>
<span id="more"></span>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/0-siamrcnn/20210424105346.png" alt=""></p>
<p><a href="https://arxiv.org/abs/1911.12836">论文</a><br><a href="https://github.com/SJTU-DL-lab/SiamRCNN">代码</a></p>
<h1 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h1><ol>
<li>将重检测思想用于跟踪。通过判断建议区域（region proposal）是否与模板区域（template region）相同，重检测图像中任何位置的模板对象，并对该对象的边界框进行回归，这种方法对目标大小和长宽比变化比较鲁棒；</li>
<li>提出一种新颖的 hard example mining 方法，专门针对困难干扰物训练；</li>
<li>提出 Tracklet Dynamic Programming Algorithm (TDPA)，可以同时跟踪所有潜在的目标，包括干扰物。通过重检测前一帧所有目标候选框，并将这些候选框随时间分组到tracklets(短目标轨迹)中。然后利用动态规划的思想，根据视频中所有目标和干扰物tracklets的完整历史选择当前时间步长的最佳对象。Siam R-CNN通过明确地建模所有潜在对象的运动和交互作用，并将检测到的相似信息汇集到tracklets中，能够有效地进行长时跟踪，同时抵抗跟踪器漂移，在物体消失后可以立即重检测目标。</li>
</ol>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/0-siamrcnn/20210424105651.png" alt=""></p>
<p>上图为Siam RCNN整体框架，作者设计了一个Siamese two-stage detection network作为跟踪器。第一阶段是RPN，第二阶段通过将感兴趣区域(RoI)的特征和参考特征拼接起来，包括以第一帧的GT作为参考和以前一帧的检测结果作为参考的两个redetction head。最后构建Tracklet Dynamic Programming Algorithm (TDPA)去跟踪所有潜在目标。</p>
<p>接下来按照论文结构对每一部分进行说明：</p>
<h2 id="Siam-RCNN"><a href="#Siam-RCNN" class="headerlink" title="Siam RCNN"></a>Siam RCNN</h2><p>本小节主要是讲如何将 Faster RCNN 的那一套用于重检测，核心是将固定类别的detection head换成本文的re-detection head。re-detection head的输出只有两类，即候选区域是否是参考对象目标。训练时backbone和RPN的参数冻结，只有re-detection head参与训练。</p>
<h2 id="Video-Hard-Example-Mining"><a href="#Video-Hard-Example-Mining" class="headerlink" title="Video Hard Example Mining"></a>Video Hard Example Mining</h2><p>在传统 Faster RCNN 训练中，负样本是从RPN得到的区域中采样得到的。但是，在许多图像中，仅有少量负样本。为了最大化 re-detection head 的判别能力，作者认为需要在难负样本（hard negative samples）上训练。并且与检测中的通用难样本不同，这里的难样本是从其他视频中检索出来的与参考目标类似的样本。</p>
<ul>
<li><p><strong>Embedding Network</strong> 最直接的方法就是从其他视频中寻找与当前对象属于同一个类别的对象作为难负样本。然而，物体的类别标签并不总是可靠，一些同类的物体很容易区分，不同类别的物体反而可能是理想的难负样本。所以，本文受到行人重识别的启发，提出利用 embedding network 的方法，将 GT Box 中的物体映射为 embedding vector。这个网络来源于 PReMVOS，用 batch-hard triplet loss 来训练，期望达到的效果是消除单个对象实例之间的歧义，例如，两个不同的人在嵌入空间中应该离得很远，而同一个人在不同帧中的向量距离应该很近。</p>
</li>
<li><p><strong>Index Structure</strong> 接下来为近邻 queries 构建一个有效的索引结构，将其用于寻找所需要跟踪的物体在 embedding space 中的最近邻。图 3 展示了一些检索得到的难负样本。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/0-siamrcnn/20210424105907.png" alt=""></p>
</li>
<li><p><strong>Training Procedure</strong> 训练时在其他视频上实时检索当前视频帧的难负样本过于耗时。本文预先对训练数据的每一个 GT Box 都提取其 RoI-aligned  features。在训练的每一步，随机选择一个 video 和 object，然后随机的选择一个 reference 和 target frame。在此之后，用上一节提到的 indexing structure 来检索 10000 个最近邻的 reference box，从中选择出 100 个 negative training examples。</p>
</li>
</ul>
<h2 id="Tracklet-Dynamic-Programming-Algorithm"><a href="#Tracklet-Dynamic-Programming-Algorithm" class="headerlink" title="Tracklet Dynamic Programming Algorithm"></a>Tracklet Dynamic Programming Algorithm</h2><p>本文所提出的 片段动态规划算法（Tracklet Dynamic Programming Algorithm）隐式地跟踪感兴趣目标和潜在干扰物，从而持续抑制干扰对象。为此，TDPA 维护一组 tracklets，即：short sequences of detections。然后利用动态规划的评分算法为模板对象在第一帧和当前帧之间选择最可能的 tracklets序列。每个detection都定义为：a bounding box, a re-dection score 和 RoI-aligned features。此外，每个 detection 都是 tracklet 的组成部分。tracklet有一个开始时间和一个结束时间，并由一组 detection 定义，从开始到结束时间的每一个时间步对应一个detection ，也就是说，在tracklet中不允许有空隙。</p>
<ul>
<li><strong>Tracklet Building</strong> </li>
</ul>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/0-siamrcnn/20210424110139.png" alt=""></p>
<p>Line 2 提取backbone特征</p>
<p>Line 3 RPN得到感兴趣区域，为了补偿潜在的假负例影响，加入上一帧的结果；</p>
<p>Line 4 将Roi和gt送入redetection head得到第一帧的重检测结果 $ det_{S_t} $（包括相似性得分和目标框）；</p>
<p>Line 5-6 将当前帧结果 $ det_{S_t} $ 与上一帧结果 $ det_{S_{t-1}} $ 送入redetection head计算每一对检测结果的相似性得分（为了减少计算，仅把当前帧与上一帧框的归一化空间距离小于 r 时才送入head计算，否则相似性得分设为负无穷。如图2，上一帧有3个结果，当前帧有2个结果，理论上两两之间应该计算6个相似得分，但通过框的空间距离约束，实际参与相似得分计算的只有4组）；</p>
<p>Line 7-20 扩展tracklets。遍历当前帧的检测结果 $d_t \in det_{S_t}$ 加入tracklet需要满足：</p>
<ol>
<li>当前结果 $d_t$ 与上一帧结果 $\hat{d} _ {t-1}$ 的最大相似性得分 $s_{1}$ 大于阈值 $\alpha$</li>
<li>当前帧没有其他检测结果与 $ \hat{d} _ {t-1} $ 具有同样高的相似性，即当前帧除 $d_t$ 以外的结果与 $ \hat{d} _ {t-1} $ 的相似性要满足 $s_{2} \leq s_{1}-\beta$</li>
<li>上一帧没有其他检测结果与当前的 $d_t$ 具有同样高的相似性，即上一帧除 $ \hat{d} _ {t-1} $ 以外的结果与 $d_t$ 的相似性要满足 $s_{3} \leq s_{1}-\beta$</li>
</ol>
<p><br/></p>
<ul>
<li><strong>Scoring</strong>  </li>
</ul>
<p>轨迹 $A=(a_1,…,a_N)$ 是一个包含N个不重复tracklets的序列，其中 $end(a_i) &lt; start(a_{i+1}), \forall i∈\{1,…,N-1\}$ ，即后一个tracklet的开始帧一定比前一个tracklet的结束帧大。一个轨迹的总分由衡量单个轨迹质量的一元分数 unuary 和惩罚轨迹之间空间距离的位置分数 loc_score 组成。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/0-siamrcnn/20210424111306.png" style="zoom:80%;" /></p>
<p>其中 ff_score 表示tracklet $a_{i}$ 在 t 时刻以第一帧gt为参考的重检测分数；</p>
<p>而 ff_tracklet_score 表示 $a_{i}$ 以gt所在tracklet的最后一个检测结果为参考的重检测分数（因为一个tracklet的所有检测结果都有很高概率与这个tracklet的第一个检测结果相同，否则这个tracklet就会终止，所以gt所在的tracklet的最后一个检测结果也有很大概率是正确的）；</p>
<p>Location score是计算前一个tracklet最后一个检测结果和后一个tracklet的第一个检测结果的bbox之间的L1距离，是负的，作为惩罚项，越小越好。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/0-siamrcnn/20210424111534.png" style="zoom:80%;" /></p>
<ul>
<li><strong>Online Dynamic Programming</strong> </li>
</ul>
<p>这一节介绍如何高效找到具有最大总得分的tracklets序列。</p>
<p>定义 $\theta[a]$ 表示从第一个tracklet开始到第a个tracklet的总得分。由于一旦tracklet不被扩展，就会终止。因此当新的一帧到来时，只有被扩展或者新建的tracklet需要重新计算score。</p>
<p>首先设置 $\theta[a_{ff}]=0$，$a_{ff}$ 是first-frame tracklet（即包含gt的tracklet），当每一个tracklet a被创建或需要更新时</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/0-siamrcnn/20210424111717.png" style="zoom:80%;" /></p>
<p>在更新了当前帧的 $\theta$ 后，选择最大动态规划得分的tracklet $\hat{a}=arg max_{a} \theta[a]$；如果被选中的tracklet在当前帧中没有检测结果，则算法会指出目标不存在。由于benchmarks要求每帧都输出结果，所以我们用选定的tracklet最近一次的检测的box，并且将其得分置0。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>实验做的很充分，在目前主流数据集中都有结果。在short-term的测试中，大部分达到SOTA水平或者比SOTA稍低一些，在long-term的测试效果则非常好，基本吊打第二名。其中VOT的测试因为其自带一个重启，会与算法本身的重启冲突，所以作者专门做了一个short-term版本的用于VOT测试。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/0-siamrcnn/20210424111909.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/0-siamrcnn/20210424111922.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/0-siamrcnn/20210424112042.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/0-siamrcnn/20210424113951.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/0-siamrcnn/20210424111947.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/0-siamrcnn/20210424114012.png" alt=""></p>
<p>消融实验比较了 使用hard example mining的效果，TPDA与直接使用每一帧最大的重检测得分(Argmax)的对比，和专门用于测试VOT2018的short-term版本；以及改变backbone和ROI的参数量来提速的验证。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文最大的贡献在于将两阶段结构和重检测用于跟踪，设计了一套适合长时跟踪的算法。两阶段结构在SPM-Tracker已经证明了其对于鲁棒性和判别性之间有较好的平衡；而重检测这类检索的方法同样对于目标变化的适应性更强。整套算法做的非常完善，美中不足在于速度太慢。</p>
<p>P.S. SiamRCNN很长一段时间都作为各类跟踪数据集的天花板，直到CVPR2021各类transform架构的跟踪算法出现后才被超过，可以参考 <a href="https://kongbia.github.io/2021/04/23/tracking/33-transform-tracking/">Transform与目标跟踪</a></p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>CVPR2020</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测框架在目标跟踪中的应用</title>
    <url>/tracking/2-detection-in-tracking/</url>
    <content><![CDATA[<p>从SiamRPN将跟踪问题定义为one-shot detection任务之后，出现了大量将检测组件由于跟踪的研究。不过Siamese系列一个很大的问题在于其本质仍然是一个模板匹配问题，网络关注的是寻找与target相似的东西，而忽视了区分target和distractor的判别能力，这正是目标检测任务所擅长的。目标检测和目标跟踪的关键差异在于检测是一个class-level的任务，而跟踪是一个instance-level的任务（即检测只关注类间差异而不重视类内差异，跟踪需要关注每一个实例，同时跟踪的类别是不可知的）。</p>
<p>本篇笔记关注如何将目标检测框架应用在跟踪中，主要介绍其思想，细节部分不做过多描述，记录论文包含：</p>
<span id="more"></span>
<ul>
<li><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Bridging_the_Gap_Between_Detection_and_Tracking_A_Unified_Approach_ICCV_2019_paper.pdf">Bridging the Gap Between Detection and Tracking: A Unified Approach</a></li>
<li><a href="https://arxiv.org/abs/1912.08531">GlobalTrack: A Simple and Strong Baseline for Long-term Tracking</a></li>
<li><a href="https://arxiv.org/abs/1910.11844">Learning to Track Any Object</a></li>
<li><a href="https://arxiv.org/abs/1911.12836">Siam R-CNN: Visual Tracking by Re-Detection</a></li>
<li><a href="https://arxiv.org/abs/2004.00830">Tracking by Instance Detection: A Meta-Learning Approach</a></li>
</ul>
<h1 id="Bridging-the-Gap-Between-Detection-and-Tracking-A-Unified-Approach"><a href="#Bridging-the-Gap-Between-Detection-and-Tracking-A-Unified-Approach" class="headerlink" title="Bridging the Gap Between Detection and Tracking: A Unified Approach"></a>Bridging the Gap Between Detection and Tracking: A Unified Approach</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/1-detection-in-tracking/20210424234243.png" alt=""></p>
<p>从结构图可以很直观的看出这就是Faster RCNN的框架，作者将目标跟踪任务看成是one-shot object detection 和 few-shot instance classification 的组合。前者是一个class-level的子任务用来寻找和目标相似的候选框，后者是instance-level的任务用来区分目标和干扰物。主要有两个模块：Target-guidence module(TGM) 和 few-shot instance classifier。</p>
<p>TGM对目标和搜索区域的特征以及它们在主干中的相互作用进行编码，相当于让网络更关注于与目标相关的instance，后面几篇文章也用了不同的方法来实现这个目的。</p>
<p>TGM虽然使检测器聚焦于与目标相关的物体，但忽略了周围的背景干扰。为了弥补这一点，提出了few-shot instance classifier。 然而，直接从头开始训练耗时且容易导过拟合。因此作者通过Model-Agnostic Meta Learning (MAML)进行few-shot finetune，增强判别性进一步消除distractors。</p>
<p>MAML的目的是训练一组初始化参数，<strong>通过在初始参数的基础上进行一或多步的梯度调整，来达到仅用少量数据就能快速适应新task的目的，</strong>示意图如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/1-detection-in-tracking/20210424234331.png" alt=""></p>
<p>域自适应的检测器整体训练流程如下图：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/1-detection-in-tracking/20210424234353.png" alt=""></p>
<p>输入是三元组，包括examplar, support, query，训练分为inner and outer optimization loops。对于Inner optimization loop中，在support image上计算的loss用来微调meta-layers即detector heads的参数，然后用微调后的参数计算meta-loss其梯度用于更新outer optimization loop。具体公式可以参考原文。</p>
<p>在线跟踪中将之前帧的检测结果作为训练样本在线更新detector head的参数。</p>
<p>作者称这是第一篇将目标检测框架应用到跟踪上的通用框架，检测模型可以用Faster RCNN，SSD等，速度上SSD模型为10FPS   Faster RCNN模型为3FPS。</p>
<hr>
<h1 id="GlobalTrack-A-Simple-and-Strong-Baseline-for-Long-term-Tracking"><a href="#GlobalTrack-A-Simple-and-Strong-Baseline-for-Long-term-Tracking" class="headerlink" title="GlobalTrack: A Simple and Strong Baseline for Long-term Tracking"></a>GlobalTrack: A Simple and Strong Baseline for Long-term Tracking</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/1-detection-in-tracking/20210424234652.png" alt=""></p>
<p>这篇文章构建了一个global instance search tracker，主要思想是利用target来引导网络搜索特定instance，与上一篇的TGM模块思想类似，不过这里对在RPN阶段和分类回归阶段都加入了target信息进行引导。对应的就是Query-guided RPN 和 Query-guided RCNN。</p>
<p><strong>Query-guided RPN</strong></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/1-detection-in-tracking/20210424234747.png" alt=""></p>
<p>$z \in [k,k,c]$ 是query的ROI特征，$x \in [h,w,c]$ 是搜索图像的特征。$f_z$ 用 $k \times k$ 0-padding的卷积将 $z$ 转换为 $1 \times 1$ 的核作用于搜索区域，$f_x$ 使用 $3 \times 3$ 1-padding的卷积。$f_{out}$ 是 $1 \times 1 \times c$  的卷积将通道数变回为c，这个过程不使用正则化和激活函数。</p>
<p><strong>Query-guided RCNN</strong></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/1-detection-in-tracking/20210424235530.png" alt=""></p>
<p> $z$ 定义同上，$x_i \in [k,k,c]$ 是提取的proposal的特征。$h_z$ 和 $h_x$ 均为 $3 \times 3$ 1-padding的卷积，$h_{out}$ 为$1 \times 1 \times c$  卷积。</p>
<p>GlobalTrack 对视频每一帧的跟踪完全不依赖相邻帧，没有累计误差使得它在长期跟踪问题中准确率保持稳定。速度为6FPS。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/1-detection-in-tracking/20210426092832.png" alt=""></p>
<center>车牌在长期跟踪过程中消失了一段时间，当车牌再次出现的时候，其他跟踪算法就再也无法恢复跟踪了，而没有累计误差的 GlobalTrack不受前面的影响立刻跟踪到了目标。</center>

<hr>
<h1 id="Learning-to-Track-Any-Object"><a href="#Learning-to-Track-Any-Object" class="headerlink" title="Learning to Track Any Object"></a>Learning to Track Any Object</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/1-detection-in-tracking/20210426093105.png" alt=""></p>
<center>图1 (a)从基于图像的数据集学习一个通用对象先验，(b)通过计算一个封闭形式的目标和背景之间的线性判别器使其适应于一个感兴趣的特定对象(例如左上角的总线)。这允许跟踪物体通过显著的变形，而不捕获干扰物</center>

<p>本文重点在于将category-specific object detector 变成 category-agnostic, object-specific detector来做跟踪。想达到这个目的，需要处理如下两个关键的问题，如图1所示：</p>
<ol>
<li>如何将 category specific prior 改为 generic objectness prior？</li>
<li>如何进一步的将这种 generic prior 改为 particular instance of interst？</li>
</ol>
<p>针对问题1，作者构建了 a joint model for category-specific object detection and category-agnostic tracking。和之前类似，也是添加了目标特征的检测框架（基于 Mask R-CNN）如下图2所示 。其将目标模板作为输入，计算 feature embedding。然后该模板特征与测试图像计算相似性得到attention mask。attention mask又被用于重新加权空间特征，以检测感兴趣的物体。另外这个框架可以同时用于检测、跟踪和分割。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/1-detection-in-tracking/20210426100348.png" alt=""></p>
<p>针对问题 2，本文计算一个线性分类器来区分第一帧的感兴趣目标和其他目标，通过最小二乘方法得到闭式解从而可以学习到一个更关注感兴趣instance的鲁棒特征。下图3通过一个例子说明，左下是直接用feature embedding计算的attention map，右下是用线性分类器计算的attention map，显然右下效果更好。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/1-detection-in-tracking/20210426100433.png" alt=""></p>
<p>最后运行速度为7FPS。</p>
<hr>
<h1 id="Siam-R-CNN-Visual-Tracking-by-Re-Detection"><a href="#Siam-R-CNN-Visual-Tracking-by-Re-Detection" class="headerlink" title="Siam R-CNN: Visual Tracking by Re-Detection"></a>Siam R-CNN: Visual Tracking by Re-Detection</h1><p>这个就是用重检测的思想做跟踪，也是基于RCNN框架的，同时使用Tracklet Dynamic Programming Algorithm去跟踪所有潜在的目标。</p>
<p>具体解读见前一篇笔记</p>
<p><a href="https://kongbia.github.io/2021/04/24/tracking/1-siamrcnn/">Siam R-CNN: Visual Tracking by Re-Detection | CV home (kongbia.github.io)</a></p>
<hr>
<h1 id="Tracking-by-Instance-Detection-A-Meta-Learning-Approach"><a href="#Tracking-by-Instance-Detection-A-Meta-Learning-Approach" class="headerlink" title="Tracking by Instance Detection: A Meta-Learning Approach"></a>Tracking by Instance Detection: A Meta-Learning Approach</h1><p>同样是域自适应方法将检测器转化成跟踪器，此篇更像是第一篇Bridging the Gap Between Detection and Tracking: A Unified Approach的进阶，不同的是本文没有额外添加模板引导分支，而是直接用标准的检测器通过元学习的方式做域自适应。避免了冗余结构使得速度大幅提升，达到40FPS。另外就是训练的时候加入了很多来自MAML++喝MetaSGD的技巧，效果更好。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/1-detection-in-tracking/20210426100802.png" alt=""></p>
<p>具体的解读可以参考我b站的笔记，后续会搬运到博客。</p>
<p><a href="https://www.bilibili.com/read/cv5521209/">[Note7] Tracking by Instance Detection: A Meta-Learning Approach - 哔哩哔哩专栏 (bilibili.com)</a></p>
<hr>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这几篇文章的一个共同思路都是融合了Siamese架构和目标检测框架，将目标实例信息以各种形式加入待检测图像中，从而将class-level的通用检测转变成instance-level的实例检测（跟踪）。借助目标检测对尺度，形变等复杂条件的优越性来解决跟踪中的问题，同时将跟踪转变成one-shot的检测任务也避免了更新带来的漂移（第一篇里面使用了MAML进行更新，主要原因猜测是单纯往RPN中融合目标信息还不够work，像globaltracker在head上也添加了instance，而第三篇则是构建一个分类器增强鲁棒性）。当然引入检测框架带来的计算开销也是很大的，最后一种方法避免了额外的模板分支相当于跳出了Siamese框架，给实时带来了可能。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title>Transform与目标跟踪</title>
    <url>/tracking/33-transform-tracking/</url>
    <content><![CDATA[<p>Transform在视觉领域遍地开花，终于目标跟踪也没能逃过。并行的长距离依赖（空间和时间皆可）对于目标跟踪似乎有着天然的优势，本篇笔记简要概述今年CVPR2021关于Transform在目标跟踪中的应用，主要介绍动机和结构，细节和实验部分以后有空再补充。</p>
<span id="more"></span>
<p>论文列表：</p>
<ul>
<li>Transformer Tracking</li>
<li>Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking</li>
<li>Learning Spatio-Temporal Transformer for Visual Tracking</li>
<li>Target Transformed Regression for Accurate Tracking</li>
</ul>
<h1 id="Transformer-Tracking"><a href="#Transformer-Tracking" class="headerlink" title="Transformer Tracking"></a>Transformer Tracking</h1><p><a href="https://arxiv.org/abs/2103.15436">论文</a><br><a href="https://github.com/chenxin-dlut/TransT">代码</a><br><a href="http://naotu.baidu.com/file/0348e011f8d04a784134e3329a328076?token=53a180e3aa5251ab">代码框架解析</a></p>
<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>跟踪中常用的correlation存在问题：<br>是一个局部线性匹配过程，没有利用全局上下文，容易陷入局部最优；<br>得到的相似图丢失一定程度的语义信息，导致对目标边界预测不准。</p>
<p>利用transform的attention有效融合模板特征和ROI特征，相比correlation能产生更多的语义特征。作者提出了基于self-attention的ego-context augment module (ECA)和基于cross-attention的cross-feature augment module (CFA)<br><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/1.png" style="zoom:80%;" /></p>
<h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p>重复N=4次fusion layer最后再接一个CFA<br><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/2.png" alt=""></p>
<center>整体跟踪框架</center>

<p><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/3.png" style="zoom:80%;" /></p>
<center>ECA和CFA结构</center>

<p>transform工作过程</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/4.png" style="zoom:80%;" /></p>
<ul>
<li>n=1 self search 没有来自模板的信息，因此会看到所有目标，而self template关注模板的关键信息（蚂蚁上的红点）；cross search和template同时具有目标和搜索的特征，因此可以更关注重要信息；</li>
<li>n=2 每一个attention输入都同时包含目标和搜索特征，self search对相似物的响应被抑制了，而cross search此时非常确信其预测。template的注意力此时开始关注目标边界；</li>
<li>n=3 进一步强化，模板特征成为包含大量目标边界信息的信息库，而搜索区域特征保留了目标的空间信息；</li>
<li>n=4 模板的分布变得混乱，这可能是因为，在目标确定之后，模板分支的特征不再需要保留模板本身的信息，而是存储了大量目标的边界信息，成为一个为回归服务的特征库。</li>
</ul>
<hr>
<h1 id="Transformer-Meets-Tracker-Exploiting-Temporal-Context-for-Robust-Visual-Tracking"><a href="#Transformer-Meets-Tracker-Exploiting-Temporal-Context-for-Robust-Visual-Tracking" class="headerlink" title="Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking"></a>Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking</h1><p><a href="https://arxiv.org/abs/2103.11681">论文</a><br><a href="https://github.com/594422814/TransformerTrack">代码</a></p>
<h2 id="动机-1"><a href="#动机-1" class="headerlink" title="动机"></a>动机</h2><p>现有的跟踪器常常忽略连续帧之间的 temporal contexts</p>
<ol>
<li><p>单帧独立检测方法： 对时域信息的利用只有运动先验（余弦窗）</p>
</li>
<li><p>模型更新方法：视频帧是独立的，没有相互推理关系；噪声会污染模型更新</p>
</li>
</ol>
<p>transform中的注意机制，能够建立跨帧的像素对应关系，在时间域内自由传递各种信号。</p>
<p>本文将各个独立的视频帧进行桥接，并通过 transformer 架构来探索它们之间的 temporal contexts，以实现鲁棒的目标跟踪。与经典的 transformer 的结构不同，作者将其编码器和解码器分离成两个平行的分支，并在 Siamese-like 跟踪管道中对其精心设计。</p>
<h2 id="结构-1"><a href="#结构-1" class="headerlink" title="结构"></a>结构</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/5.png" style="zoom:80%;" /></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/10.png" style="zoom:80%;" /></p>
<p>编码器通过基于注意力的特征强化来促进目标模板，有利于高质量的跟踪模型生成；</p>
<p>解码器将之前模板中的跟踪线索传播到当前帧，有利于目标搜索过程。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/6.png" style="zoom:80%;" /></p>
<p>与经典transform结构的差异：</p>
<ol>
<li>Encoder-decoder Separation. 没有将编码器和解码器级联，而是将编码器和解码器分离为两个分支，以适应Siamese-like跟踪方法；</li>
<li>Block Weight-sharing. 编码器和解码器中的self-attention(图4中的黄色方框)共享权值，将模板和搜索转换到同一特征空间中，便于进一步cross-attention；</li>
<li>Instance Normalization. 将Layer Norm换成Instance Norm；</li>
<li>Slimming Design. 移除FFN，并且使用single-head attention。</li>
</ol>
<p>图4编码器解码器结构细节：</p>
<p>编码器： 输入模板特征 $T \in [N_T, C], N_T=n \times H \times W $, $n$为模板数量；</p>
<p>解码器： 输入搜索特征 $S \in [N_S, C], N_S=H \times W $</p>
<p>高斯Mask     $M \in [N_T, 1] $</p>
<p>Mask Transformation 关注空间注意力，Feature Transformation 关注上下文信息</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/7.png" style="zoom:80%;" /></p>
<h3 id="跟踪框架"><a href="#跟踪框架" class="headerlink" title="跟踪框架"></a>跟踪框架</h3><p><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/8.png" style="zoom:80%;" /></p>
<ul>
<li><p>Siamese框架将编码器特征crop后和解码器特征做相关；</p>
</li>
<li><p>DCF框架用编码器特征训练Dimp的kernel，作用于解码器特征；</p>
</li>
<li><p><center>将上一帧中所有目标框的最小外接矩形bm扩大一定倍率得到搜素区域bs。<center></p>
</li>
<li><p>模板池每5帧更新一次，先入先出。</p>
</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>严重（完全）遮挡，出视野，高计算量</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/9.png" style="zoom:80%;" /></p>
<hr>
<h1 id="Learning-Spatio-Temporal-Transformer-for-Visual-Tracking"><a href="#Learning-Spatio-Temporal-Transformer-for-Visual-Tracking" class="headerlink" title="Learning Spatio-Temporal Transformer for Visual Tracking"></a>Learning Spatio-Temporal Transformer for Visual Tracking</h1><p><a href="https://arxiv.org/abs/2103.17154">论文</a><br><a href="https://github.com/researchmm/Stark">代码</a></p>
<h2 id="动机-2"><a href="#动机-2" class="headerlink" title="动机"></a>动机</h2><p>卷积只处理空间或时间上的局部关系，不擅长建立长距离的全局依赖关系。因此在面对目标发生较大形变或频繁进出视野时容易失败。另外，当前的方法将空间和时间分离处理，并没有明确建模空间和时间之间的关系。</p>
<p>考虑到transform在建模全局依赖方面的优势，作者利用它整合空间和时间信息进行跟踪，生成判别的时空特征用于目标定位。</p>
<p>编码器对目标对象和搜索区域之间的全局时空特征依赖关系进行建模，而解码器学习一个查询嵌入来预测目标对象的空间位置。该方法将目标跟踪作为一个直接的边框预测问题（角点预测），没有后处理。</p>
<h2 id="结构-2"><a href="#结构-2" class="headerlink" title="结构"></a>结构</h2><h3 id="Baseline-spatial-only"><a href="#Baseline-spatial-only" class="headerlink" title="Baseline (spatial-only)"></a>Baseline (spatial-only)</h3><p><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/11.png" style="zoom:80%;" /></p>
<p><strong>编码器</strong>输入将模板和搜索特征拉平拼接；</p>
<p><strong>解码器</strong>中query可以注意到模板和搜索区域的所有位置的特征，从而学习鲁棒表示，以进行边框预测；</p>
<p><strong>预测头</strong>将Encoder输出中的搜索特征和decoder输出经过图3的结构，通过概率预测两个角点，最后输出唯一的框，用L1和IOU loss优化。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/12.png" style="zoom:80%;" /></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/13.png" style="zoom:80%;" /></p>
<h3 id="Spatio-Temporal-Transformer-Tracking"><a href="#Spatio-Temporal-Transformer-Tracking" class="headerlink" title="Spatio-Temporal Transformer Tracking"></a>Spatio-Temporal Transformer Tracking</h3><p><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/14.png" style="zoom:80%;" /></p>
<p>相比baseline的改变：三元输入、增加分数预测头、训练&amp;推理策略</p>
<p>训练分为两阶段：第一阶段不训练score head，搜索图像全部包含目标；第二阶段固定其他参数单独训练score head，搜索图像中有一半不包含目标（训练时只要搜索图像包含目标则认为可以更新）；</p>
<p>推理时达到更新间隔且分数大于阈值更新模板</p>
<h3 id="本文结构与DETR的区别"><a href="#本文结构与DETR的区别" class="headerlink" title="本文结构与DETR的区别"></a>本文结构与DETR的区别</h3><ol>
<li>任务不同，检测vs跟踪</li>
<li>输入不同，detr输入整个图像，本文输入三元组，一个search和两个template；</li>
<li>query和训练策略，detr有100个query并且每个都需要匈牙利匹配gt，而本文只有一个query和唯一gt；</li>
<li>预测头不同，detr三层感知器，本文基于角点预测</li>
</ol>
<hr>
<h1 id="Target-Transformed-Regression-for-Accurate-Tracking"><a href="#Target-Transformed-Regression-for-Accurate-Tracking" class="headerlink" title="Target Transformed Regression for Accurate Tracking"></a>Target Transformed Regression for Accurate Tracking</h1><p><a href="https://arxiv.org/abs/2104.00403">论文</a><br><a href="https://github.com/MCG-NJU/TREG">代码</a></p>
<h2 id="动机-3"><a href="#动机-3" class="headerlink" title="动机"></a>动机</h2><p>如何将目标信息整合到回归分支中，<strong>保留精确的边界信息</strong>并<strong>及时处理各种目标变化</strong>对于跟踪是至关重要的。</p>
<p>dw-corr将整个目标当成滤波器，只有目标的全局信息，面对物体变形时难以准确反映边界；</p>
<p>pix-corr忽略了目标模板中的少量背景会对目标外部区域赋予较大的注意力权重。</p>
<p>作者利用transform的交叉注意力来建模模板和搜索区域的每个元素之间的pair-wise关系，并用其增强原始特征。这种特征表达能够增强目标相关信息，帮助精确定位边界，并由于其局部和密集匹配机制，在一定程度上适应目标变形。</p>
<p>此外，设计了一个简单的在线模板更新机制来选择可靠的模板，提高了对目标外观变化和几何变形的鲁棒性。</p>
<h2 id="结构-3"><a href="#结构-3" class="headerlink" title="结构"></a>结构</h2><p>设计准则：</p>
<ol>
<li>目标集成模块，保留充足的目标信息以生成精确目标边界；</li>
<li>像素级的上下文建模，以增强目标相关的特征和处理形变；</li>
<li>高效的在线机制，以处理连续序列中的外观变化。</li>
</ol>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/15.png" alt=""></p>
<center>TREG整体结构，核心是黄色的target-aware transformer，其余结构参考FCOT</center>

<p><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/16.png" alt=""></p>
<center>Online Target-aware Transformer for Regression. (a) Target-aware transformer (b) Online template update mechanism</center>

<p>将搜索特征看成query，目标被编码成key和value，对每一个query，都利用所有key和value为其提供加权聚合响应。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/17.png" style="zoom:80%;" /></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/18.png" style="zoom:80%;" /></p>
<p>$x_i$是搜索特征，$t_j$是目标特征，$Ω_k$表示目标模板的所有位置，$k$表示模板池的序号；</p>
<p>$\theta_{x_i}, \phi_{t_j}, \omega_{t_j}$ 分别表示 query, key, value；</p>
<p>注意这里归一化使用1/N而不是softmax。</p>
<blockquote>
<p>The reason lies in that some positions in background and distractors of the search region are expected to have low dependency with target, while Softmax function will amplify this noise influence as the sum of attention weights between the query and all the keys is always 1.</p>
</blockquote>
<p>在线更新模板，构建模板序列，包含3个静态模板和4个动态模板，静态的由第一帧变换增广生成，动态的取每n帧中得分最高的。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/20.png" style="zoom:80%;" /></p>
<h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/transform-tracking/19.png" style="zoom:80%;" /></p>
<p>图4展示物体在序列发生了变化，本文的transform增强了目标包括头部和脚在内的边界。</p>
<p>表1 TAT-Cls表示将transform用于分类，效果稍微下降，因为pixel-to-pixel的匹配方法往往忽略了目标的整体信息，不适合区分相似的对象。</p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>本文的结构和CVPR2021另外一篇文章也有些类似，即Graph Attention Tracking，可以参考我在b站的<a href="https://www.bilibili.com/read/cv8692025">笔记</a>。作者将模板和搜索特征的每个位置看成节点，使用图注意力构建局部密集的匹配关系用于加强原始特征。实现方式也和transform的交叉注意力类似，可以说是殊途同归。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>综述</tag>
        <tag>transform</tag>
        <tag>CVPR2021</tag>
      </tags>
  </entry>
  <entry>
    <title>Anchor Free的孪生目标跟踪</title>
    <url>/tracking/3-anchorfree-siamese/</url>
    <content><![CDATA[<p>Anchor-free+孪生网络做跟踪在2020年非常火爆，相关笔记在b站记录。本文主要对其整合进行简单归纳。</p>
<p><a href="https://www.bilibili.com/read/cv4987634/?from=readlist">[Note3] Anchor Free的目标跟踪 - 哔哩哔哩专栏 (bilibili.com)</a></p>
<p><a href="https://www.bilibili.com/read/cv6518222/?from=readlist">[Note17] Anchor-free的目标跟踪(下) - 哔哩哔哩专栏 (bilibili.com)</a></p>
<p>跟踪任务可以看成是分类任务与状态估计任务的结合。分类任务的目的是精确定位目标的位置，而状态估计获得目标的姿态（即目标框）。SiamFC++一文将当前的跟踪器按照不同<strong>状态估计</strong>的方法分为三类：</p>
<span id="more"></span>
<ol>
<li>以DCF和SiamFC为主的跟踪器，构建多尺度金字塔，将搜索区域缩放到多个比例，选择最高得分对应的尺度，这种方式是最不精确的同时先验的固定长宽比不适合现实任务；</li>
<li>以ATOM为主的跟踪器，借鉴IOUNet，通过IOU的梯度迭代来细化box，提升精度的同时带来了较多的超参数以及时间上的消耗；</li>
<li>以SiamRPN为主的追踪器，通过RPN预设anchor来回归框，这类方法虽然很高效，但是anchor的设定不但会引入模糊的相似性得分，而且anchor的设置需要有大量的数据分布先验信息，与通用跟踪的目的不符合。</li>
</ol>
<p>本文主要记录用Anchor Free的思想来解决上述目标跟踪状态估计中存在的问题。目前比较主流的都是基于FCOS和CenterNet两种无锚框方式展开的。</p>
<h1 id="FCOS类"><a href="#FCOS类" class="headerlink" title="FCOS类"></a>FCOS类</h1><h2 id="SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines"><a href="#SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines" class="headerlink" title="SiamFC++: Towards Robust and Accurate Visual Tracking with Target Estimation Guidelines"></a>SiamFC++: Towards Robust and Accurate Visual Tracking with Target Estimation Guidelines</h2><p><a href="https://arxiv.org/abs/1911.06188">论文</a> <a href="https://github.com/MegviiDetection/video_analyst">代码</a></p>
<p>针对siam网络分析了之前的工作不合理的地方，提出了4条guidelines：</p>
<p>G1：decomposition of classification and state estimation：跟踪任务可以分解为分类与状态估计。分类影响鲁棒性，状态估计影响精确性。多尺度金字塔的方式忽略了状态估计所以精确性很低；</p>
<p>G2：non-ambiguous scoring：分类得分应该直接表示为目标在视野中存在的置信度分数，而不是像预定义的anchor那样匹配anchor和目标，这样容易产False positive；</p>
<p>G3：prior knowledge-free：跟踪器不应该依赖过多的先验知识（如尺度/长宽比）。现有的方法普遍存在对数据分布先验知识的依赖，阻碍了其泛化能力；</p>
<p>G4：estimation quality assessment：不能直接使用分类置信度来评价状态估计，需要使用独立于分类的质量评估方式。（如RPN系列直接就是选择分类置信度最高的位置进行边框预测，而ATOM，DIMP则另外加入了IOU信息来指导边框调整）</p>
<p>作者依据这4条guidelines设计了SiamFC++，将目标检测中的Anchor Free的FCOS应用到Siamese框架中，整体结构如下，细节部分可以去开头我在b站的专栏。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426104215.png" alt=""></p>
<hr>
<h2 id="SiamCAR-Siamese-Fully-Convolutional-Classification-and-Regression-for-Visual-Tracking"><a href="#SiamCAR-Siamese-Fully-Convolutional-Classification-and-Regression-for-Visual-Tracking" class="headerlink" title="SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking"></a>SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking</h2><p><a href="https://arxiv.org/abs/1911.07241v2">论文</a> <a href="https://github.com/ohhhyeahhh/SiamCAR">代码</a></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426111123.png" alt=""></p>
<p>这一篇和SiamFC++很类似，这里仅标注一些实践细节的差异。</p>
<ul>
<li>backbone采用了改造的resnet50；</li>
<li>multi-stage融合对相关结果拼接用1*1卷积降维/融合，而不是像siamrpn++那样对相关后的分类预测响应图加权相加；</li>
<li>分类和回归均由一个相关引出，而不是每个分支对应一个相关。这样计算量更小效率更高，而性能差不多；</li>
<li>inference阶段为了避免抖动取了中心点周围top-k的均值作为最终结果。</li>
</ul>
<p>细节同样参照开头b站专栏。</p>
<hr>
<h2 id="Siamese-Box-Adaptive-Network-for-Visual-Tracking"><a href="#Siamese-Box-Adaptive-Network-for-Visual-Tracking" class="headerlink" title="Siamese Box Adaptive Network for Visual Tracking"></a>Siamese Box Adaptive Network for Visual Tracking</h2><p><a href="https://arxiv.org/abs/2003.06761">论文</a> <a href="https://github.com/hqucv/siamban">代码</a> <a href="https://www.bilibili.com/read/cv5400217">解读</a></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426112506.png" alt=""></p>
<p>同样是FCOS的应用，比较insight的地方是打标签的时候使用椭圆标签，两个椭圆，小椭圆E2内的点是positive，大椭圆E1外的点是negative，两个椭圆中间的部分为ignore。椭圆标签能够更紧凑地标注正负样本，并且设置了缓冲(ignore)以忽略模棱两可的样本。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426112854.png" style="zoom:67%;" /></p>
<hr>
<h2 id="Fully-Conventional-Anchor-Free-Siamese-Networks-for-Object-Tracking"><a href="#Fully-Conventional-Anchor-Free-Siamese-Networks-for-Object-Tracking" class="headerlink" title="Fully Conventional Anchor-Free Siamese Networks for Object Tracking"></a>Fully Conventional Anchor-Free Siamese Networks for Object Tracking</h2><p><a href="https://www.researchgate.net/publication/335467780_Fully_Conventional_Anchor-Free_Siamese_Networks_for_Object_Tracking">论文</a></p>
<p>将FCOS与级联结构结合，另一个就是分配GT到AFPN层时采用了FCOS一样的思路（划分[0,64], [64,128], [128,∞]）</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426114147.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426114239.png" style="zoom: 50%;" /></p>
<hr>
<h2 id="Ocean-Object-aware-Anchor-free-Tracking"><a href="#Ocean-Object-aware-Anchor-free-Tracking" class="headerlink" title="Ocean: Object-aware Anchor-free Tracking"></a>Ocean: Object-aware Anchor-free Tracking</h2><p><a href="https://arxiv.org/abs/2006.10721">论文</a> <a href="https://github.com/researchmm/TracKit">代码</a> <a href="https://www.bilibili.com/read/cv6615313">解读</a></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426115451.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426115537.png" alt=""></p>
<ul>
<li>anchor-base方法对于弱预测的修正能力较差，因为训练时只考虑了IOU大于阈值的anchor的回归，对于跟踪过程中如果出现overlap很小的anchor很难去refine。而anchor-free可以针对每个点进行预测；</li>
<li>作者设计了一个feature alignment module来从预测框中学习object-aware feature（图2c），从而对物体尺度敏感；</li>
<li>特征融合上采用xy轴膨胀系数不同的膨胀卷积进行融合，不同膨胀的卷积可以捕获不同尺度的特征，提高最终融合特征的尺度不变性。</li>
</ul>
<hr>
<h1 id="CenterNet类"><a href="#CenterNet类" class="headerlink" title="CenterNet类"></a>CenterNet类</h1><h2 id="Siamese-Attentional-Keypoint-Network-for-High-Performance-Visual-Tracking"><a href="#Siamese-Attentional-Keypoint-Network-for-High-Performance-Visual-Tracking" class="headerlink" title="Siamese Attentional Keypoint Network for High Performance Visual Tracking"></a>Siamese Attentional Keypoint Network for High Performance Visual Tracking</h2><p><a href="https://arxiv.org/abs/1904.10128v2">论文</a></p>
<p>这篇将CenterNet和CornerNet结合到跟踪中，分别预测中心点和两个角点，以及运用了CBAM注意力机制强化上下文信息，应该是第一个将CenterNet/CornerNet用进来的，遗憾的是性能没有刷的很高。细节同样参照开头b站专栏。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426164747.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426164810.png" alt=""></p>
<hr>
<h2 id="Accurate-Anchor-Free-Tracking"><a href="#Accurate-Anchor-Free-Tracking" class="headerlink" title="Accurate Anchor Free Tracking"></a>Accurate Anchor Free Tracking</h2><p><a href="https://arxiv.org/abs/2006.07560">论文</a></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426165201.png" alt=""></p>
<p>这篇就是比较典型的CenterNet模式了，预测中心点，中心偏移以及宽高。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426165235.png" alt=""></p>
<p>作者另外设计了backbone，最后在VOT2018性能虽然比siamrpn++略低但是速度是它的3.9倍（136FPS v.s. 35FPS）。</p>
<hr>
<h2 id="Siamese-Keypoint-Prediction-Network-for-Visual-Object-Tracking"><a href="#Siamese-Keypoint-Prediction-Network-for-Visual-Object-Tracking" class="headerlink" title="Siamese Keypoint Prediction Network for Visual Object Tracking"></a>Siamese Keypoint Prediction Network for Visual Object Tracking</h2><p><a href="https://arxiv.org/abs/2006.04078">论文</a> <a href="https://github.com/ZekuiQin/SiamKPN">代码</a></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426171705.png" alt=""></p>
<p>这一篇将casscade的思想结合在centernet类的siamese跟踪器中，看上面图2结构已经很清晰了，KPN结构如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426171748.png" style="zoom:80%;" /></p>
<p>还有一个需要关注的就是每个stage训练的时候分类标签的高斯方差不一样，遵循的原则就是越高的stage峰值越收束。目的即随着级联的进行，监管信号越来越严格。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426171845.png" alt=""></p>
<hr>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="Correlation-Guided-Attention-for-Corner-Detection-Based-Visual-Tracking"><a href="#Correlation-Guided-Attention-for-Corner-Detection-Based-Visual-Tracking" class="headerlink" title="Correlation-Guided Attention for Corner Detection Based Visual Tracking"></a>Correlation-Guided Attention for Corner Detection Based Visual Tracking</h2><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Du_Correlation-Guided_Attention_for_Corner_Detection_Based_Visual_Tracking_CVPR_2020_paper.pdf">论文</a> <a href="https://www.bilibili.com/read/cv6647311">解读</a></p>
<p>作者为了解决跟踪中回归框估计不准确的问题，引入角点检测来得到更紧致的回归框。分析了之前一些角点检测方法在目标跟踪中无法取得好性能的原因，并提出了两阶段的correlation-guided attentional corner detection (CGACD)方法。第一阶段使用siamese网络得到目标区域的粗略ROI，第二阶段通过空间和通道两个correlation-guided attention来探索模板和ROI之间的关系，突出角点区域进行检测。速度可以达到70FPS。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426173132.png" alt=""></p>
<hr>
<h2 id="RPT-Learning-Point-Set-Representation-for-Siamese-Visual-Tracking"><a href="#RPT-Learning-Point-Set-Representation-for-Siamese-Visual-Tracking" class="headerlink" title="RPT: Learning Point Set Representation for Siamese Visual Tracking"></a>RPT: Learning Point Set Representation for Siamese Visual Tracking</h2><p><a href="https://arxiv.org/abs/2008.03467">论文</a> <a href="https://github.com/zhanght021/RPT">代码</a> <a href="https://zhuanlan.zhihu.com/p/257854666">原作者解读</a></p>
<p>现有的跟踪方法往往采用矩形框或四边形来表示目标的状态（位置和大小），这种方式忽略了目标自身会变化的特点（形变、姿态变化），因此作者受启发自Reppoints检测方法，采用表示点（Representative Points）方法来描述目标的外观特征，学习表示点的特征，根据表示点的分布确定目标的状态，实现更精确的目标状态估计。</p>
<p>具体可以参考原作者在知乎的解读，该方法取得了VOT2020-ST的冠军。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426173720.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426173734.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/3-anchorfree-siamese/20210426173759.png" style="zoom:80%;" /></p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>综述</tag>
        <tag>Anchor-free</tag>
      </tags>
  </entry>
  <entry>
    <title>STMTrack: Template-free Visual Tracking with Space-time Memory Networks</title>
    <url>/tracking/34-stmtrack/</url>
    <content><![CDATA[<p><img src="https://github.com/kongbia/picgo/raw/master/blog/34-stmtrack/20210427102245.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2104.00324">论文</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>离线训练的Siamese跟踪器已经充分完全挖掘了第一帧模板信息，但它们抵抗目标外观变化的能力依然有限。现有的模板更新机制大多依赖耗时的数值优化或复杂的手工设计策略，这阻碍了它们的实时跟踪和实际应用。本文提出了一种基于时空记忆网络的跟踪框架，该框架能够充分利用与目标相关的历史信息，从而更好地适应跟踪过程中的外观变化。这样避免了模板更新，所以叫template-free。主要创新点包括：</p>
<ul>
<li>引入记忆机制存储目标的历史信息，引导跟踪器聚焦在当前帧中信息最丰富的区域；</li>
<li>memory network的像素级相似度计算能够生成更精确的目标框。</li>
</ul>
<p>运行速度37 FPS</p>
<span id="more"></span>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/34-stmtrack/20210427102842.png" alt=""></p>
<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>整体框架如图2所示，主要包括三部分：特征提取网络、时空记忆网络和头部预测。特征提取网络包括<font color=LightGreen>绿色</font>的记忆分支和<font color=LightBlue>蓝色</font>的查询分支。记忆分支输入多个历史帧以及对应的前背景标签，查询分支输入当前帧。特征提取后，时空记忆网络从所有记忆帧中检索与目标相关的信息，生成综合特征图进行最后的分类回归。</p>
<h2 id="Feature-Extraction-Network"><a href="#Feature-Extraction-Network" class="headerlink" title="Feature Extraction Network"></a>Feature Extraction Network</h2><p>对memory的每个分支，特征提取为：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/34-stmtrack/20210427104555.png" style="zoom:80%;" /></p>
<p>$m_i$ 表示第i帧图像，$c_i$ 表示第i个前背景标签（根据gt构建的0/1 mask），$ \varphi_0^m $ 和 $g$ 将图像和标签统一到同一维度从而相加，$ h_m $ 对特征降维到512。</p>
<p>对于query分支：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/34-stmtrack/20210427105233.png" style="zoom:80%;" /></p>
<p>$ \varphi^m $ 和 $ \varphi^q $ 结构一样但<strong>参数不共享</strong>。</p>
<h2 id="Space-time-Memory-Network"><a href="#Space-time-Memory-Network" class="headerlink" title="Space-time Memory Network"></a>Space-time Memory Network</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/34-stmtrack/20210427111921.png" alt=""></p>
<p>首先计算memory的 $f_m $ 和 query的 $f_q $  每个像素之间的相似性 $\omega \in THW \times HW $</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/34-stmtrack/20210427111950.png" style="zoom:80%;" /></p>
<p>这个操作类似non-local或self-attention，相当于用query去检索历史帧中与目标相关的信息作为<strong>软权重</strong>来加权 $f_m $。最后将加权后的结果与$f_q $ 拼接起来</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/34-stmtrack/20210427112745.png" style="zoom:80%;" /></p>
<p>与non-local区别在于该方法采用相似性矩阵作为软权值从多个记忆帧中检索目标信息，而不是计算特征图中每个像素对的非局部自注意。</p>
<h2 id="Head-Network"><a href="#Head-Network" class="headerlink" title="Head Network"></a>Head Network</h2><p>常规的FCOS类预测头。</p>
<h2 id="Inference-Phase"><a href="#Inference-Phase" class="headerlink" title="Inference Phase"></a>Inference Phase</h2><p>训练和推理时记忆帧的数量是可以不一致的，因为不影响优化参数的数量。</p>
<p>推理时的记忆帧采样策略为：首先保留最可靠的第一帧和最相似的上一帧，剩下的帧数均匀采样</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/34-stmtrack/20210427113629.png" style="zoom:80%;" /></p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p>主要分析了是否共享backbone参数，前背景标签的作用，记忆帧的数量的影响。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/34-stmtrack/20210427114407.png" style="zoom:80%;" /></p>
<p>加了 fb_label 之后，不共享backbone参数效果更好。可能是因为此时两个分支的特征空间已经不一样了，期望学到的东西也是不一样的。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/34-stmtrack/20210427115130.png" style="zoom:80%;" /></p>
<p>随着记忆帧的数量增加，性能都是先增后减。</p>
<p>训练时帧数越多，可以训练的目标模式越多，但与当前帧相似的帧也会越多。在这种情况下，网络倾向于比较最相似的图像对，而不是学习当前帧与有杂波背景或部分遮挡的帧之间的相似性。</p>
<h2 id="Comparison-with-the-stateoftheart"><a href="#Comparison-with-the-stateoftheart" class="headerlink" title="Comparison with the stateoftheart"></a>Comparison with the stateoftheart</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/34-stmtrack/20210427115649.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/34-stmtrack/20210427115737.png" style="zoom:80%;" /></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/34-stmtrack/20210427115757.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/34-stmtrack/20210427115804.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/34-stmtrack/20210427115816.png" style="zoom:80%;" /></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>提出了一种新的基于时空记忆网络的跟踪框架。该框架摒弃了传统的基于模板的跟踪机制，使用多个记忆帧和前背景标签映射来定位查询帧中的目标。在时空记忆网络中，通过查询帧自适应地检索存储在多个记忆帧中的目标信息，使跟踪器对目标变化具有较强的自适应能力。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>CVPR2021</tag>
        <tag>时序</tag>
      </tags>
  </entry>
  <entry>
    <title>Real-Time Visual Object Tracking via Few-Shot Learning</title>
    <url>/tracking/35-FSL-tracking/</url>
    <content><![CDATA[<p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427173306.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2103.10130">论文</a></p>
<p>跟踪可以看成是一个特殊的 few-shot learning (FSL) 问题，本文提出了一个通用的两阶段框架，它能够使用大量的FSL算法并且保持较快的速度。第一阶段通过SiamRPN生成若干潜在候选框，第二阶段通过少样本分类的思想对候选框进行分类。按照这种coarse-to-fine结构，第一阶段为第二阶段提供稀疏的样本，在第二阶段可以更方便、高效地进行多种FSL算法。作者选取了几种基于优化的少样本学习方法进行证明。此外，该框架可将大多数FSL算法直接应用到视觉跟踪中，使研究人员能够在这两个领域相互交流。</p>
<span id="more"></span>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>跟踪任务要求在有限的时间内通过少量数据学习对目标和背景的分类，这与FSL任务非常相似。在少样本学习中，我们假设训练任务和新任务之间存在共享的元知识；在视觉跟踪领域，这可以解释为模型在序列中跟踪任何未见过对象的适应性。</p>
<p>已经有一些方法将FSL的概念引入跟踪了，比如DiMP和一些MAML的方法，它们将在线更新纳入离线训练阶段作为内环 (inner loop)，使得在线更新可以由手工设计转变成数据驱动。然而，这些方法大多局限于对<strong>整个图像</strong>的<strong>特定卷积核</strong>进行优化设计，而不是像在FSL中，使用<strong>稀疏样本</strong>进行<strong>更定制化的权值学习</strong>(例如矩阵乘法因子)，这限制了直接引入各种新的FSL算法，因为直接应用各种FSL算法，将整个图像的所有位置作为输入样本，必然会牺牲其跟踪速度，而且大量简单负样本会在学习中占主导导致模型判别力下降。</p>
<p>因此作者提出这个通用的两阶段级联结构，在第一阶段过滤掉大量简单负样本，从而使得第二阶段可以应用各种FSL算法在信息丰富的稀疏样本上实现高效的跟踪。作者选取了几种具有不同目标函数，优化方法，或解空间的少样本学习方法进行验证，速度在40-60 FPS。</p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Cascaded-Tracking"><a href="#Cascaded-Tracking" class="headerlink" title="Cascaded Tracking"></a>Cascaded Tracking</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427203740.png" alt=""></p>
<p>整体跟踪框架如图1所示，第一阶段利用SiamRPN++生成若干个候选框，经过ROI Align后送入第二阶段进行少样本分类。第二阶段是一个N-shot-2-way的分类任务，所有候选使用在线生成的伪标签进行标记。</p>
<h2 id="Second-Stage-as-Few-Shot-Learning"><a href="#Second-Stage-as-Few-Shot-Learning" class="headerlink" title="Second Stage as Few-Shot Learning"></a>Second Stage as Few-Shot Learning</h2><p>首先用数学形式定义任务，公式比较多直接贴图：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427205448.png" style="zoom:80%;" /></p>
<p>其中 $\theta$ 是meta-training stage (inner loop)中使用base loss $ L_{base} $ 优化得到的sequence-specific参数；然后用$\theta$ 去计算meta loss $ L_{meta} $，用于 meta-testing (outer loop) stage 来更新few-shot learner 的参数 $\rho$ 。而$\varphi $ 表示feature embedding的参数，就是前面一大堆特征提取之类的。</p>
<p>这里的meta loss是用于分类，所以采用focal loss：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427211809.png" style="zoom:80%;" /></p>
<p>其中 $\gamma$ 是可学习的缩放因子，$\theta^T$ 是前景和背景的权重。$\alpha, \beta$ 是focal loss的超参数。</p>
<p>few-shot learner $\Lambda$ 的选择对于公式1的影响巨大，它会被用来分类第一阶段得到的稀疏候选样本，下面介绍几种常用的基于优化（optimization-based）的方法。</p>
<h2 id="Optimization-Based-Few-Shot-Learners"><a href="#Optimization-Based-Few-Shot-Learners" class="headerlink" title="Optimization-Based Few-Shot Learners"></a>Optimization-Based Few-Shot Learners</h2><p>首先默认目标是一个线性凸优化问题。</p>
<p><strong>RR-prim-itr</strong> 就是在原空间求解岭回归问题，base loss $ L_{base} $ 为L2 loss，可以看成MAML的一种特例。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427215927.png" style="zoom:80%;" /></p>
<p>其中 $\omega_n$ 是每个样本的权重，为了加速优化，借鉴DiMP中的最速梯度下降：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427220212.png" style="zoom:80%;" /></p>
<p><strong>RR-dual-itr</strong> 迭代求解对偶空间中的岭回归，$\theta$ 当成训练集的特征向量的线性组合。对偶变量 $a$ 作为权重因子。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427220526.png" style="zoom:80%;" /></p>
<p>将公式5带入公式3，得到优化目标</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427220643.png" style="zoom:80%;" /></p>
<p>用二次规划的方式求解对偶变量 $a$ ，同样可以利用GPU加速。此外，特征向量在训练集中进行线性组合可以缓解过拟合问题。</p>
<p><strong>RR-dual-cls</strong> 在对偶空间中开发了岭回归的封闭解用于分类。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427221514.png" style="zoom:80%;" /></p>
<p>优势就是把原始闭式解形式中的 $ \Phi_T\ \Phi$ 变成了 $ \Phi \Phi_T$，使得计算量由 $O(Nd^2)$ 下降到了$O(N^2d)$ ，样本量N远小于特征维数d。</p>
<p><strong>SVM-dual-itr</strong> 在对偶空间迭代求解稀疏核用于线性分类。主要解决岭回归容易过拟合以及对噪声不鲁棒的问题。用SVM替换了最小二乘，细节不太明白，要去看原文。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427222444.png" style="zoom:80%;" /></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427222555.png" style="zoom:80%;" /></p>
<h2 id="Online-Tracking"><a href="#Online-Tracking" class="headerlink" title="Online Tracking"></a>Online Tracking</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427230231.png" alt=""></p>
<p><strong>Candidate Selection</strong> 候选框根据第一阶段的检测结果用阈值为0.2的NMS进行筛选，再通过ROI Align作用于第二阶段。初始化时选择24个样本，IOU最高的作为正样本，其余为负样本，并通过数据增广生成额外的8个正样本，共同用于初始化few-shot learner。跟踪过程中，选择NMS后M=8个样本送入few-shot learner进行判断输出最终结果。用融合得分（两个阶段的得分加权相加）top-k个候选框更新support set中最老的样本，其中最高的为正样本，其余为负样本，k=4。</p>
<p><strong>Support Set Maintenance</strong> 先入先出（FIFO），对于每个样本赋予权重 $\omega$，随着与当前帧的间隔而指数衰减。原空间中的求解存储1000个样本，对偶空间只有60个，一方面是对CPU资源要求高，另外60个的性能已经很好。此外，第一帧的正样本永远会留在support set中。</p>
<p><strong>Few-Shot Learner Update</strong> 原空间用公式4更新，对偶空间用滑动平均更新</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427231851.png" style="zoom:80%;" /></p>
<p><strong>Stage Fusion</strong> 将两个阶段的分类和回归结果融合，第二阶段除了得到分类得分，还会进行类似RCNN的回归refine。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427232137.png" style="zoom:80%;" /></p>
<h1 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h1><p>训练类似DiMP，每个task从一个视频中选择3个support和2个query。对于第二阶段，NMS后采样16个样本，IOU大于0.8为正，小于0.2为负。每张图片总共8个样本，其中最多两个正样本。meta-training和meta-test均按照上述准则。第一阶段的SiamRPN++用ATSS进行label assign。</p>
<p>训练loss如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427233344.png" style="zoom:80%;" /></p>
<p>两个阶段都需要分类回归，分类为focal loss，回归为L1 loss。注意这里rcnn的回归是在第一阶段的正样本基础上进行的。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427234324.png" alt=""></p>
<p>比较了baseline（第一栏），不使用FSL的基于全连接的距离度量方法（第二栏），metric-based少样本学习方法（第三栏），以及本文使用的四种optimization-based少样本学习方法（第四栏）。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427234748.png" alt=""></p>
<p>只用第一阶段方法，SiamRPN++远不如DiMP；加入第二阶段后，这个差距被消除了，且速度将近领先了3倍。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427234945.png" alt=""></p>
<p>support set数量的影响，对于不同数据集最佳的数量是不一样的。但是即使是最低限度的M=40鲁棒性依然不错。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210428093340.png" alt=""></p>
<p>最后可视化了候选框及置信度，可以看到第二阶段对于区分目标和干扰物非常有效。</p>
<h2 id="Comparison-with-State-of-the-Arts"><a href="#Comparison-with-State-of-the-Arts" class="headerlink" title="Comparison with State-of-the-Arts"></a>Comparison with State-of-the-Arts</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427235535.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427235554.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/35-FSL-tracking/20210427235609.png" alt=""></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文的创新点主要集中在两阶段和少样本学习。这两个东西单独来看都分别有人做过了，像SPM，SPLT就是先检候选框再分类的两阶段方法；而DiMP还有一些用了MAML的跟踪方法都属于少样本学习。作者将二者结合，最大的好处在于第二阶段得到的稀疏样本更适合FSL的任务设置，可以不用局限于任何特定类型的FSL算法，大幅加强了跟踪和FSL的联系。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>FSL</tag>
        <tag>arxiv</tag>
      </tags>
  </entry>
  <entry>
    <title>Learning Target Candidate Association to Keep Track of What Not to Track</title>
    <url>/tracking/36-learning-target-candidate-association/</url>
    <content><![CDATA[<p><img src="https://github.com/kongbia/picgo/raw/master/blog/36-learning-target-candidate-association/20210428110851.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2103.16556">论文</a> <a href="https://github.com/visionml/pytracking">代码</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>Martin参与的新作，出发点和之前的<a href="https://arxiv.org/abs/2003.11014">KYS</a>类似，均指出仅外观模型不足以区分目标和干扰物，因此需要对所有潜在目标保持跟踪。不同的是KYS是通过一个传播模块隐式地跟踪所有对象，最后作用于外观模型的输出来抑制干扰响应；而本文则是借助SuperGlue显式地匹配帧间所有的候选对象，构建跟踪链，有点多目标跟踪的意思，可解释性也更强。</p>
<ul>
<li>主流跟踪方法大多聚焦于建立强大的外观模型，然而仅依靠外观模型对于干扰物的鲁棒性较差；</li>
<li>作者提出另一种思路，即对干扰物也保持跟踪。为此构建一个可学习的关联网络（受启发自SuperGlue），允许在帧与帧之间传播所有候选目标；</li>
<li>针对跟踪数据集没有对干扰物标注的问题，提出了一种结合部分标注和自监督的训练策略。</li>
</ul>
<span id="more"></span>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/36-learning-target-candidate-association/20210428113928.png" alt=""></p>
<p>整体框架如上图，base tracker使用SuperDiMP预测出分数图$s$，选择高得分的几个位置用于生成目标候选 $v_i$。然后对每个候选提取一系列特征，包括分类分数$s_i$，位置$c_i$，和 backbone提取的外观模型$f_i$，将这些特征编码编码成一个单独的特征向量$z_i$。将当前帧和前一帧的所有候选送入candidate embedding network，一起处理得到每个候选对象的丰富嵌入$h_i$。最后利用这些特征来计算相似矩阵S，并使用最优匹配策略估计两个连续帧之间的候选分配矩阵A。</p>
<p>有了这个分配概率矩阵，就可以建立当前帧的所有候选目标$O$与上一帧所有识别出来的目标$O’$之间的关联，包括消失和新出现的目标。用这种传播策略来推断当前帧中的真实目标对象$\hat{o}$。此外，在在线更新目标分类器时，通过计算目标检测置信度$\beta$来管理存储和控制样本权重。</p>
<h2 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h2><p>首先将问题公式化，定义候选目标集合 $V = \{v_i\} _ {i=1}^N$，N为候选个数，$V’$和$V$分别表示前一帧和当前帧的候选集合。两帧的候选关联问题就是寻找$V’$和$V$之间的分配矩阵$A$。若$v’ _ {i}$和$v_{j}$关联，那么$A_{i,j}=1$，否则$A_{i,j}=0$。</p>
<p>实际操作中，并不是每个候选都能找到相应的匹配，因此引入dustbin来处理未匹配的节点。大致的想法就是额外增加一行一列来存放未匹配的节点。比如候选$v_j$只在集合$V$中出现，则$A_{N’+1,j}=1$；类似的，若$v’ _ i$ 在集合$V$中没有匹配对象，则$A_{i,N+1}=1$。</p>
<p>作者设计了一种可学习方法来预测这个分配矩阵A，首先需要提取候选对象的特征表示，接下来讨论。</p>
<h2 id="Target-Candidate-Extraction"><a href="#Target-Candidate-Extraction" class="headerlink" title="Target Candidate Extraction"></a>Target Candidate Extraction</h2><p>目标候选对象需要满足两个条件，响应得分是局部最大，且要超过一定阈值。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/36-learning-target-candidate-association/20210428170210.png" style="zoom:;" /></p>
<p>实现时使用$5 \times 5$ max-pooling找到局部极值，阈值$\tau=0.05$。</p>
<p>接下来为目标候选构建特征编码</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/36-learning-target-candidate-association/20210428170144.png"  /></p>
<p>包括响应得分$s_i$，位置$c_i$，和 backbone在该位置提取的外观模型$f_i$。$\psi$ 表示MLP，将 s 和 c 变换到和 f 一样的维度。</p>
<h2 id="Candidate-Embedding-Network"><a href="#Candidate-Embedding-Network" class="headerlink" title="Candidate Embedding Network"></a>Candidate Embedding Network</h2><p>为了进一步丰富编码特征，特别是便于提取特征的同时又能识别邻近的候选特征，作者引入了候选嵌入网络。这里借鉴了SuperGlue中的self-attention和cross-attention交换不同节点的信息。最后经过一个线性变换，得到每个候选$v_i$的编码$h_i$。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/36-learning-target-candidate-association/20210428171233.png" alt=""></p>
<h2 id="Candidate-Matching"><a href="#Candidate-Matching" class="headerlink" title="Candidate Matching"></a>Candidate Matching</h2><p>用特征点积来度量候选$v’ _ {i} \in V’$和$v_j \in V$之间的相似性，$S_{i,j}=&lt;h’_i,h_j&gt;$，h是对应的特征编码。</p>
<p>有了相似性得分矩阵S，接下来需要构建分配矩阵A，通过最大化总体得分$\Sigma_{i,j} S_{i,j} A_{i,j} $ 可得到A，这是一个最优传输问题。</p>
<p>在此之前，还需要考虑dustbin，它是一个虚拟概念，没有对应的特征编码h，因此不能直接预测相似性得分。只有当一个候选对象与所有其他候选对象的相似度分数足够低时，它才属于dustbin。上面得到的相似度矩阵S仅代表了不考虑dustbin的候选对象之间的初始关联预测。本文没有提及如何处理dustbin的相似性得分，参照SuperGlue是给所有dustbin赋予一个相同的可学习参数。</p>
<p>下面就是给这个最优匹配问题设计约束条件：</p>
<ul>
<li>当$v’ _ i$和$v_j$匹配时，需要同时满足 $\Sigma_{i=1}^{N’} A_{i,j}=1$ 和 $\Sigma_{j=1}^{N} A_{i,j}=1$，确保一对一的匹配，即(i, j)所在行和列仅有唯一的匹配位置；</li>
<li>所有未匹配到其他候选的候选必须匹配到dustbin，数学表达为$\Sigma_{j} A_{N’+1,j}=N-M$ 和$\Sigma_{i} A_{i,N+1}=N’-M$，其中$M = \Sigma_{(i\le N’,j \le N)} A_{i,j}$表示候选之间匹配上的数量。</li>
</ul>
<p>最后通过Sinkhorn算法（迭代）解出最优分配矩阵。</p>
<p>这一套流程也是借鉴了SuperGlue，这里贴出它的示意图。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/36-learning-target-candidate-association/20210428181327.png" alt=""></p>
<h2 id="Learning-Candidate-Association"><a href="#Learning-Candidate-Association" class="headerlink" title="Learning Candidate Association"></a>Learning Candidate Association</h2><p>上述流程的训练需要知道所有候选之间的匹配关系作为标签，但跟踪数据集只有唯一目标的标注。所以作者提出了部分监督和自监督结合的方式。</p>
<p><strong>部分监督损失：</strong>只对有标注的那对目标候选计算loss，另外为了模拟遮挡和重检测，人为地排除一些候选并将其替换为dustbin</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/36-learning-target-candidate-association/20210428182311.png"  /></p>
<p>其中$(l’,l)=(i,j)$，$(l’,l)=(N’+1,j)$, $(l’,l)=(i,N+1)$.</p>
<p><strong>自监督损失：</strong>自监督的一个候选$V$是由另外的候选$V’$增广变换而来的，因此它们可以构建一一对应关系$C=\{(i,i)\}^N_{i=1}$，增广策略包括随机平移位置$c_i$，增减响应得分$s_i$，以及变换图像对特征$f_i$进行变换。此外，也像上面部分监督一样模拟了遮挡和重检测。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/36-learning-target-candidate-association/20210428202146.png"  /></p>
<p>最后将二者相加得到最终loss。</p>
<p><strong>Data Mining:</strong> 有价值的训练样本是那些出现干扰物导致跟踪失败或者跟踪置信度很低的子序列，这样才能包含更多候选供网络学习。因此先使用base tracker跑一遍找到这些困难样本。</p>
<p><strong>Training Details:</strong> 先训练SuperDiMP（这里不训练在线分类的参数，可能是因为想要获取更多失败的样本），跑一遍数据找到难样本。然后冻结base tracker的参数只训练后半部分网络。</p>
<h2 id="Object-Association"><a href="#Object-Association" class="headerlink" title="Object Association"></a>Object Association</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/36-learning-target-candidate-association/20210428205314.png" alt=""></p>
<p>在线跟踪时对所有潜在对象保持跟踪，并构建跟踪链，如图3所示。若当前的候选对象都与该对象没有关联，那么该对象就会从场景中消失。若有新的对象出现则新增链并赋予新的对象id。对于关联上的对象，将得分$s_i$添加到历史分数中。此外，当候选对应关系不确定，即分配概率小于$\tau=0.75$时，我们删除旧的对象并创建一个新的对象。</p>
<p>这样所有候选对象都与一个已经存在的或新创建的对象相关联，若上一帧检测到的真实目标在当前帧有关联对象，则暂定该关联对象是目标。为了避免该关联对象是干扰，还需将它的历史得分与当前其他候选对象的得分进行比较，如果另一个对象在当前帧中获得的分类分数高于当前选择对象在过去获得的所有分数，我们就选择这个对象作为目标；否则还是保持当前选择对象不变。</p>
<p>若上一帧检测到的真实目标在当前帧没有关联对象，则进行重检测，需满足：1. 分类得分最高；2. 分类得分大于阈值$\eta =0.25$。</p>
<p>整个算法流程如下图：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/36-learning-target-candidate-association/20210429161216.png" alt=""></p>
<center>'.' 表示访问对象的属性</center>

<h2 id="Memory-Sample-Confidence"><a href="#Memory-Sample-Confidence" class="headerlink" title="Memory Sample Confidence"></a>Memory Sample Confidence</h2><p>base tracker在线更新的训练样本采用先入先出的策略进行替换，并仅基于存在的寿命时间对样本加权。本文则是将分类得分也考虑进来，计算在线优化损失时综合考虑了寿命时间和分类得分对样本进行加权：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/36-learning-target-candidate-association/20210429165636.png"  /></p>
<p>$\alpha$ 表示样本存在的时间，越早的样本值越小；$\beta$表示分类得分。此外，训练样本更新时替换的是$\alpha \beta$最小的样本而不是简单替换最早的样本。</p>
<p><strong>Inference details</strong></p>
<p>base tracker搜索区域取决于当前估计的边界框大小。当目标物体被遮挡或出视野时，跟踪器通常只检测到目标的一小部分，并估计出比之前帧更小的边界框，相应的搜索区域也会变小，这对于跟踪失败后的重捕是不利的。因此，如果在目标丢失前搜索区域大幅缩小，我们将搜索区域重置为以前的大小，以便于重新检测。</p>
<p>此外，如果只有一个高分目标出现在前一帧和当前帧中。我们选择这个候选对象作为目标，省略运行目标候选关联网络来加速跟踪。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/36-learning-target-candidate-association/20210429165050.png" alt=""></p>
<p>memory sample confidence 在LaSOT提升比较大，NFS和UAV123没啥影响，说明它在长时跟踪中的作用更大。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/36-learning-target-candidate-association/20210429165254.png" alt=""></p>
<p>比较了提出的部分监督损失，自监督损失和数据挖掘的作用，每一项都有提升。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/36-learning-target-candidate-association/20210429165349.png" alt=""></p>
<p>表3分析了学习在线分类器过程中加入分类得分的影响。第一列表示增加了分类得分来考虑替换训练样本，第二列表示在计算在线损失时增加分类得分对样本加权，第三列表示在线学习时忽略了得分较低的低质量样本。</p>
<h2 id="State-of-the-art-Comparison"><a href="#State-of-the-art-Comparison" class="headerlink" title="State-of-the-art Comparison"></a>State-of-the-art Comparison</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/36-learning-target-candidate-association/20210429172159.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/36-learning-target-candidate-association/20210429172214.png" alt=""></p>
<p>对LaSOT数据集，$T&lt;0.7$时本文方法指标明显更高，证明其鲁棒性。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/36-learning-target-candidate-association/20210429172329.png" alt=""></p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>ICCV2021</tag>
      </tags>
  </entry>
  <entry>
    <title>Learning to Filter: Siamese Relation Network for Robust Tracking</title>
    <url>/tracking/37-siamrn/</url>
    <content><![CDATA[<p><img src="https://github.com/kongbia/picgo/raw/master/blog/37-siamrn/20210508213527.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2104.00829">论文</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><ul>
<li>Siamese跟踪器的训练设置只是在大量的图像对中匹配同一个目标，而忽略了它们之间的区别，因此对相似干扰物的判别能力不够好；</li>
<li>分类和回归是独立优化的，造成二者之间的不匹配。具体来说，分类置信度最高的位置对应的目标框可能并不是最准确的（类似检测中general focal loss等文章的观点）。</li>
</ul>
<p>针对上述问题，作者提出了两个模块：</p>
<ul>
<li><strong>Relation Detector (RD)</strong> 构造了一个2-way-1-shot的少样本学习方法来过滤干扰物。并且使用对比训练策略 (contrastive training strategy)，不仅学习匹配相同的目标，而且学习如何区分不同的目标 ；</li>
<li><strong>Refinement Module (RM)</strong> 将RD和分类分支获得的信息进行整合，细化跟踪结果。RM可以联合优化分类分支和回归分支，缓解两个分支的不匹配。</li>
</ul>
<span id="more"></span>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/37-siamrn/20210509122001.png" alt=""></p>
<p>整体框架如图2，baseline是SiamBAN，经过特征提取后，将目标和<strong>回归分支</strong>生成的proposal送入RD计算它们之间的相似性(metric-based)，RD输出的结果与分类分支的结果融合后得到最终的目标输出。</p>
<h2 id="Relation-Detector"><a href="#Relation-Detector" class="headerlink" title="Relation Detector"></a>Relation Detector</h2><p>比较目标和proposal相似性的最简单的方法就是各种线性距离（欧式距离、余弦距离等），然而面对难以分辨的干扰物时很容易失效。因此，本文提出了一种自适应的非线性比较器。该方法受启发自CVPR2020的 Few-Shot Object Detection with Attention-RPN and Multi-Relation Detector，是一个metric-based的少样本学习方法，具体包括Global Detector, Local Detector, and Patch Detector三个模块，如图3所示。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/37-siamrn/20210509165405.png" alt=""></p>
<p>回归分支响应图的每个位置都会预测一个框，提取其ROI特征(query)，与目标模板的ROI特征(support)共同送入3个Detector中：</p>
<ul>
<li>Global Detector 将目标和proposal的ROI特征拼接后做全局池化，经过几层FC得到global分数；</li>
<li>Local Detector 将两个ROI特征做DW-Corr得到local分数；</li>
<li>Patch Detector 将两个ROI特征拼接后经过若干卷积得到patch分数。</li>
</ul>
<p>最后将三个得分加起来就得到最终的匹配相似性分数，该分数能够同时考虑全局、局部以及patch之间的关系。</p>
<h2 id="Contrastive-Training-Strategy"><a href="#Contrastive-Training-Strategy" class="headerlink" title="Contrastive Training Strategy"></a>Contrastive Training Strategy</h2><p>RD的训练是一个2-way-1-shot的少样本学习过程，2-way表示目标和非目标两类。在训练RD时，作者不仅对属于目标类别的物体进行匹配，还对非目标类别的干扰物进行区分。因此构建一个三元组训练集 $(s_c, q_c, s_n)$，其中$s_c$和$s_n$分别表示positive support图像和negative support图像，$q_c$是query图像。$s_c, q_c$取自同一个视频，而$s_n$取自其他视频（表示干扰物）。通过输入的三元组图像可以生成不同的样本组合，定义$s_p$为positive support的gt，$p_p$表示positive proposal，$n_n$表示negative support的gt，$p_n$表示negative proposal，将它们结合可以得到不同的样本对 $(s_p, p_p), (s_p, p_n), (n_n, p_p / p_n)$（这些样本对就是图3中输入的support和query），其比例为1: 2: 1。</p>
<p>在训练初期，应用简单样本可以使模型稳定收敛。为了进一步增强模型的判别能力，在训练中后期引入难样本挖掘，包括离线和在线两种方式。在线就是从IOU小于0.2的proposal中选择得分最高的作为难负样本；离线就是像SiamRCNN一样构建索引表，从中选取在嵌入空间中的最接近的难样本。</p>
<h2 id="Refinement-Module"><a href="#Refinement-Module" class="headerlink" title="Refinement Module"></a>Refinement Module</h2><p>分类和回归的独立优化导致分类得分最高的位置对应的框不一定是最准确的，甚至可能都不是目标，因此作者通过将RD嵌入孪生框架使得分类和回归的学习能够统一。具体来说，首先将RD的输出转换成$25 \times 25 \times 1$的匹配分数图，这个输出就反映了每个位置预测框内的物体与目标之间的相似性；将其作为权重与分类分支的分数图进行元素点乘，可以过滤掉背景中的干扰；最后将refine后的分数图经过一层卷积得到最后的分类得分，并获取最大响应位置对应的预测框。图4展示了一些响应图可视化结果，可以看到，RM将回归分支和分类分支的信息结合起来预测目标位置，从而缓解了不匹配的问题。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/37-siamrn/20210509213558.png" style="zoom:80%;" /></p>
<h2 id="Ground-truth-and-Loss-function"><a href="#Ground-truth-and-Loss-function" class="headerlink" title="Ground-truth and Loss function"></a>Ground-truth and Loss function</h2><p>大部分都和SiamBAN一样，增加的RD模块采用MSE loss</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/37-siamrn/20210509215957.png" alt=""></p>
<p>其中$r_{i,j}$表示位置$(i,j)$处的relation score；$y_{i,j}$是对应标签，上文提及的三种样本对中，$(s_p, p_p)$标签为1，$(s_p, p_n), (n_n, p_p / p_n)$标签为0。</p>
<h2 id="Training-and-Inference"><a href="#Training-and-Inference" class="headerlink" title="Training and Inference"></a>Training and Inference</h2><p>训练首先从来自同一个序列的模板和搜索区域中选择16个正样本和48个负样本来训练分类和回归分支，然后再加入另一个序列中的搜索图像（负例）用于生成RD的训练样本。第5个epoch开始在线难样本挖掘，第15个epoch开始离线难样本挖掘。整个网络端到端训练不需要fine-tune。</p>
<p>推理阶段第一帧先提取模板特征并通过precise ROI pooling提取ROI特征。后续帧对回归分支每个位置的输出生成proposal，提取proposal的ROI特征与模板ROI特征共同送入RD计算它们之间的关系，最后将RD的输出与分类分支的输出送入RM得到最终结果。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Comparison-with-the-state-of-the-art"><a href="#Comparison-with-the-state-of-the-art" class="headerlink" title="Comparison with the state-of-the-art"></a>Comparison with the state-of-the-art</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/37-siamrn/20210509223013.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/37-siamrn/20210509223028.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/37-siamrn/20210509223105.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/37-siamrn/20210509223114.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/37-siamrn/20210509223124.png" alt=""></p>
<h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p>比较了多尺度预测和使用不同组合的Relation Head的结果。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/37-siamrn/20210509223228.png" alt=""></p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>CVPR2021</tag>
        <tag>FSL</tag>
      </tags>
  </entry>
  <entry>
    <title>Updatable Siamese Tracker with Two-stage One-shot Learning</title>
    <url>/tracking/38-siamtol/</url>
    <content><![CDATA[<p><img src="https://github.com/kongbia/picgo/raw/master/blog/38-siamtol/20210510102825.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2104.15049">论文</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>本文主要解决孪生跟踪器缺乏在线更新能力的问题。传统的线性模板更新难以处理目标的不规则变化和采样噪声，造成跟踪漂移；而一些像updatenet采用网络进行自适应更新的方法，其更新网络和跟踪器在结构上是分离的，不能从联合训练中受益，也不能以最佳方式合作。</p>
<p>为了实现高质量的自适应更新，作者从 one-shot learning的角度提出一个two-stage one-shot learner，利用不同阶段的目标样本预测分类器的参数。具体来说，除了使用模板分支来学习初始目标特征，作者额外增加了一个输入分支用于捕获后续帧中的目标特征，并设计了一个残差模块来使用这些特征更新初始模板。通过残差学习融合多帧目标特征，跟踪器可以用更合适的模板跟踪当前目标。此外，还设计了一种多方面(multi-aspect)的训练损失来避免过拟合。</p>
<span id="more"></span>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="One-shot-learning-formulation"><a href="#One-shot-learning-formulation" class="headerlink" title="One-shot learning formulation"></a>One-shot learning formulation</h2><p>SiamRPN指出孪生跟踪框架是一个one-shot learner，即通过初始帧的一次学习使得模型能够跟踪到后续帧中的目标，本节首先对其进行公式化定义。</p>
<p>目标跟踪的典型框架是判别分类器$\varphi (x,W)$，目标是在训练数据集上找到能使总损失L最小化的参数W：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/38-siamtol/20210510115919.png" alt=""></p>
<p>$n$表示训练样本数，$l_i$是样本$x_i$的标签。尽管分类器在目标跟踪上具有很强的竞争力，但需要大量计算量和样本在线训练学习。</p>
<p>另外一种跟踪框架是孪生网络，目标是学习模板和搜索区域之间的相似性度量：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/38-siamtol/20210510152537.png" alt=""></p>
<p>$z_i$和$x_i$分别表示模板和搜索区域，$\varphi ‘$和 $\zeta$ 分别表示特征提取网络和匹配网络。</p>
<p>进一步分析，我们发现公式2的孪生网络模型可以被重新解释为类似公式1的one-shot分类器模型。对于分类器$\varphi (x,W)$，若仅通过一个感兴趣样本$z_i$就能学到分类器参数$W$，那么这就是一个one-shot learner。因此，孪生网络的模板分支可以看成是一个元学习函数$\omega$，它将模板特征映射成分类器参数$W$；而搜索分支和互相关就是一个检测器，整个目标函数定义如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/38-siamtol/20210510154435.png" alt=""></p>
<p>至此，我们就把孪生框架解释为了one-shot learning，图1直观展示了这种表达方式</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/38-siamtol/20210510155519.png" alt=""></p>
<h2 id="Two-stage-One-shot-learner"><a href="#Two-stage-One-shot-learner" class="headerlink" title="Two-stage One-shot learner"></a>Two-stage One-shot learner</h2><p>孪生网络框架只能在初始帧这个stage通过one-shot learning学到目标信息，因此无法在线更新。那自然会想能不能让这个learner在不同的stage（跟踪阶段）去学习目标信息呢？因此作者提出了 two-stage one-shot learner (TOL)，可以结合具有不同属性的样本来预测分类器的参数$W$。目标函数定义如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/38-siamtol/20210510160306.png" alt=""></p>
<p>相比公式3，就是增加了一个来自后续帧中的样本$u_i$来学习$\omega$ 。</p>
<h2 id="Updatable-Siamese-Network"><a href="#Updatable-Siamese-Network" class="headerlink" title="Updatable Siamese Network"></a>Updatable Siamese Network</h2><p>基于上述的 two-stage one-shot learner，本文提出一个可更新的孪生网络，如下图</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/38-siamtol/20210510162506.png" alt=""></p>
<p>相比原始的孪生框架，增加了一个update分支，然后把两个分支的目标特征进行融合。其实抛开上面讲的那些one-shot learning也不妨碍理解这个结构，无非就是把初始帧和跟踪过程中的历史帧通过网络融合生成一个更好的模板以适应跟踪中的变化。</p>
<p>融合模板的过程（或者说元学习函数）定义如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/38-siamtol/20210510163640.png" alt=""></p>
<p>其中$zf$和$uf$分别表示模板和更新样本的特征，M表示特征融合网络（三层卷积）。</p>
<p><strong>Multi-aspect loss training</strong> 训练网络时在模板和搜索图像之间的间隔图像中额外扣一个更新样本，并且计算损失分别考虑了模板样本-搜索样本，更新样本-搜索样本，融合模板样本-搜索样本三方面损失，如图3所示。这样做是因为网络包括一个基本的孪生跟踪器和一个在线调整的更新器两部分，如果直接用一个整体损失训练，网络可能难以平衡这两部分。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/38-siamtol/20210510164536.png" alt=""></p>
<p><strong>Online update</strong> 在线跟踪过程中，更新样本每N=10帧更新一次，且满足置信度大于阈值0.9。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Comparison-with-the-state-of-the-arts"><a href="#Comparison-with-the-state-of-the-arts" class="headerlink" title="Comparison with the state-of-the-arts"></a>Comparison with the state-of-the-arts</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/38-siamtol/20210510165043.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/38-siamtol/20210510165257.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/38-siamtol/20210510165536.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/38-siamtol/20210510165551.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/38-siamtol/20210510165627.png" alt=""></p>
<h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/38-siamtol/20210510165730.png" alt=""></p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>FSL</tag>
        <tag>arxiv</tag>
        <tag>模型更新</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepMix: Online Auto Data Augmentation for Robust Visual Object Tracking</title>
    <url>/tracking/39-deepmix/</url>
    <content><![CDATA[<p><img src="https://github.com/kongbia/picgo/raw/master/blog/39-deepmixl/20210510171656.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2104.11585">论文</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><ul>
<li>通过历史帧样本在线更新目标模型对跟踪具有重要意义。最近的研究主要集中在构建有效的更新方法，而忽略了用于学习判别模型的训练样本；</li>
<li>本文提出DeepMix，对历史帧样本的特征进行在线增广，从而强化模型的在线更新能力。具体包括通过object-aware filtering在线增强历史样本，以及通过离线训练的MixNet混合多个样本进行数据增强；</li>
<li>最后通过三个典型的跟踪器DiMP, DSiam和SiamRPN++验证提出的方法。</li>
</ul>
<span id="more"></span>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>首先点明数据增广对于提升模型判别能力非常重要，因此可以利用数据增广给现有的模型更新方法增加有效的训练样本。用公式表达如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/39-deepmixl/20210510203101.png" alt=""></p>
<p>其中$X_t$表示原始训练样本，$T$表示各种增广方法，$\hat {X_t}$表示增强后的样本。而现有的不管是传统还是深度学习的增广方法都不能直接适应跟踪的在线数据增强，主要包括两个原因：</p>
<ol>
<li>现有方法都是sample-level的，即会生成新的样本，对这些新样本需要额外花费大量时间提取它们的深度特征；</li>
<li>现有方法都是基于退化因素(例如，噪声、模糊、雾、雨等)，这些退化因素会破坏原始样本，降低模型判别力。</li>
</ol>
<p>为了解决上述问题，作者提出要对训练样本的<strong>特征</strong>(embedding)进行增强。具体来说，首先提取样本$X_t$的特征embedding得到$\{\varphi(I_i) \in \mathbb{R}^{C \times W \times H} | I_i \in X_t\}$，然后将所有embedding拼接起来得到特征张量 $X_t \in \mathbb{R}^{N \times C \times W \times H} $，N是训练样本数量。之后把$X_t$通过数据增强映射到一个新的特征$\hat{X_t} \in \mathbb{R}^{K \times C \times W \times H} $ 送入更新模块以生成目标模型。</p>
<p>在特征上进行增强比直接在样本上增强更加高效，此外，作者在混合样本特征时加入了对应跟踪结果的指导。从直观上看，在视频采集过程中，感兴趣的物体可能位于场景中的任何位置，将物体放置在可能的背景区域中来增加训练样本是合理的。因此，作者根据跟踪框将样本分为目标区域和背景区域，并把它们混合以生成新样本。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/39-deepmixl/20210510211559.png" alt=""></p>
<p>其中 $M_o \in \mathbb{R}^{K \times C \times W \times H} $（此处作者笔误，第一个维度为K而不是N，已向作者求证）是二值掩膜，1表示目标区域，0表示背景区域。$W_{ \{o \ or \ b\} } \in \mathbb{R}^{K \times N \times 3 \times 3} $表示卷积，K是输出样本的个数。这个卷积的目的就是混合N个原始样本生成K个新样本，其中$W_o$和$W_b$分别负责混合目标和背景区域。下一步就需要考虑如何计算$W_o$和$W_b$，作者设计了MixNet，如图2所示。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/39-deepmixl/20210510212546.png" alt=""></p>
<p>输入$X_t$，通过两个结构一致参数不同的卷积分支生成kernel $W_o$和$W_b$。这个网络可以预先离线训练好，在线跟踪时只需一次前向传播就可以高效地获得增强样本。</p>
<p><strong>Implementation for SOTA Trackers</strong></p>
<p>为了验证DeepMix的效果，作者在SiamRPN++, DSiam和DiMP三个跟踪器上进行实验。将原始样本$X_t$和增强样本$\hat {X_t}$相加后 ($\alpha_1 \hat {X_t} + \alpha_2 X_t$) 送入更新模块，$\alpha_1=0.05, \alpha_2=0.8$。<strong>注意</strong> 不是直接用生成的新样本替换原始样本，而是将两者进行混合。</p>
<p>对于SiamRPN++和DSiam，将历史N=15帧样本混合成K=1个新样本，注意SiamRPN++虽然并没有更新模块，但是MixNet也可以混合历史样本输出一个更干净的搜索特征从而促进跟踪。而在训练时，将搜索图像随机增广成15个样本送入MixNet生成1个新样本与原始图像混合后输入到之后的模块。<strong>论文这里作者笔误写成了对模板进行增广，已向作者求证。</strong></p>
<p>对于DiMP，则将历史N=50帧样本混合成K=N个新样本。同样将MixNet嵌入DiMP的训练中，为了与测试保持一致，将训练过程中的3个样本改成了50个。</p>
<p>这里额外提一下关于生成样本的mask $M_o \in \mathbb{R}^{K \times C \times W \times H} $如何设置。对于SiamRPN++，K=1，mask根据上一帧的跟踪结果设置；对于DiMP，K=N，mask就直接根据这N帧跟踪结果设置。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="State-of-the-art-Comparison"><a href="#State-of-the-art-Comparison" class="headerlink" title="State-of-the-art Comparison"></a>State-of-the-art Comparison</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/39-deepmixl/20210510214422.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/39-deepmixl/20210510214430.png" alt=""></p>
<p>尽管baseline比原始论文得到的要低一些，但是加入了DeepMix性能依然明显提升。</p>
<h2 id="Ablation-study"><a href="#Ablation-study" class="headerlink" title="Ablation study"></a>Ablation study</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/39-deepmixl/20210510215021.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/39-deepmixl/20210510215028.png" alt=""></p>
<p>表3中DeepMix在ResNet18的提升比ResNet50大，可能是因为更强大的网络对于增广的依赖更小。</p>
<p>表4比较了DeepMix几种不同的实现方法，-Opt表示通过梯度下降在线优化目标函数来取代所提出的MixNet，-single表示不区分目标和模板区域只用一个分支生成新样本。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>这篇工作的切入点挺新奇的，以往的模型更新都在考虑如何设计更新方法，而本文则关注样本本身，从在线数据增广的角度切入，将历史样本混合生成新的样本用于模型更新。本文灵感应该还是来自于Mix类的增广方法，那是不是意味着我们可以把其他数据增广的方法也运用进来，刚好我最近也在思考类似的问题，本文可以给到一定的启发。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>模型更新</tag>
        <tag>Data augmentation</tag>
        <tag>ICME2021</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Meta Learning for Real-Time Target-Aware Visual Tracking</title>
    <url>/tracking/4-mlt/</url>
    <content><![CDATA[<p><img src="https://github.com/kongbia/picgo/raw/master/blog/4-mlt/20210810211247.png" alt=""></p>
<p><a href="https://arxiv.org/abs/1712.09153">论文</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/4-mlt/20210810214920.png" style="zoom:80%;" /></p>
<p>当跟踪目标外观发生变化时，需要对模型进行更新。使用传统的SGD、岭回归这些更新方法耗时且容易过拟合，因此本文提出用元学习(meta-learning)方式对模型进行更新。本文方法的motivation如图1所示，分为匹配网络和元学习网络。前者是标准的孪生网络，元学习网络接收来自匹配网络的元信息，并为匹配网络提供自适应的target-specific特征空间。而元学习网络仅需一次前向传播就可以得到target-specific特征空间的参数，实时性较好，达到了48fps。</p>
<span id="more"></span>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/4-mlt/20210810223903.png" alt=""></p>
<p>图2是算法整体框架，可以看到，图1中描述的匹配网络输入到元学习网络的元信息指的是训练样本的loss梯度，而元学习网络输出的target-specific信息指额外的卷积核和通道注意力。</p>
<p><strong>Components</strong></p>
<p>匹配网络是标准的孪生网络</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/4-mlt/20210811091726.png" style="zoom:80%;" /></p>
<p>其中，$x$是模板，$z$是搜索区域，$w = \{ w_1, w_2, …, w_N \}$​表示每一层卷积层的权重参数。</p>
<p>元学习网络根据历史得到的M个context patch $z = \{z_1, z_2, …, z_M\}$以及target patch $x$，来计算能够适应目标变化的新增参数。首先计算损失函数对匹配网络最后一层卷积层的平均负梯度 $\delta$：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/4-mlt/20210811092640.png" style="zoom:80%;" /> </p>
<p>其中 $\widetilde{y}_i$ 表示假设目标落在正确位置时生成的<strong>二值</strong>响应图。meta-learner设计的依据是目标发生变化时 $\delta$ 也发生变化，所以将 $\delta$​​ 作为元学习网络的输入，可以生成target-specific的权重。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/4-mlt/20210811094325.png" style="zoom:80%;" /></p>
<p>将生成的$\omega^{target}$与原始的权重拼接起来得到新的自适应权重$w^{adapt} = \{ w_1, w_2, …, [w_N, \omega^{target}] \}$​，将其用于目标定位</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/4-mlt/20210811094727.png" style="zoom:80%;" /></p>
<p>此外，元学习网络还生成了通道注意力权重进一步调整特征表示空间。</p>
<p><strong>Tracking algorithm</strong></p>
<p>在跟踪过程中，会保存一个context image的集合 $z_{mem} = \{ z_1, z_2, …, z_K \}$​​​​​​ 以及对应生成的响应图$\hat{y}_{mem} = \{ \hat{y}_1, \hat{y}_2, …, \hat{y}_K \}$​​​​​​。只有当 $\hat{y}$​​​​​​ 的最大响应值大于阈值时，$z$​​​​​​才会被加入memory集合中。在更新权重时，从K个context image中选择M个来进行计算，筛选标准是$\hat{y}_{mem}$​​​​​​​的最小熵，熵越大响应图分布越杂乱，这样可以避免模糊的响应图引入false positive。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/4-mlt/20210811101915.png" style="zoom:80%;" /></p>
<p>整体跟踪流程如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/4-mlt/20210811094851.png" style="zoom:80%;" /></p>
<p><strong>Training</strong></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/4-mlt/20210811102117.png" style="zoom: 67%;" /></p>
<p>首先单独训练匹配网络，然后在此基础上按照图3的方式训练元学习网络。类似在线跟踪过程，从同一个视频序列中随机采样 M’ 个context patch，然后从中选取M个送入元学习网络训练（M’ ＞M），这样是为了避免过拟合。优化损失为：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/4-mlt/20210811102811.png" style="zoom:80%;" /></p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/4-mlt/20210811103012.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/4-mlt/20210811103215.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/4-mlt/20210811103248.png" alt=""></p>
<p>消融实验，MLT-mt表示只有匹配网络，MLT-mt+ft表示对匹配网络最后一层进行在线微调更新。可以发现在线微调甚至比不进行更新效果更差，因为过拟合。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/4-mlt/20210811103028.png" style="zoom: 67%;" /></p>
<p>最后可视化了target-specific特征空间的效果，可以更聚焦目标并且抑制周围的干扰。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/4-mlt/20210811103525.png" style="zoom:80%;" /></p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>这篇文章发表在ICCV2019，整体实验结果并没有刷得太高，但是meta learning运用在siamese跟踪中的思想无疑是优秀的。目标跟踪同样可以看做是一个few-shot learning的任务，如何运用历史跟踪的少量结果来微调模型参数，meta learning是一个不错的方案。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>meta learning</tag>
        <tag>ICCV2019</tag>
      </tags>
  </entry>
  <entry>
    <title>Siamese Natural Language Tracker: Tracking by Natural Language Descriptions with Siamese Trackers</title>
    <url>/tracking/40-snlt/</url>
    <content><![CDATA[<p><img src="https://github.com/kongbia/picgo/raw/master/blog/40-snlp/20210519110756.png" alt=""></p>
<p><a href="https://arxiv.org/abs/1912.02048">论文</a></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文研究的课题为tracking by natural language(NL) 。人类的学习过程是视觉和语言共同作用的，而在基于外观的跟踪过程中引入语言描述同样可以使得跟踪器更加精确、灵活和鲁棒（如图1的例子）。因此，本文将孪生跟踪器与语言描述结合，将语言描述编码成一个卷积核嵌入到孪生框架中（SNL-RPN），并将视觉和语言的预测进行动态聚合（Dynamic Aggregation），为tracking by NL任务提供了一个新的baseline。具体贡献总结如下：</p>
<ol>
<li>提出一种新的tracking by NL的baseline，Siamese Natural Language Region Proposal Network (SNL-RPN)；</li>
<li>提出了一种基于视觉和语言预测的动态聚合（Dynamic Aggregation），将SNL-RPN转换为Siamese Natural Language Tracker (SNLT)；</li>
<li>在NL标注的数据集上将孪生跟踪器的性能提升了3-7个百分点，并且性能超过其他NL tracker，速度为50FPS。</li>
</ol>
<span id="more"></span>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/40-snlp/20210519113920.png" alt=""></p>
<center>图1 （摘自CVPR2021 TNL2K） (a) 只利用box时跟踪器可能会混淆跟踪目标是自行车还是人的腿，加入语言描述使得跟踪对象更加清晰；(b) 当目标发生剧烈形变时加入语言描述可以减少漂移；(c) 语言描述可以更灵活地指定目标，如这里需要跟踪持球的运动员，使用传统跟踪器达到这一效果需要反复重新初始化跟踪器。  </center>

<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/40-snlp/20210519115620.png" alt=""></p>
<p>上图展示了SNLT整体框架，将语言描述嵌入到SiamRPN++中。输入包含三部分：模板、搜索区域和查询语言Q。模板和搜索区域经过卷积提取特征$Z$和$X$，查询语言经过语言模型（BERT, GloVe, HGLMM）提取句子嵌入编码$Z_Q$。三元组$(Z,Z_Q,X)$送入SNL-RPN，分别预测视觉和语言的分类和回归响应。最后二者通过Dynamic Aggregation Module进行融合。</p>
<h2 id="Architecture-of-the-SNL-RPN"><a href="#Architecture-of-the-SNL-RPN" class="headerlink" title="Architecture of the SNL-RPN"></a>Architecture of the SNL-RPN</h2><p>SNL-RPN如上图b所示，包括蓝色部分的visual head和红色部分的NL head，都是通过DW-XCorr生成相应的的分类和回归响应。类似SiamRPN++，同样在ResNet的三个stage上进行预测，并将结果融合，融合权重通过离线训练得到。visual head和NL head各包含两个分支，总共有四组融合参数：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/40-snlp/20210519154224.png" alt=""></p>
<p>$S_{VIS}, S_{NL}$表示分类分支的视觉和语言预测，$B_{VIS},B_{NL}$表示回归分支的两种预测。</p>
<h2 id="Aggregation-of-the-SNL-RPN-Predictions"><a href="#Aggregation-of-the-SNL-RPN-Predictions" class="headerlink" title="Aggregation of the SNL-RPN Predictions"></a>Aggregation of the SNL-RPN Predictions</h2><p>上一节对不同stage的预测融合，本节作者对视觉和语言的结果再次进行融合：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/40-snlp/20210519160117.png" alt=""></p>
<p>$\omega_{VIS},\omega_{NL}$ 可以简单像上面一样离线训练得到，但是融合对象来自两个不同的输入，用固定的权重融合并不是最优的。因此，作者提出一个基于预测响应图熵的动态融合方式：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/40-snlp/20210519160727.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/40-snlp/20210519160737.png" alt=""></p>
<p>其中$\sigma$表示softmax函数，$\alpha$表示缩放因子。公式4的含义是<strong>熵越大的项赋予更小的权重</strong>。熵越大表示预测图分布越混乱，越不准确，所以权重更小。</p>
<h2 id="Training-the-SNL-RPN-and-Loss-Functions"><a href="#Training-the-SNL-RPN-and-Loss-Functions" class="headerlink" title="Training the SNL-RPN and Loss Functions"></a>Training the SNL-RPN and Loss Functions</h2><p>训练设置和siamese框架一样，额外增加了一个语言模型。损失函数分别计算视觉和语言对应的分类回归损失。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/40-snlp/20210519161412.png" alt=""></p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>训练数据集使用的是 MSCOCO, YouTube-BB, VisualGenome, LaSOT, OTB-99-LANG，其中后三个都是有语言标注的，前两个只有类别标注。</p>
<p>测试数据集使用有语言标注的OTB-99-LANG和LaSOT。</p>
<h2 id="Comparison-with-Visual-and-NL-Trackers"><a href="#Comparison-with-Visual-and-NL-Trackers" class="headerlink" title="Comparison with Visual and NL Trackers"></a>Comparison with Visual and NL Trackers</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/40-snlp/20210519161835.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/40-snlp/20210519161846.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/40-snlp/20210519161952.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/40-snlp/20210519162121.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/40-snlp/20210519162142.png" style="zoom:80%;" /></p>
<p>表2最后一行表示只使用语义类别作为语言输入测试LaSOT（原始的LaSOT里语言描述是一句话），这样效果会下降，证明SNLT可以学到比语义类别更多的东西。</p>
<h2 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/40-snlp/20210519162612.png" style="zoom:80%;" /></p>
<p>表1对比了不同语言模型的效果</p>
<p><img src="C:/Users/zjp/AppData/Roaming/Typora/typora-user-images/image-20210519162738919.png" alt="image-20210519162738919"></p>
<p>图6的两个例子，第一个展示了语言描述可以辅助跟踪器避免模型漂移，而第二个例子语言描述不能唯一地描述目标，导致跟踪器漂移。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/40-snlp/20210519163102.png" style="zoom:80%;" /></p>
<p>图8展示了视觉和语言的分类响应图，NL head的响应更加准确，可能是由于遮挡阻碍了视觉模型。通过计算响应图的熵，赋予熵更小的NL响应图更大的权重。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>CVPR2021</tag>
        <tag>Natural Language</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习在目标跟踪中的应用</title>
    <url>/tracking/42-rl-tracking/</url>
    <content><![CDATA[<p>强化学习讨论的问题是智能体(agent) 如何在一个复杂不确定的环境(environment) 里去最大化它能获得的奖励。 今天介绍三篇关于强化学习在目标跟踪中的工作，分别利用强化学习来决策使用的特征，多个跟踪器的切换以及是否更新模板。</p>
<p>论文列表：</p>
<ul>
<li><a href="https://arxiv.org/abs/1708.02973">Learning Policies for Adaptive Tracking with Deep Feature Cascades</a></li>
<li><a href="https://proceedings.neurips.cc//paper/2020/file/885b2c7a6deb4fea10f319c4ce993e02-Paper.pdf">Online Decision Based Visual Tracking via Reinforcement Learning</a></li>
<li><a href="https://arxiv.org/abs/2004.07538">Fast Template Matching and Update for Video Object Tracking and Segmentation</a></li>
</ul>
<span id="more"></span>
<h1 id="强化学习方法"><a href="#强化学习方法" class="headerlink" title="强化学习方法"></a>强化学习方法</h1><p>强化学习包含环境, 动作和奖励三部分, 其本质是agent 通过与环境的交互, 使得其作出的action所得到的总奖励达到最大, 或者说是期望最大。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210608202931.png" style="zoom: 50%;" /></p>
<p>强化学习方法主要可以分为Value-Based，Policy-Based以及二者结合的Actor-Critic方法。Value-Based方法通过Temporal Difference (TD) Learning学习动作价值函数；Policy-Based方法通过Policy Gradient学习策略函数；而Actor-Critic方法将二者结合，actor学习一个策略来得到尽量高的回报，critic对当前策略的值函数进行估计，即评估actor的好坏。</p>
<p>关于强化学习的更多细节可以参考王树森老师的视频课程<a href="https://www.bilibili.com/video/BV12o4y197US">【王树森】深度强化学习(DRL)</a></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210608203415.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210608204616.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210608204711.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210608204741.png" alt=""></p>
<hr>
<h1 id="Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades"><a href="#Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades" class="headerlink" title="Learning Policies for Adaptive Tracking with Deep Feature Cascades"></a>Learning Policies for Adaptive Tracking with Deep Feature Cascades</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>第一篇来自ICCV2017，出发点是不同复杂程度的跟踪场景对特征的需求是不同的，对于简单场景使用浅层特征（甚至像素特征）就能处理，而对于一些复杂场景才需要具有更强语义信息的深度特征。这个<strong>自适应决策</strong>的问题可以通过基于Q-learning的强化学习完成，如图1所示，学习一个agent来判断当前特征是否已经可以以较高的置信度定位目标，还是需要继续计算更深层的特征来寻找目标。</p>
<p>这样对简单目标提前终止的策略可以大幅提升推理速度，相比baseline平均速度提升了大约10倍，GPU速度158.9FPS，并且在cpu上也能以23.2FPS的速度接近实时运行。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210629104958.png" alt=""></p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>首先定义一些公式符号，孪生网络每一层的互相关层定义：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210629121458.png" alt=""></p>
<p>其中$\varphi_l$ 表示第 $l$ 层的特征，$F_l$ 表示第 $l$ 层的互相关结果。</p>
<p>整体框架如图2所示，在每一层互相关结果$F_l$后面接一个Q-Net，用于判断是否在该层停止，或者调整预测框的形状并继续使用下一层特征。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210629121328.png" alt=""></p>
<p>agent采用强化学习的方式训练，基本元素包括状态(state S) ，动作(action A)和奖励(reward R)。在每一个step即第l层中，agent根据当前状态$S_l$ 采取动作$A_l$来决定是否调整预测框或者在该层停止并输出结果，动作$A_l$的目的是减少预测的框的不确定性。训练时根据预测框与GT的IOU给出相应的奖励$R_l$（有正有负），通过最大化期望奖励，agent能学到最好的决策来采取行动，在精度和效率上取得平衡。</p>
<p><strong>Actions:</strong> 包括7个各向异性的的尺度变换和一个stop动作。7个尺度变换里包括2个全局的缩放和4个局部缩放，缩放比例为0.2。还有一个不缩放(no scaling)的动作，这一操作用于在当前响应图不明确或无法做出决策时推迟决策。</p>
<p><strong>States:</strong> 状态是一个包含响应图$F’_l$和历史动作$h_l$的二元组 $(F’_l,h_l)$。$F’_l$使用的是当前层和之前所有层响应图的平均，相当于结合了浅层的细节和深层的语义。$h_l$包含历史4个动作的向量，每个动作是8维的one-hot的向量，所以$h_l$总共是32维。</p>
<p><strong>Rewards:</strong> 奖励函数$R(S_{l-1},S_l)$反应了采取动作$A_l$后，从状态$S_{l-1}$到状态$S_l$的定位精度提升（或下降），精度采用IOU衡量，奖励函数计算如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210629174619.png" style="zoom:80%;" /></p>
<p>当动作不为stop时，若该动作能使IOU增大，则奖励+1，否则惩罚-1。若采取任何尺度变换都不能进一步提升IOU或者已经到达最后一层了，则采取stop动作，此时以IOU阈值0.6来决定奖惩。</p>
<p><strong>Deep Q-learning</strong>：本文使用value-based的DQN来选择动作，该方法需要学习一个动作-价值函数$Q(S_l,A_l)$, 选择能够使得Q最大的动作A。Q函数用网络模拟，如图2虚线框所示，包含两个128维的FC层，输出对应8维动作的回报。训练时采用TD learning进行迭代。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210629181015.png" style="zoom:80%;" /></p>
<p>其中R表示当前奖励，$Q(S‘,A’)$表示未来总的回报，$\gamma$是折扣因子。</p>
<p>测试阶段无需奖励，只根据Q函数调整预测框直到输出stop动作。作者在OTB50上验证平均只需要2.1步输出结果，即只需要两层网络，因此可以大幅提速。</p>
<p>此外，这套策略还可以集成一些简单的特征，比如像素特征和hog特征，计算更快。</p>
<p>图3展示了一些early stop的例子，如跟踪清晰的人脸时只需C1-C2的特征，但跟踪一个模糊的人脸则需要更深层的C5特征。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210629194612.png" alt=""></p>
<hr>
<h1 id="Online-Decision-Based-Visual-Tracking-via-Reinforcement-Learning"><a href="#Online-Decision-Based-Visual-Tracking-via-Reinforcement-Learning" class="headerlink" title="Online Decision Based Visual Tracking via Reinforcement Learning"></a>Online Decision Based Visual Tracking via Reinforcement Learning</h1><h2 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h2><p>第二篇来自NIPS2020。目前主流的跟踪方法有基于检测的和基于模板匹配的，二者各有优劣。基于检测的方法容易受遮挡等影响错误更新网络，但是能适应形变；而基于模板匹配的方法只利用第一帧模板，与上述情况刚好相反。很自然会想到将二者结合，但这是两套完全不同的跟踪原理，直接融合并不能同时收敛到各自的最优解。因此本文提出了一个基于分层强化学习(HRL)的在线决策机制。决策机制实现了一种智能切换策略，其中检测器和模板跟踪器必须相互竞争，以便在它们擅长的不同场景中进行跟踪。</p>
<h2 id="Method-1"><a href="#Method-1" class="headerlink" title="Method"></a>Method</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210629205511.png" alt=""></p>
<p>整体框架如图2所示，包括决策模块和跟踪模块。决策模块是一个Actor-Critic（or Option-Critic？）结构，包括switch network和termination network。首先将初始帧模板和上一帧跟踪结果送入switch network，输出一个二元信号选择跟踪器。跟踪器结果送入termination network，输出终止当前跟踪器的概率。注意这里终止之后并不一定切换到另一个跟踪器，因为并不能保证另一个就更好，而是要经过switch network重新选择。</p>
<p><strong>Decision Module</strong></p>
<p>给定一组状态$S$和动作$A$，马尔可夫选项$\omega \in \Omega$ 包括三部分：intra-option policy $\pi: S \times A \rightarrow [0,1]$, termination condition $\beta: S^{+}  \rightarrow [0,1]$, initiation set $I \subseteq  S$。当option $\omega$选定后，根据$\pi_{\omega}$选择相应的动作，直到终止函数 $\beta_{\omega}$ 判断终止。</p>
<p>这是一个标准的Option-Critic结构，一大堆公式就省略了。但是最后作者却用Actor-Critic去解释图2，即switch network是option-value函数，作为Critic来评价option，并且为termination network提供更新梯度（参考上面Actor-Critic的ppt）。termination network作为Actor评估正在使用的跟踪器性能，以决定它是否应该在当前帧终止。</p>
<p>switch network的奖励函数定义如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210629214943.png" style="zoom:80%;" /></p>
<p>其中$P$表示被选中的跟踪器的跟踪框与GT的IOU，$P^*$表示未被选中的跟踪器的跟踪框与GT的IOU，$D_{IoU}$表示两者的差。按照公式7总共有3种情况：一个成功一个失败，两个均成功，两个均失败。</p>
<p>训练按照Actor-Critic训练，Critic使用贝尔曼方程（TD learning）更新，Actor使用策略梯度更新。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210629215908.png" alt=""></p>
<p>图5是一个可视化结果，其中终止概率1表示终止，0表示保持不变。可以看到初始是SiamFC（黄框）表现较好；当发生形变后，FCT的价值函数更大，终止概率趋近1，跟踪器切换；之后一直都是FCT表现更好，因此终止概率始终在0附近。</p>
<hr>
<h1 id="Fast-Template-Matching-and-Update-for-Video-Object-Tracking-and-Segmentation"><a href="#Fast-Template-Matching-and-Update-for-Video-Object-Tracking-and-Segmentation" class="headerlink" title="Fast Template Matching and Update for Video Object Tracking and Segmentation"></a>Fast Template Matching and Update for Video Object Tracking and Segmentation</h1><h2 id="Motivation-2"><a href="#Motivation-2" class="headerlink" title="Motivation"></a>Motivation</h2><p>第三篇来自CVPR2020。本文针对的任务是<strong>多实例半监督视频目标分割</strong>(VOS)。基于检测的算法被广泛应用于这一任务，难点在于选择匹配方法来预测结果，以及是否更新目标模板。 本文利用强化学习来同时做出这两个决策。具体来说，<strong>agent根据预测结果的质量来决定是否更新目标模板</strong>。 匹配方法的选择则基于agent的动作历史来确定。</p>
<p>目前大部分VOT或VOS方法主要分为三步：</p>
<ol>
<li>对当前帧进行实例分割，生成一系列候选proposal；</li>
<li>将目标模板和所有proposal进行匹配，找到正确的proposal作为最终结果；</li>
<li>使用当前帧的预测结果替换目标模板。</li>
</ol>
<p>针对步骤2，基于外观的匹配方法（siamese）准确但非常耗时，而直接利用候选框与前一帧预测框的IOU进行快速匹配只适用于目标缓慢移动或变化。针对步骤3，现有方法简单地直接用当前结果替换模板，不考虑结果的正确性，会导致误差逐渐累积。因此需要利用强化学习智能切换。</p>
<h2 id="Method-2"><a href="#Method-2" class="headerlink" title="Method"></a>Method</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210630113658.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210630171122.png" alt=""></p>
<center>将上一帧中所有目标框的最小外接矩形bm扩大一定倍率得到搜素区域bs。</center>

<p>整体方法如图2所示，分为三个步骤：</p>
<ol>
<li>按照图3的方式确定目标搜索区域，采用实例分割网络（如YOLACT, Mask-RCNN）生成候选预测，然后利用基于IOU的匹配方法得到初步匹配结果；</li>
<li>通过agent判断初步结果的正确性和质量，决定是否更新模板；</li>
<li>确定是否需要切换到基于外观的匹配的方法。若连续N帧初步结果都不好（即第二步预测不更新模板），则切换到基于外观的匹配。此时会将整个图像送入网络。</li>
</ol>
<p>下面介绍将Actor-Critic的框架嵌入上述模型</p>
<p><strong>Action</strong></p>
<p>首先定义相关的符号，如图4所示，目标模板包括边界框 $T_{box}$，mask $T_{mask}$，$T_{box}$中的图像内容$T’_{box}$，$T_{mask}$中的图像内容$T’_{mask}$。而预测结果则是类似的$P_{box}, P_{mask}, P’_{box}, P’_{mask}$。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210630173712.png" style="zoom:80%;" /></p>
<p>第一个决策是是否更新模板，agent的动作 $a_i \in A$ 有两种情况，$a_0$表示用当前结果更新模板，$a_1$表示不更新。</p>
<p>第二个决策是匹配方法选择快速的基于IOU匹配还是精确的基于外观匹配，在精度和速度间取得平衡。前者分别计算模板和候选预测的box IOU和mask IOU，选择IOU最大的作为匹配结果；后者则是计算模板图像块$T’_{box}$和候选预测图像块$P’_{box}$的相似性，选择最像的作为匹配结果。<strong>注意这里没有另外增加一个agent</strong>，而是根据第一个agent的历史决策来决定。若agent连续N帧预测$a_1$，表示目标很可能丢失，此时需要切换到基于外观的匹配方法。</p>
<p><strong>State</strong> </p>
<p>输入agent的状态$s_t$包括两部分，如下式。第一部分$S_T$是模板图像，其中边框$T’_{box}$之内的内容保持不变，之外的内容填充黑色；第二部分$S_P$是搜索图像，同样将mask之内的内容保持不变，之外的内容填充黑色。提取这两种图像的特征并相加得到输入状态。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210630175114.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210630175123.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210630175138.png" alt=""></p>
<p><strong>Reward</strong></p>
<p>奖励函数定义：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210630175755.png" alt=""></p>
<p>$J_t$表示$P_{mask}$和GT mask之间的IOU。</p>
<p><strong>Actor-Critic Training</strong></p>
<p>基本元素确定后，按照Actor-Critic框架训练。</p>
<p>critic网络用value-based的方式训练：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210630180416.png" alt=""></p>
<p>$\delta_t$ 表示TD error，公式8中梯度下降用加号是因为公式9减法顺序和常规的是反过来的。</p>
<p>actor用policy-based方法训练：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210630192837.png" alt=""></p>
<p>公式11减$V(s_t)$表示是带baseline的策略梯度。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210630193309.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/42-rl-tracking/20210630193324.png" alt=""></p>
<p>图1和图6表面本文的方法在VOT和VOS任务上均能在速度和精度上取得一个较好的平衡。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上三种方法分别介绍了利用强化学习来决策使用的跟踪特征，多个跟踪器的切换以及是否更新模板。可以发现，应用的方向基本都是把跟踪方法中某些需要启发式设计的模块换成了强化学习进行智能决策。此外，第一篇和第三篇均提到了引入强化学习可以在一定程度上提速，对于某些简单的情况，agent可以决策使用简单的方法进行跟踪。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>综述</tag>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking</title>
    <url>/tracking/41-siamrcr/</url>
    <content><![CDATA[<p><img src="https://github.com/kongbia/picgo/raw/master/blog/41-siamrcr/20210526210529.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2105.11237">论文</a></p>
<p>本文解决的是老生常谈的分类和回归不匹配的问题。作者提出在分类和回归之间建立<strong>双向</strong>的连接，可以动态地重新加权每个正样本的损失。此外，增加了一个定位分支用于预测定位精度，可以在推理过程中替代回归辅助连接(regression assistance link)，使得训练和测试更加一致。最终运行速度为65FPS。</p>
<span id="more"></span>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/41-siamrcr/20210526212918.png" alt=""></p>
<p>首先点出问题，即孪生跟踪架构中分类和回归是分开独立优化的，导致二者不匹配。如图1所示，分类得分最高的位置生成的预测框不一定是最好的，或者预测比较好的框分类得分很低。这个图和IOUNet中的图很相似，事实上，这个问题在检测任务中已经被很多学者研究，并且有些成果也被应用到跟踪中。比如SiamFC++借鉴FCOS架构增加了一个衡量定位精度的分支，ATOM/DiMP系列使用IOUNet进行回归。作者指出这些方法仍然存在不匹配，因为并没有解决分类和回归独立优化的问题。</p>
<p>因此，本文提出在分类和回归之间建立一个互惠关系 (reciprocal relationship)，使它们同步优化，以生成精度一致的输出。整体框架如图2所示，在分类和回归分支之间增加了两个连接 classification / regression assistance link。classification assistance 用分类置信度给回归损失加权，使得回归可以更关注高置信度的位置；regression assistance 用定位精度（预测框和gt的IOU）给分类损失加权，迫使分类分数与回归精度更加一致。</p>
<p>而在推理阶段，gt是未知的，无法通过计算IOU得到定位精度，因此额外增加一个定位分支专门用于预测定位精度。将分类置信度与定位预测置信度相乘，在推理阶段生成新的跟踪评分/置信度图，保证了与训练过程的一致性（类似FCOS / SiamFC++）。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/41-siamrcr/20210526221151.png" alt=""></p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>图2整体是一个anchor-free的孪生框架，预测头部分采用的是CenterNet的中心、中心偏移量、宽高的形式。这里将中心偏移量和宽高放在同一个分支输出，所以回归分支输出维度是4  $t^<em>_{x,y} = (w^</em>, h^<em>, \Delta x^</em>, \Delta y^*)$ 。</p>
<h2 id="Reciprocal-Classification-and-Regression"><a href="#Reciprocal-Classification-and-Regression" class="headerlink" title="Reciprocal Classification and Regression"></a>Reciprocal Classification and Regression</h2><p>下面开始介绍两种辅助连接，设计原则就是：</p>
<ol>
<li>当回归框的定位精度较低时，相应的分类得分不应该很高，因为如果该位置成为分类置信度的赢家，差的回归结果将导致跟踪性能较差；</li>
<li>当回归框的分类分数较低时，提高其定位精度是没有意义的，因为这个框一定不会是最后的输出。</li>
</ol>
<p><strong>Regression Assistance Link</strong> 针对准则1，将回归分支生成的框与gt计算IOU，看成一种动态的样本重加权作用于分类损失：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/41-siamrcr/20210526223907.png" alt=""></p>
<p>$N_{pos}$表示正样本个数，$B,B^*$分别表示预测框和gt。</p>
<p><strong>Classification Assistance Link</strong> 针对准则2，将分类置信度 $p^{cls}_{x,y}$ 动态地重新加权回归损失：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/41-siamrcr/20210526225051.png" alt=""></p>
<p><strong>Localization Score Branch</strong> 在训练时，regression assistance link使得分类分支考虑了回归精度，但这需要借助gt。而在推理阶段没有gt，为了保证训练和推理的一致性，使得推理阶段的分类分支也能考虑定位精度，作者额外增加了一个定位分支专门用于预测定位精度，作用类似FCOS / SiamFC++的centerness。其训练损失就是计算定位分支输出与IOU之间的交叉熵损失。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/41-siamrcr/20210526225941.png" alt=""></p>
<p>推理阶段示意图如图3，将分类分支和定位分支的得分相乘生成最后的跟踪分数图。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/41-siamrcr/20210527094602.png" alt=""></p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/41-siamrcr/20210526230610.png" alt=""></p>
<p>表1中这两个组件的性能提升几乎是正交的，联合使用的提升(5.05%)几乎等于单独使用时各自性能提升的和(3.54%+2.86%)。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/41-siamrcr/20210526234418.png" alt=""></p>
<p>图4展示了不同方法的跟踪分数和回归框IOU之间的相关性，(a)是baseline，(b)是centerness，(c)是basline+定位分支，(d)是SiamRCR。其中(b)和(c)类似，区别是分别使用IOU和centerness来衡量定位精度。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/41-siamrcr/20210526234837.png" alt=""></p>
<p>最后展示了划分正负样本时不同半径对结果的影响。</p>
<h2 id="Comparison-with-the-State-of-the-Art"><a href="#Comparison-with-the-State-of-the-Art" class="headerlink" title="Comparison with the State-of-the-Art"></a>Comparison with the State-of-the-Art</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/41-siamrcr/20210526234927.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/41-siamrcr/20210526234937.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/41-siamrcr/20210526234948.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/41-siamrcr/20210526234956.png" alt=""></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文解决了孪生跟踪框架中分类和回归不一致的问题。印象中最先研究这个问题的是检测领域的PISA (Prime Sample Attention in Object Detection)，根据IOU对样本进行排序然后选择prime sample。之后也有许多相关文章，包括本文的Regression Assistance Link也能看到一些检测里面的影子（吐槽一下我19年投了一篇文章有个点和这个一毛一样然而到现在都还没中hhh）。但目前的研究其实基本上都是在用回归矫正分类，而本文做了一个双向的矫正，（显式地）增加了用分类矫正回归的过程（为什么说是显式，因为其实只要把分类和回归乘到一起，这个作用就是相互的，比如公式2中，我可以把focal loss当成权重，IOU当成优化变量，那就变成了分类矫正回归）。这种显式的矫正似乎监督能力更强，如果有个消融实验对比一下单独使用公式3和公式4的效果就更直观了。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>IJCAI2021</tag>
      </tags>
  </entry>
  <entry>
    <title>Crop-Transform-Paste: Self-Supervised Learning for Visual Tracking</title>
    <url>/tracking/43-Crop-Transform-Paste/</url>
    <content><![CDATA[<p><img src="https://github.com/kongbia/picgo/raw/master/blog/43-Crop-Transform-Paste/20210630203150.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2106.10900">论文</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>现有的跟踪方法依赖大量高质量标注数据集训练，但很多复杂挑战场景并未出现在训练数据中。为了避免昂贵的人工标注，本文提出一种<strong>自监督</strong>学习方式。作者设计了Crop-Transform-Paste，通过模拟跟踪过程中各种场景变化，合成足够的训练数据。在合成数据中目标状态是已知的，因此无需人工标注。</p>
<p>本文提出的自监督方法可以无缝集成到任何现有的跟踪框架中进行训练，实验证明提出的方法：1）在<strong>少样本</strong>跟踪场景中取得比监督学习更好的性能；2）能够处理目标形变、遮挡、背景干扰等各种挑战；3）可以与监督学习相结合，进一步提高性能。</p>
<span id="more"></span>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/43-Crop-Transform-Paste/20210701002056.png" alt=""></p>
<p>整体框架如图2所示，作者利用Crop-Transform-Paste来合成训练样本。给定一个只有第一帧有标注的跟踪序列，首先裁剪目标区域（crop）；然后对目标进行跟踪相关的变换，模拟跟踪过程中的目标变化（transform）；最后将变换后的目标粘贴到不同帧的不同位置上，模拟目标周围的背景(上下文)变化（paste）。</p>
<p><strong>Crop</strong></p>
<p>根据给定的bbox裁剪目标patch。其实也可裁剪任意的patch，但这样缺乏有效的语义特征和边界。</p>
<p><strong>Transform</strong></p>
<p>如图2中间部分所示，采用shear, blur, cutout, color jittering, rescale 来模拟跟踪过程中的形变、运动模糊、遮挡、光照和尺度变换。其中跟踪场景中出现的模糊通常是由相机抖动或快速运动引起的，因此采用shaking blur（滤波器核仅在水平和垂直中心线非零）。shaking blur不和cutout同时使用，因为cutout填充的黑块会影响blur，而且现实中又模糊又遮挡的场景也比较少见。</p>
<p><strong>Paste</strong></p>
<p>如图2右边部分所示，把变换后的目标块随机粘贴到其他帧的不同位置上。为了减轻直接粘贴在边界处产生的尖锐突变，作者在目标patch外增加了pad，并在pad区域使用混合相加以平滑过渡。此外，还可以从其他视频中选择相似目标块粘贴到背景上，模拟背景（相似物）干扰，作者将其称为Similar-Patch Paste (SPatchP)。</p>
<p><strong>Discussions</strong></p>
<p>利用上述Crop-Transform-Paste可以生成各种训练样本而无需人工标注，因此属于自监督学习。作者讨论了它与典型的关注表征学习的自监督学习（SSL-RL）之间的差异。</p>
<ol>
<li>不同的框架。SSL-RL按照contrastive learning框架训练一个独立的特征学习模块，而本文的方法聚焦在生成无需人工标注的训练数据，因此可以嵌入到任何现有跟踪框架中；</li>
<li>不同的data transformation技术。SSL-RL通过data transformation构造训练对来训练其特征学习模块，而本文设计了特定的跟踪相关的transformations来模拟跟踪过程中的场景变化。由于目标不同，采用了不同的transformation技术。比如后续实验中发现翻转(flip)会产生不利的影响。</li>
</ol>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>本文的实验非常充分，对比了不同的跟踪器，不同的数据变换方式，训练的数据量，以及和有监督方式的结合等。</p>
<h2 id="Analysis-of-Crop-Transform-Paste"><a href="#Analysis-of-Crop-Transform-Paste" class="headerlink" title="Analysis of Crop-Transform-Paste"></a>Analysis of Crop-Transform-Paste</h2><p><strong>Selection of base target patch</strong> 表1对比了不同的目标块生成方式，SS-random表示随机裁剪，SS-annotated表示根据标注裁剪。显然后者优于前者，因为标注目标包含更有效的语义特征和更多的边界信息。并且自监督学习的性能和监督学习（Su）也比较接近。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/43-Crop-Transform-Paste/20210701114700.png" alt=""></p>
<p><strong>Selection of images to paste onto</strong> 表2对比了不同的粘贴方式。裁剪的目标粘贴到相同的视频序列中优于粘贴到其他视频序列中。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/43-Crop-Transform-Paste/20210701115051.png" alt=""></p>
<h2 id="Ablation-of-Each-Transformation"><a href="#Ablation-of-Each-Transformation" class="headerlink" title="Ablation of Each Transformation"></a>Ablation of Each Transformation</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/43-Crop-Transform-Paste/20210701151326.png" alt=""></p>
<p>作者做了两组实验分析每种变换的作用，第一组在只有shift变换的baseline1上添加一个变换来显示性能增益，第二组从使用所有变换的baseline2中删除一个变换来呈现性能下降。表3显示rescale和shift的作用最大，说明尺度和平移变换是最常见的。blur和color jitter单独使用不够好，可能过拟合，而与其他变换结合使用时有益的。flip不管单独使用还是结合其他变换一起均带来负面影响，作者认为它破坏了目标的边界信息（为什么？）。</p>
<h2 id="Advantages-of-the-Proposed-Method"><a href="#Advantages-of-the-Proposed-Method" class="headerlink" title="Advantages of the Proposed Method"></a>Advantages of the Proposed Method</h2><p><strong>Challenge-oriented transformations</strong> Crop-Transform-Paste的一个明显优势在于可以针对跟踪挑战定制变换方法，比如1) rescale for Scale Variation (SV); 2) Cutout for Occlusion (OCC); 3) Blur for Motion Blur (MB) and 4) Similar-Patch Paste (SPatchP) for Background Clutter (BC)。如图3所示，针对OTB100中的各种挑战，使用对应的变换方法能够有效提升性能。在遮挡、运动模糊和背景杂波的测试序列上，cutout, blur, 和 similar-patch paste 在SS+Su模式下的提升甚至超过单独SS模式下的提升，说明这些场景的标注数据很少。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/43-Crop-Transform-Paste/20210701153651.png" alt=""></p>
<p><strong>Data efficiency</strong> 图1和图4对比了自监督学习（SS）、监督学习（Su）和二者结合（SS+Su）在使用不同数量的标注样本训练时的性能。可以看到，SS仅需要很少的标注样本0.6K就能收敛到一个不错的结果。并且二者结合可以进一步提升监督学习的性能。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/43-Crop-Transform-Paste/20210701155122.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/43-Crop-Transform-Paste/20210701155137.png" alt=""></p>
<p><strong>Integration with supervised methods</strong> </p>
<p>表4用SiamRPN++和DiMP证明提出的自监督方法可以嵌入到现有的有监督框架中进一步提升性能。DiMP提升较小是因为它本身有个在线训练模块能够一定程度 处理跟踪目标或场景的变化。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/43-Crop-Transform-Paste/20210701155221.png" alt=""></p>
<h2 id="Evaluation-against-Unsupervised-Methods"><a href="#Evaluation-against-Unsupervised-Methods" class="headerlink" title="Evaluation against Unsupervised Methods"></a>Evaluation against Unsupervised Methods</h2><p>最后是和无监督方法的对比，同样是更优的。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/43-Crop-Transform-Paste/20210701155442.png" alt=""></p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>本文提出了一种基于Crop-Transform-Paste的自监督学习方法，用于在没有人工标注的场景下训练跟踪模型。方法简洁，实验充分。个人认为这篇文章的重点在于自监督而不是数据增广，毕竟增广那部分只是单纯做了些常见的变换，核心还是利用这些变换有针对地合成各类样本。</p>
<p>但就带来一个新的思考：如果我不用合成样本的自监督学习，直接把所有的变换全部作为数据增广加到有监督学习的训练中效果如何？以一个视频序列举例，本文的合成样本只利用视频的某一帧进行变换再粘贴到其他帧上生成N个样本；而传统的监督方法是获取视频中任意N帧得到N个样本。那假如对这N个不同的真实样本直接做数据增广，理论上能够覆盖的样本变换范围应该更大，效果是不是会更好？</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>Data augmentation</tag>
        <tag>Self-supervised</tag>
      </tags>
  </entry>
  <entry>
    <title>Domain Adaptive SiamRPN++ for Object Tracking in the Wild</title>
    <url>/tracking/44-DASiamRPN/</url>
    <content><![CDATA[<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210701165637.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2106.07862">论文</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>基于孪生网络的跟踪算法均假定训练和测试数据遵循相同的分布，然而在正常图像上训练的跟踪器并不能保证在其他领域的数据上（如雨雾天气的序列）也表现良好，即存在域偏移（domain shift）问题，如图1和图2。作者称本文是首次将域分布差异问题引入视觉跟踪领域。</p>
<p>针对这一问题，本文提出一种域自适应方法，包括Pixel Domain Adaptation (PDA) 和 Semantic Domain Adaptation (SDA)。PDA分别对（不同域的）模板和搜索图像的特征对齐，消除天气、光照等引起的像素级域偏移；SDA将（不同域的）跟踪目标的特征表达对齐，以消除语义级的域偏移。二者均通过对抗训练的方式学习域分类器，域分类器强制网络学习域不变的特征表达，从而实现域自适应。</p>
<span id="more"></span>
<p>最后作者在有雾和红外序列两个不同域的数据集上进行了验证。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702104303.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702104337.png" alt=""></p>
<h1 id="Theoretical-Preliminaries"><a href="#Theoretical-Preliminaries" class="headerlink" title="Theoretical Preliminaries"></a>Theoretical Preliminaries</h1><p>最简单粗暴的方法就是搜集许多具有不同域的标注训练数据，但这显然不现实。因此我们的目标是针对<strong>无监督域自适应场景</strong>（即源域有标记而目标域未标记），使跟踪器在源域和目标域上都表现良好，而不需要额外的标注成本。一种通用的方案就是学习域不变（domain-invariant）的特征表达来缩小不同域之间的差异。作者利用 <em>A</em>-distance理论和概率分析来实现这一目的，下面先简单介绍这些概念。</p>
<h2 id="A-distance"><a href="#A-distance" class="headerlink" title="A-distance"></a><em>A</em>-distance</h2><p>给定源域 $S$ 和目标域 $T$， <em>A</em>-distance可以用于衡量两个域样本分布的差异，定义如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702113837.png" alt=""></p>
<p>其中h表示域分类器，$h(x)\rightarrow 0$表示样本x属于源域，$h(x)\rightarrow 1$表示样本x属于目标域。$min \ error(h(x))$表示理想域分类器的预测误差，显然，误差越小（越容易区分）表示域差异越大。现在要最小化域差异$d_A(S,T)$以实现特征对齐，等价于要最大化理想域分类器误差，即</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702114945.png" alt=""></p>
<p>其中 f 表示样本 x 的特征表达。公式(3)是特征提取器 f 和域分类器 h 之间的minimax优化问题。这个怎么理解呢？其实类似GAN，域分类器 h 需要尽可能区分不同域的样本，而特征提取 f 需要欺骗分类器让其难以区分不同域，即让 f 提取到域不变特征。</p>
<p>作者在优化这个问题时采用 Gradient Reversed Layer (GRL)，如下图所示，在梯度从域分类器传到特征提取之前将其取负号反转，希望粉色部分的参数向$L_d$减小的方向优化，绿色部分的参数向$L_d$增大的方向优化，用一个网络一个优化器就实现了两部分有不一样的优化目标，形成<strong>对抗</strong>的关系。（参考<a href="https://www.zhihu.com/question/266710153/answer/1338864403">Gradient Reversal Layer指什么？ - Just4Fan的回答 - 知乎</a> )</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702115615.png" alt=""></p>
<h2 id="Probabilistic-Analysis-for-Object-Tracker"><a href="#Probabilistic-Analysis-for-Object-Tracker" class="headerlink" title="Probabilistic Analysis for Object Tracker"></a>Probabilistic Analysis for Object Tracker</h2><p>作者将跟踪问题看成一个后验概率 $P(S,B|Z,X)$，即给定模板Z和搜索区域X，预测分类得分S和目标框B。由于域偏移的存在，源域的联合概率分布$P_S(S,B,Z,X)$与目标域的联合概率分布$P_T(S,B,Z,X)$是不同的。</p>
<p><strong>Pixel Domain Adaptation</strong> 根据贝叶斯公式，可以将联合概率分布分解成：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702150940.png" alt=""></p>
<p>其中$i \in \{S,T\}$。条件概率$P(S,B|Z,X)$相当于跟踪器的分类回归分支，我们假设这部分对于不同域是一样，那么域偏移主要来自模板和搜索图像的特征提取$P(Z,X)$。为了消除域偏移，需要另Siamese网络提取域不变的特征映射，即$P_S(Z,X) = P_T(Z,X)$</p>
<p><strong>Semantic Domain Adaptation</strong> 上面PDA解决天气或光照引起的全局域偏移，但不同域的目标还存在外观和类别的变化，因此还需要考虑目标语义的域偏移。类似的，可以将联合概率分解成：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702151847.png" alt=""></p>
<p>同样假设条件概率$P(S | B,Z,X)$对于不同域是一样的，那么域偏移主要来自$P(B,Z,X)$。为了消除偏移，需要$P_S(B,Z,X) = P_T(B,Z,X)$，表示给定了模板、搜索区域以及对应的目标框，跟踪目标的特征表达要是一样的。考虑到目标域是没有真实框标注的，因此这里统一采用RPN的预测框表示B。</p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702152654.png" alt=""></p>
<p>图3是整体方法框架，根据上一节的<em>A</em>-distance理论以及概率分析，作者提出了PDA和SDA两个模块。其中PDA针对的是孪生网络的整体特征，SDA针对的是预测框内的目标特征。</p>
<h2 id="Pixel-Domain-Adaptation"><a href="#Pixel-Domain-Adaptation" class="headerlink" title="Pixel Domain Adaptation"></a>Pixel Domain Adaptation</h2><p>PDA包括模板对齐和搜索区域对齐，目的是通过域分类器和Siamese网络之间的minimax优化来混淆跨域的特征映射。域分类器由Conv+MaxPool+FC组成，FC层对每个像素进行二值分类，损失函数为：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702154430.png" alt=""></p>
<p>m,n为像素位置，D是标签，p是预测结果。然后按照公式3的minimax优化，需要对域分类器参数最小化该损失，对siamese特征提取参数最大化该损失，即</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702154740.png" alt=""></p>
<p>$\omega_{pda}$表示PDA域分类器参数，$\varphi$表示孪生网络参数。域分类器的参数更新方向与减少域分类损失的方向相同，这与普通的训练方法相同；而Siamese网络的参数更新方向被反转（GRL），这正是增加域分类损失的方向，二者形成对抗。</p>
<h2 id="Semantic-Domain-Adaptation"><a href="#Semantic-Domain-Adaptation" class="headerlink" title="Semantic Domain Adaptation"></a>Semantic Domain Adaptation</h2><p>由于不同域的类别、视角和姿态的变化，跟踪目标会发生明显的变化，SDA强制跟踪目标的特征表示在语义上是域不变的。具体过程为，通过ROI Align提取预测框内的multi-layer的ROI特征，域分类器（两层FC）对其进行分类，GRL放在域分类器和ROI Align之间。域分类损失为：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702155824.png" alt=""></p>
<p>同样以对抗的方式训练SDA</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702155913.png" alt=""></p>
<p>$\omega_{sda}$表示SDA域分类器参数，$\varphi$表示孪生网络参数。无论跟踪目标来自源域还是目标域，目标的域不变特征都能在分数图中获得较高的响应。</p>
<p>最后总的训练损失包括孪生跟踪器的损失和域自适应损失</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702160122.png" alt=""></p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>训练时使用LaSOT作为源域数据，Foggy GOT-10k和LSOTB-TIR作为目标域数据。Foggy GOT-10k是作者生成的有雾数据集，LSOTB-TIR是红外数据集，注意二者作为目标域数据训练时是没有标注的。模板和搜索图像的裁剪通过运行现有的SiamRPN++对目标域数据集获取伪标签得到的。</p>
<p>表1-4展示了正常天气到有雾的跨域和RGB到红外的跨域的跟踪结果。这里的比较方式有点迷，作者列出每个epoch的结果证明性能的提升，但如果只关注最好的结果发现的性能提升其实不明显。比如Foggy VOT2018 0.211 v.s. 0.218，LSOTB-TIR 0.543 v.s. 0.547。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702162344.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702162404.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702162413.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702162420.png" alt=""></p>
<p>消融实验也呈现一样的结果，如果只比较最好的性能，单独的PDA和SDA甚至不如baseline。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702161634.png" alt=""></p>
<p>其他的一些可视化结果。图6将特征压缩到平面证明了源域和目标域的特征混淆在一起，证明了域不变特征。图7证明了提出的方法在跨域性能表现良好的同时，不会损失在源域上的性能。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702161754.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702161803.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/44-DASiamRPN++/20210702161810.png" alt=""></p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>Domain adaption</tag>
      </tags>
  </entry>
  <entry>
    <title>HiFT: Hierarchical Feature Transformer for Aerial Tracking</title>
    <url>/tracking/45-HiFT/</url>
    <content><![CDATA[<p><img src="https://github.com/kongbia/picgo/raw/master/blog/45-HiFT/20210807114323.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2108.00202">论文</a> <a href="https://github.com/vision4robotics/HiFT">代码</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>现有的孪生跟踪算法大多是基于相似得分图对目标对象进行分类和回归，使用单一的相似图会降低复杂场景下的定位精度，而像SiamRPN++那样单独使用多个相似图分别进行预测又会引入较大计算负担，不适用于移动设备。因此，本文提出一种 hierarchical feature transformer (HiFT) 对多个层级的相似图进行融合，既可以捕获全局的依赖关系，又可以高效地学习多层级特征之间的依赖关系。</p>
<p>在介绍本文方法前，我们先分析经典的transformer架构应用于目标跟踪任务中的难点。</p>
<ol>
<li>预定义的(或学习的)解码query在面对任意跟踪对象时很难保持有效性；</li>
<li>transformer难以处理小目标（参考deform DETR）。</li>
</ol>
<span id="more"></span>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/45-HiFT/20210807215745.png" alt=""></p>
<p>图2为整体框架，分成特征提取，transformer和预测头三部分。特征提取采用高效的alexnet，最后三层特征输入到transformer中，预测头采用类似FCOS的三分支预测（分类、回归、定位质量）。下面详细介绍本文提出的Hierarchical Transformer。</p>
<h2 id="Hierarchical-Feature-Transformer"><a href="#Hierarchical-Feature-Transformer" class="headerlink" title="Hierarchical Feature Transformer"></a>Hierarchical Feature Transformer</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/45-HiFT/20210807221543.png" style="zoom:80%;" /></p>
<p>HiFT包含高分辨率特征编码和低分辨率特征解码，前者学习不同特征层和空间信息之间的相互依赖关系，以提高对不同尺度(特别是低分辨率)目标的关注；而后者聚合了来自低分辨率深层特征的语义信息。这种全局上下文和层次特征之间的相互依赖大大提升了对复杂跟踪场景的适应能力。</p>
<p>transform的输入是三层不同尺度的互相关相似图，如公式1所示。图3中的$M_3’$ 和 $M_4’$ 则是加上了位置编码。​</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/45-HiFT/20210807225118.png" style="zoom:80%;" /></p>
<p><strong>Feature Encoding</strong></p>
<p>首先对$M_3’$ 和 $M_4’$ 进行相加和归一化的融合，得到$M_E^1 = Norm(M_3’+M_4’)$；然后经过multi-head attention得到 $M_E^2 \in WH \times C$，attention矩阵中同时包含了$M_3’$ 和 $M_4’$的多尺度信息，注意这里MHA中Q, K, V的输入差异；</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/45-HiFT/20210808094324.png" style="zoom:80%;" /></p>
<p>此外还额外增加了一个调制层 (modulation layer)，探索$M_4’$ 和 $M_E^3$ 之间的空间信息。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/45-HiFT/20210808095158.png" style="zoom:80%;" /></p>
<p><strong>Feature Decoding</strong></p>
<p>decoder部分和标准的transformer类似，差别在于输入的查询向量不是预定义的query，而是低分辨率的特征$M_5 \in WH \times C$​​，并且无需位置编码。</p>
<p>作者在实验中堆叠了一个编码和两个解码结构。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Evaluation-on-Aerial-Benchmarks"><a href="#Evaluation-on-Aerial-Benchmarks" class="headerlink" title="Evaluation on Aerial Benchmarks"></a>Evaluation on Aerial Benchmarks</h2><p>本文的应用环境是无人机跟踪，所以测试数据集均在无人机数据集测试。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/45-HiFT/20210808100142.png" style="zoom:80%;" /></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/45-HiFT/20210808100159.png" style="zoom:80%;" /></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/45-HiFT/20210808100237.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/45-HiFT/20210808100251.png" style="zoom:80%;" /></p>
<h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/45-HiFT/20210808100338.png" style="zoom:80%;" /></p>
<p>图5中OT表示标准transformer结构，FT表示用特征取代解码器中的object query，PE表示在解码输入中加入位置编码，RL表示在GT的矩形框内采样正样本（本文用的椭圆采样策略）。可以看到，OT使得性能下降，证明预定义的object query不适用于目标任意的跟踪任务；增加了PE后相比不用PE性能大幅下降；使用RL性能同样大幅下降，这样看上去似乎label assign策略的影响都要大于HFT了。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/45-HiFT/20210808102502.png" style="zoom:80%;" /></p>
<p>图5展示了本文方法在快速运动、低分辨率和遮挡等场景均可以更聚焦目标。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/45-HiFT/20210808102632.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/45-HiFT/20210808102731.png" alt=""></p>
<p>速度一骑绝尘，大于100FPS，并且使用alexnet的性能超过了很多resnet50的算法。并且作者在嵌入式平台NVIDIA AGX Xavier中实验也达到了31.2FPS(未使用tensorrt)，非常适合应用。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>transform</tag>
        <tag>ICCV2021</tag>
      </tags>
  </entry>
  <entry>
    <title>Learn to Match: Automatic Matching Network Design for Visual Tracking</title>
    <url>/tracking/46-AutoMatch/</url>
    <content><![CDATA[<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210808103150.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2108.00803">论文</a> <a href="https://github.com/JudasDie/SOTS">代码</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>本文聚焦于孪生跟踪算法的匹配过程，目前主流的互相关操作是启发式设计的，严重依赖人工经验，并且单一的匹配方法无法适应各种复杂的跟踪场景。因此，本文引入了6种新的匹配算子来替代互相关。通过分析这些算子在不同跟踪挑战场景下的适应性，作者发现可以将它们结合起来进行互补，并借鉴NAS思想提出一种搜索方法 binary channel manipulation (BCM) 探索这些匹配算子的最优组合。</p>
<span id="more"></span>
<h1 id="Analysis-of-Matching-Operators"><a href="#Analysis-of-Matching-Operators" class="headerlink" title="Analysis of Matching Operators"></a>Analysis of Matching Operators</h1><p>首先介绍本文采用的6种匹配算子，Concatenation, Pointwise-Addition , Pairwise-Relation , FiLM , Simple-Transformer 和Transductive-Guidance。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210808161330.png" alt=""></p>
<p><strong>Concatenation</strong> 图2(a)，将模板特征$F_z$ pool成 $f_z \in 1 \times 1 \times C$，再与搜索特征$F_x$拼接。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210808161411.png" style="zoom:80%;" /></p>
<p><strong>Pointwise-Addition</strong> 图2(b)，将上面的拼接换成了对应元素相加。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210808161701.png" style="zoom:80%;" /></p>
<p><strong>Pairwise-Relation</strong> 图2(c)，类似non-local，将$F_x, F_z$分别reshape成 $H_x W_x \times C$ 和 $C \times H_z W_z $​，然后做矩阵乘法。相当于将模板中的每个元素都与搜索特征的所有元素衡量相似性。​</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210808192301.png" style="zoom:80%;" /></p>
<p><strong>FiLM</strong> 图2(d)，这个思路源于视觉推理，通过对神经网络的“中间特征”应用仿射变换来自适应地影响网络的输出。如公式5所示，对$f_Z$卷积变换后得到系数$\gamma$和偏置$\beta$，将其作用于搜索特征$F_x$​</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210808202649.png" style="zoom:80%;" /></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210808202908.png" style="zoom:80%;" /></p>
<p><strong>Simple-Transformer</strong> 图3(a)，就是multi head attention。</p>
<p><strong>Transductive-Guidance</strong>  图3(b)，源于VOS中的掩码传播机制，即用前一帧的掩码引导当前帧的预测。首先像Pairwise-Relation一样计算模板和搜索特征之间的对应关系(affinity)（公式7），然后用第一帧的伪掩码进行调制（公式8），生成的$G$中的每个位置表示前景的概率，最后将$G$与原始搜索特征$F_x$相加（公式9）。整个过程可以参考该作者另一篇文章<a href="https://arxiv.org/abs/2008.02745v2">OceanPlus</a>。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210808221525.png" style="zoom:80%;" /></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210808222703.png" style="zoom:80%;" /></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210808222743.png" style="zoom:80%;" /></p>
<p><strong>Analysis</strong></p>
<p>作者将上述6种算子与传统的互相关匹配分别在OTB100上进行测试，比较其性能，如表1所示。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210808223451.png" alt=""></p>
<p>对于单一的匹配算子，除了6和7，其他的均取得了和dw-corr相当的性能，甚至2（直接拼接）的性能更好。</p>
<p>虽然2的性能最优，但并不能保证其在所有跟踪挑战中都是最好的，比如在SV，OPR，OV和LR上就被其他算子超过。作者在图4进一步可视化了响应图，可以看到depthwise cross-correlation (a), Pairwise-relation (d), and Transductive-Guidance (g)更关注目标本身；而the concatenation (b), Pointwise-Addition (c), Simple-Transformer (e), and FiLM (e)则包含了更多上下文信息。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210808225804.png" style="zoom:80%;" /></p>
<p>上述结果表明不同的匹配算子在不同跟踪场景下具有不同的可靠性，由此想到是否可以将它们结合起来，利用这些特征进行互补。因此，作者接下来提出一种自适应学习的自动选择和组合匹配算子的方法。</p>
<h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210809101847.png" alt=""></p>
<p>图5为整体框架，首先将模板和搜索特征送入多个匹配算子构成的搜索空间，生成m个响应特征 $\{ r_1, r_2, … , r_m \}$​，给每个响应特征的每个通道都赋予一个可学习的调制器(manipulator) $\omega_i^j$​​​，表示该通道的贡献。然后引入二元Gumbel-Softmax来离散化这些调制器进行二元决策，训练过程参考DARTS的两层优化。最后根据学好的调制器保留两个匹配算子构建新的跟踪器再次训练。</p>
<p>下面分别介绍二元的通道调制器(Binary Channel Manipulation)和两层优化(Bilevel Optimization)</p>
<h2 id="Binary-Channel-Manipulation"><a href="#Binary-Channel-Manipulation" class="headerlink" title="Binary Channel Manipulation"></a>Binary Channel Manipulation</h2><p>Binary Channel Manipulation (BCM) 用于决定匹配算子对目标状态预测的贡献，它给每个匹配响应特征的每个通道都赋予一个可学习的调制器(manipulator) $\omega_i^j$​​，将所有调制后的特征拼接聚合成一个大的特征（有点类似NAS里的supernet）。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210809111736.png" style="zoom:80%;" /></p>
<p>其中$r_i^j$​ 表示第 i 个响应特征的第 j 个通道。对于每个匹配算子，我们将其对应的所有通道调制器相加得到该算子的potential $p_i$​，这个在后面选择匹配算子时会用到。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210809112703.png" style="zoom:80%;" /></p>
<p>接下来，将连续的$\omega_i^j$​​转换为离散的，离散后的类别是二值分类，概率向量为$\pi = \{ \pi_1 = \sigma(\omega_i^j), \pi_2 = 1 - \sigma(\omega_i^j) \}$​​。为了能够可微的反向传播，这里利用<a href="https://www.cnblogs.com/initial-h/p/9468974.html">Gumbel-Softmax</a>进行训练。对概率向量$\pi$​​对应的随机变量添加Gumbel噪声$g_k$​​后随机采样</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210809114815.png" style="zoom:80%;" /></p>
<p>然后将argmax替换为Softmax定义一个连续可微的近似</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210809114947.png" style="zoom:80%;" /></p>
<p>因为这只是一个二值概率分布，公式14带入$\pi$的表达式后可以被简化成</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210809115114.png" style="zoom:80%;" /></p>
<p>推导如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210809120120.png" alt=""></p>
<p>作者这里令$\tau=1, g_k=0$​​，对于离散采样的样本d，前向传播时使用硬值(0或1)，反向传播采用软值获得梯度。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210809120823.png" style="zoom:80%;" /></p>
<h2 id="Bilevel-Optimization"><a href="#Bilevel-Optimization" class="headerlink" title="Bilevel Optimization"></a>Bilevel Optimization</h2><p>优化的目标有两部分，一个是调制器参数$\omega$，另一个是匹配网络的卷积层参数$\theta$。这里借鉴DARTS的优化方法，利用训练集优化$\theta$，利用验证集优化$\omega$，记作</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210809154925.png" style="zoom:80%;" /></p>
<p>为了加速收敛，将其简化成</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210809155027.png" style="zoom:80%;" /></p>
<p>按上述方式训练后，保留potential $p_i$最高的两个匹配算子构成新的跟踪器重新训练。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h2><p>训练过程分成匹配网络的搜索和新跟踪器的训练两个阶段，第一阶段用Bilevel Optimization搜索最优的匹配网络组合，第二阶段用优化的匹配网络构建一个新的跟踪器进行常规的训练。搜索算法为分类和回归分支确定不同的匹配网络。经过第一阶段的训练后，分类分支使用Simple-Transformer和FiLM，而回归分支使用FiLM 和 Pairwise-Relation。</p>
<h2 id="State-of-the-art-Comparison"><a href="#State-of-the-art-Comparison" class="headerlink" title="State-of-the-art Comparison"></a>State-of-the-art Comparison</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210809163533.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210809163718.png" style="zoom:80%;" /></p>
<h2 id="Ablation-and-Analysis"><a href="#Ablation-and-Analysis" class="headerlink" title="Ablation and Analysis"></a>Ablation and Analysis</h2><p><strong>One or Many Manipulators</strong> 本文给匹配算子的每个通道都赋予一个调制器，作者也尝试了给每个匹配算子只赋予一个标量的调制器，性能会有所下降 OTB100 69.5，LaSOT 54.7。</p>
<p><strong>Random Search</strong> 随机选择匹配算子同样会使性能下降，OTB100 69.1，LaSOT 53.2。</p>
<p><strong>NAS-like Matching Cell</strong> 用DARTS的方法构建一个有向无环图进行搜索，如图7所示，同样不如本文的结果，且速度会大幅下降。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/46-AutoMatch/20210809165018.png" style="zoom:80%;" /></p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><ol>
<li>DW-Corr并不是孪生跟踪算法的最优选择，本文设计了6种新的匹配算子；</li>
<li>本文用一种自动搜索的方式组合出最优的匹配网络。</li>
</ol>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>ICCV2021</tag>
        <tag>matching</tag>
      </tags>
  </entry>
  <entry>
    <title>Saliency-Associated Object Tracking</title>
    <url>/tracking/47-SAOT/</url>
    <content><![CDATA[<p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210817170638.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2108.03637">论文</a> <a href="https://github.com/ZikunZhou/SAOT.git">代码</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>和上一篇一样是研究跟踪的匹配问题。目前主流的跟踪策略分为全局和局部两种，全局策略（如DW-Corr）学习目标的整体表示，当目标发生较大变化时表现不佳。局部策略（如PG-Corr）将目标分割成多个patch，并行跟踪所有patch，通过聚合这些patch的跟踪结果，推断出目标状态。而局部策略的局限在于并不是所有patch都包含丰富的信息，一些没有判别能力的patch难以跟踪，可能对推断目标状态产生不利影响。因此，本文提出只跟踪目标的显著局部区域而不是简单跟踪所有局部块，具体提出了细粒度的显著性挖掘模块(fine-grained saliency mining module)，用于捕获局部显著性；以及显著性关联模块(saliency-association modeling module) 将捕获的显著区域关联在一起，学习目标模板与搜索图像之间的全局相关性，以进行状态估计。</p>
<span id="more"></span>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210817211327.png" alt=""></p>
<h2 id="Overall-Framework"><a href="#Overall-Framework" class="headerlink" title="Overall Framework"></a>Overall Framework</h2><p>整体框架如图2所示，模板特征$F_x \in \mathbb{R}^{h_x \times w_x \times c } $ 和搜索特征 $F_s \in \mathbb{R}^{h_s \times w_s \times c } $ 首先送入Saliency Mining module用于提取模板的局部显著性，该模块计算模板和搜索特征的pixel-to-pixel相似性，选择局部最大且最锐利的点(local sharp maximum points) 作为显著点，这些显著点表示模板中最具判别性的区域。</p>
<p>然后，Saliency-Association Modeling module将捕获的显著点进行关联，以学习模板和搜索图像之间的有效全局相关性表示。最后再对相关性结果进行状态估计。</p>
<h2 id="Saliency-Mining"><a href="#Saliency-Mining" class="headerlink" title="Saliency Mining"></a>Saliency Mining</h2><p>显著性挖掘模块用于捕获模板中具有判别能力的局部显著区域，包括两个步骤：</p>
<ol>
<li>为模板$F_x$中的每个点构建与搜索特征$F_s$的相似图；</li>
<li>根据相似图衡量$F_x$中每个点的显著性，从而进行选择。</li>
</ol>
<p><strong>Construction of similarity maps</strong> </p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210818103912.png" alt=""></p>
<p>pixel-to-pixel相似图的构建如图3所示，将模板特征中的每个点$(u,v)$与搜索特征计算余弦距离，得到单通道的相似图$S_(u,v) \in  \mathbb{R}^{h_s \times w_s}$，总共会生成$h_x \times w_x$这样的相似图。</p>
<p><strong>Saliency evaluation</strong> 接下来根据每个相似性图中最大值点附近的峰值分布（包括强度和集中度）来评估对应模板特征点的显著性。强度使用峰值旁瓣比PSR进行衡量</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210818112612.png" style="zoom:80%;" /></p>
<p>其中，$\Phi$表示旁瓣，$\mu_{\Phi}, \sigma_{\Phi}$分别表示旁瓣的均值和方差。在原始的PSR中，主瓣$\Psi$的大小是预定义的固定值，作者认为这样定义是不合理的，因为没有考虑响应图的分布。图4展示了两种不同峰分布的相似图，这两个相似图显然对应着主瓣的大小不同。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210818115217.png" alt=""></p>
<p>为了消除这一影响，作者将主瓣的边界定义为峰附近最靠近的轮廓，其高度等于整个相似图的平均值。至此，我们可以得到相似图$S_{ (u,v) }$的峰强度$\gamma$：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210818160143.png" style="zoom:80%;" /></p>
<p>用于显著性评价的另一个度量是峰分布的集中度(concentration)，它与主瓣覆盖面积$A_{\Psi}$成反比。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210818160322.png" style="zoom:80%;" /></p>
<p>将上面的强度和集中度结合起来就得到了相似图$S_{ (u,v) }$的显著性$s( S_{ (u,v) } )$：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210818160635.png" style="zoom:80%;" /></p>
<p>考虑到应该尽可能跟踪目标模板的中心，因此在计算显著性时额外增加一个高斯正则项，公式7中的高斯项$g$是与模板中心对齐的。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210818161803.png" style="zoom:80%;" /></p>
<p>根据公式7的显著性评价方式，我们可以计算模板特征$F_x$中每个点的显著性，并选取K个最大的点构成显著性集合 $P_x = \{ p_x^k \}_{k=1}^K$，这个显著性结合在搜索特征$F_s$的对应匹配点集合为$P_s = \{ p_s^k \}_{k=1}^K$。</p>
<h2 id="Saliency-Association-Modeling"><a href="#Saliency-Association-Modeling" class="headerlink" title="Saliency-Association Modeling"></a>Saliency-Association Modeling</h2><p>显著性关联模块利用捕获的显著性来学习模板和搜索特征之间的全局关联，作者利用图的方式进行构建，同样包括两步：</p>
<ol>
<li>在捕获的显著性之间构建一个图，以建模这些显著性之间的交互关系；</li>
<li>基于构造的图聚合显著性。</li>
</ol>
<p><strong>Construction of the saliency graph</strong>  构建显著性图时同时考虑了相似图$S$和搜索特征$F_s$，将二者拼接得到 $F_g$，共有$h_s \times w_s$ 个节点，每个节点的维度是 $h_x w_x + c$。然后构建图的边，如图5所示，包括两种类型，一种是对显著点之间进行连接，另一种是对每个点和其邻域点进行连接，边的集合定义为$C$。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210818164636.png" alt=""></p>
<p>本文用两层全连接层来学习边的权重，定义权重的邻接矩阵为 $A \in \mathbb{R}^{N \times N}, N=h_s w_s $</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210818170326.png" alt=""></p>
<p>其中 $v1, v2$是图的两个节点，$\phi_1, \phi_2$是两层全连接层，用sigmoid函数约束权值范围在0到1之间。只有当边$\langle i, j \rangle$符合图5中的两种情况才有权值。</p>
<p><strong>Aggregation of the captured saliencies</strong>  使用两层的GCN进行显著性聚合</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210818172505.png" alt=""></p>
<p>其中m和M表示多项式的阶数和总阶数，$\omega_m$是可学习权重。$\hat{A} = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{\frac{1}{2}}$是normalized邻接矩阵，$\tilde{A}=A+I$，$ \tilde{D}$是$\tilde{A}$的对角阵，这些均为图卷积知识。$X^{l} \in \mathbb{R}^{N \times d_l}$和$X^{l+1} \in \mathbb{R}^{N \times d_{l+1} }$是第 l 层上所有节点的输入和输出特征，其中 $d_l, d_{l+1}$是对应的特征维度，$d_0 = h_x w_x + c$，$ \Theta_m^l \in \mathbb{R}^{d_l \times d_{l+1}} $是第l层第m阶的参数矩阵。</p>
<p>通过构造显著性图并进一步进行显著性聚合，Saliency-Association Modeling module能够学习目标模板与搜索图像之间的全局相关表示，进而用于预测搜索图像中的目标状态。</p>
<h2 id="Tracking-Framework"><a href="#Tracking-Framework" class="headerlink" title="Tracking Framework"></a>Tracking Framework</h2><p>跟踪框架如图2所示，预测头采用FCOS，并增加DiMP的在线跟踪。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210818212315.png" alt=""></p>
<p><strong>Base model</strong> 只有特征提取，DiMP的在线分类，以及预测头；</p>
<p><strong>PPFM</strong> 只做图3的Pixel-to-Pixel匹配，然后将拼接后的$S$和$F_s$直接过两层卷积得到相关结果；</p>
<p><strong>PAM</strong> 没有提取显著性，将所有局部平等的进行关联（类似self-attention）；</p>
<p>PPFM和base model的对比证明建模模板和搜索图像之间的相似性是有益的，PAM和PPFM的对比证明将匹配的局部进行关联是有益的；最后SAOT和PAM的对比证明了局部显著性是有益的。最后还与经典的DW-Corr和PG-Corr进行比较。</p>
<h2 id="SOTA-Comparison"><a href="#SOTA-Comparison" class="headerlink" title="SOTA Comparison"></a>SOTA Comparison</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210818213939.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210818213952.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210818214010.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210818214021.png" alt=""></p>
<h2 id="Qualitative-Study"><a href="#Qualitative-Study" class="headerlink" title="Qualitative Study"></a>Qualitative Study</h2><p>可视化结果有点牛逼，对于形变和干扰场景的响应看上去非常好。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210818214434.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/47-SAOT/20210818214104.png" alt=""></p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>ICCV2021</tag>
        <tag>matching</tag>
      </tags>
  </entry>
  <entry>
    <title>RPT++: Customized Feature Representation for Siamese Visual Tracking</title>
    <url>/tracking/48-RPT/</url>
    <content><![CDATA[<p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211122111651.png" alt=""><a href="https://arxiv.org/abs/2110.12194">论文</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>本文是去年RPT方法的改进，作者来自同一团队。出发点是现有的跟踪方法对于分类和回归使用的是同一套特征，而作者指出这两个任务是有差异的，因此对特征的需求也是不同的。如图1所示，分类需要的是显著区域中更具有判别力的特征，而回归需要边界附近的特征来精确定位。针对这一问题，作者提出了两种定制化的特征提取，用于捕获特定任务的视觉模式。其中Polar Pooling从语义关键点收集丰富的信息，以进行更强的分类；而Extreme Pooling捕获目标边界的清晰视觉模式，实现目标状态的精确估计。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211122115832.png" alt=""></p>
<span id="more"></span>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>首先分析一下现有pipeline的局限。主流的anchor和anchor-free的方法都是在一个特征点上进行分类和回归，这种基于点的特征表示方法传递显式语义和边界信息的能力较弱。另一些方法通过提取目标框内的ROI特征来提升表征目标的能力，但矩形框可能包含多余的背景像素，且缺乏对物体几何结构的建模能力。</p>
<p>为了解决这些问题，作者先cue了一下之前的工作RPT。RPT用若干个有代表性的点集来表示目标，通过可变形卷积自动学习语义显著性和边界区域的特征。这种方法比单点特征包含更多可识别的信息，有助于真正理解对象的视觉模式。<strong>但是</strong>，RPT提取的关键点真的都可靠吗？事实上，RPT经常提取一些位于背景显著区域的错误关键点。  并且RPT用于分类和回归的特征均取自相同的点集，忽略了这两个任务之间的不对齐。如上面图1介绍的，从语义关键点提取的特征为分类提供了更具判别力的视觉模式，而边界附近的特征编码了关于空间范围的先验知识，有助于准确估计目标状态。这两个任务在特征表示中的差异极大地限制了跟踪器的性能。</p>
<p>因此，本文在RPT的基础上定制了两个特征提取器，分别从对应的关键点中获得语义显著信息和边界极值信息。其中，Polar Pooling通过<strong>计算从中心到每个语义关键点的径向最大响应</strong>来捕获目标区域内更精确的视觉模式。Extreme Pooling通过一个额外的<strong>不确定性分支</strong>来消除边界极值点估计中的模糊性。最后将这两个增强后的特征分别送入分类和回归分支。</p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h2><p>首先回顾一下baseline的RPT方法。在模板和搜索特征做互相关后，相关图上的每个特征都可以看成一个目标候选，用一系列代表性点集表示。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211122213117.png" alt=""></p>
<p>其中n表示点的个数，默认为9个。RPT通过两步来细化这些点的分布:</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211122213232.png" alt=""></p>
<p>公式2可以通过下图直观理解，对于一个初始中心点$(x_k, y_k)$，先通过回归偏移$\{(x_k^c, y_k^c)\}^n_{k=1}$得到一个粗略的点集，然后对每个点再进行一次微调$\{(x_k^r, y_k^r)\}^n_{k=1}$得到最终的目标状态。RPT通过可变形卷积得到更强大的特征表示，能够对物体的几何变换进行建模，生成的特征同时用于分类和回归。而下面介绍的改进就是通过两个不同的模块生成不同的特征进行分类和回归。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211122212826.png" alt=""></p>
<h2 id="Framework-of-RPT"><a href="#Framework-of-RPT" class="headerlink" title="Framework of RPT++"></a>Framework of RPT++</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211122214512.png" alt=""></p>
<p>RPT++整体框架如图2所示，首先预测相关特征图上每个位置的粗略关键点集，包括四个边界极值关键点（左上右下）和五个语义关键点。将四个边界点转换为伪框如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211122223816.png" alt=""></p>
<p>其中$(x_{init}, y_{init})$表示相关特征图上每个点，$\Delta x^c_{leftmost}, \Delta x^c_{topmost}, \Delta x^c_{rightmost}, \Delta x^c_{bottommost}$对应左、上、右、下的极值关键点的伪框偏移量。与RPT不同的是，本文还额外估计了每个偏移量的不确定性，用一个高斯分布表示。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211123103507.png" alt=""></p>
<p>其中均值$o_e \in \{ \Delta x^c_{leftmost}, \Delta x^c_{topmost}, \Delta x^c_{rightmost}, \Delta x^c_{bottommost}\}$表示预测偏移量，方差$\sigma$表示不确定性。这种方法常用于目标检测中，用一个概率分布来表示回归预测而不是仅仅估计一个硬值，从而预测回归的不确定性。</p>
<p>总结一下，粗略关键点集的预测对相关特征图上的每个位置，在回归分支输出n个2D偏移量，以逐点的方式细化样本点的分布；在不确定性分支输出4个伪框偏移量的不确定性。接下来，我们将语义关键点送入polar pooling得到更利于分类的视觉表示，将边界极值关键点和不确定性预测送入extreme pooling得到更精确的边界框。</p>
<h2 id="Customized-Feature-Extraction"><a href="#Customized-Feature-Extraction" class="headerlink" title="Customized Feature Extraction"></a>Customized Feature Extraction</h2><p><strong>Extreme Pooling</strong> 在某些情况下，目标的极值点是不明确的(例如，沿着车辆边界顶部的任何一点都可能被视为极值点)。这些模糊性使得有效的极值特征难以提取，直接限制了定位精度。Extreme pooling就是为了解决这一问题，具体来说，既然我们已经得到了每个边界极值点的不确定性，那么提取边界特征时就不用局限仅提取该点的特征，而是可以提取该点附近的一个不确定性区域的特征，这样可以更好地描述目标边界。整个过程如图3所示。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211123110412.png" alt=""></p>
<p>Extreme pooling首先在每个边界极值点$(x_e,y_e)$上根据其对应的不确定性$\sigma$裁剪一个区域</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211123110843.png" alt=""></p>
<p>其中$(x_1,y_1), (x_2,y_2)$表示裁剪区域的角点，$\mu$是缩放因子，这个不确定性区域的大小为$2\mu\sigma \times 2\mu\sigma \times C$。然后将该区域的特征送入ROIAlign+MaxPool，还原成$1 \times 1$的特征。将4个极值点都做一样的操作并与原始特征拼接，就得到了通道维度为5C的特征，其中包含了边界的不确定性。</p>
<p><strong>Polar Pooling</strong> 伪框中的语义关键点经常落在目标之外的背景上，这传达了不准确的视觉模式。因此作者设计了polar pooling，如图4所示。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211123112846.png" alt=""></p>
<p>将原始点到每个语义关键点的径向路径平均分为N个点，取这N个点的最大值特征作为输出，用公式7表示。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211123113127.png" alt=""></p>
<p>其中$(x_c,y_c), (x_s,y_s)$分别表示原始点和语义点的坐标，括号里的坐标可能是小数，采用双线性插值进行估计。最后，将5个语义点的输出特征值$F_s$与原始特征拼接起来，构造语义增强的特征，通道维度为6C。</p>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>损失函数包括三部分</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211123115234.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211123115247.png" alt=""></p>
<p>其中$L_{reg}^C$表示学习粗略点集的KL loss，参考这篇<a href="https://blog.csdn.net/qq_14845119/article/details/102753188">博客</a>；$L_{cls}^R, L_{reg}^R$是refine过程中的Focal loss和IOU loss。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Comparison-with-SOTA"><a href="#Comparison-with-SOTA" class="headerlink" title="Comparison with SOTA"></a>Comparison with SOTA</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211123115730.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211123115837.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211123115816.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211123120046.png" alt=""></p>
<h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/48-RPT++/20211123120206.png" alt=""></p>
<p>表5 RPT++各个组件的消融实验</p>
<p>表6 polar pooling径向采样个数N分析</p>
<p>表7 extreme pooling缩放因子$\mu$ 分析</p>
<p>表8 extreme pooling 采样窗口的影响，对比了固定大小的crop以及根据不确定性crop。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>arxiv</tag>
      </tags>
  </entry>
  <entry>
    <title>Siamese Transformer Pyramid Networks for Real-Time UAV Tracking</title>
    <url>/tracking/49-SiamTPN/</url>
    <content><![CDATA[<p><img src="https://github.com/kongbia/picgo/raw/master/blog/49-SiamTPN/20211123155750.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2110.08822">论文</a> <a href="https://github.com/RISC-NYUAD/SiamTPNTracker">代码</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>本文着眼于移动平台的无人机目标跟踪，融合了CNN和Transformer的优点。具体来说，通过轻量的shufflenet v2来构建特征金字塔，并使用Transformer对其进行强化（特征融合），以构建一个鲁棒的目标外观模型。开发了一种具有横向交叉注意力的集中式架构，用于构建增强的高级特征图。此外，作者设计了pooling attention module减少key和value的数量进一步降低了Transformer的内存消耗和时间复杂度。提出的方法在CPU端运行速度可超过30 FPS。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/49-SiamTPN/20211123164050.png" alt=""></p>
<span id="more"></span>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/49-SiamTPN/20211123171117.png" alt=""></p>
<p>图2c为本文提出的框架，注意作者没有直接迁移复杂的transformer编码器和解码器结构，而是利用编码器设计了基于注意力的特征金字塔融合网络来更有效地学习target-specific的模型。下面分别介绍各个模块。</p>
<h2 id="Feature-Extraction-Network"><a href="#Feature-Extraction-Network" class="headerlink" title="Feature Extraction Network"></a>Feature Extraction Network</h2><p>特征提取网络输出stage 3，4，5降采样倍数分别为8，16，32倍的特征，然后将模板和搜索特征分别送入Transformer Pyramid Network(TPN)进行特征融合，将融合后的特征进行互相关。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/49-SiamTPN/20211123173713.png" alt=""></p>
<p>其中，$\Gamma$表示TPN模块，M表示互相关结果。</p>
<h2 id="Feature-Fusion-Network"><a href="#Feature-Fusion-Network" class="headerlink" title="Feature Fusion Network"></a>Feature Fusion Network</h2><p><strong>Multi-head Attention</strong> 经典的MHA公式如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/49-SiamTPN/20211123173945.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/49-SiamTPN/20211123173954.png" alt=""></p>
<p><strong>Pooling Attention</strong> MHA的计算量如下</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/49-SiamTPN/20211123180328.png" alt=""></p>
<p>降低计算量的方法有三种：（1）减少query的数量，（2）减少维度C，（3）减少key和value数量。（1）和（2）都会减少输入到后续预测头的特征维度（包括空间和通道），影响跟踪精度。因此我们选择（3），通过池化操作来降低K和V的空间尺寸。</p>
<p>为了进一步降低计算量，作者去掉了位置编码，原因包括：1）输入token的排列会受到最终互相关的约束；2）位置编码会占用额外的计算和存储资源。最终的pooling attention block(PAB)可以写成</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/49-SiamTPN/20211123210427.png" alt=""></p>
<p>图3对比了PAB和传统MHA的区别</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/49-SiamTPN/20211123210646.png" alt=""></p>
<h2 id="Transformer-Pyramid-Network"><a href="#Transformer-Pyramid-Network" class="headerlink" title="Transformer Pyramid Network"></a>Transformer Pyramid Network</h2><p>为了利用同时具有低级信息和高级语义的特征金字塔，作者提出Transformer Pyramid Network (TPN)来构建具有高级语义的混合特征。TPN如图4所示，输入特征金字塔$\{P_3, P_4, P_5\}$，输出融合特征$\{P_3’, P_4’, P_5’\}$，中间包含若干个TPN block。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/49-SiamTPN/20211123211856.png" alt=""></p>
<p>TPN block使用$P_4$作为所有特征层次的query，产生3个具有不同池化尺度的组合，这些组合由3个并行的PAB模块处理。其中$P_3, P_4, P_5$的池化尺寸分别为4，2，1（图4中似乎标反了）。三个尺度的输出直接相加然后送入两个自注意力的PAB中，得到最终的语义特征。整个过程用公式表示：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/49-SiamTPN/20211123212739.png" alt=""></p>
<p>$P_3, P_5$直接恒等映射，以减少计算开销。PA block可以有效地提高层次特征之间的相关性。TPN Block重复B次，生成的特征用于后续互相关。预测头部分就是简单的无锚框分类回归结构，分类损失为交叉熵，回归损失为GIOU loss和L1 loss。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>实现细节上，搜索特征和模板大小分别为256和80，是目标大小的4倍和1.5倍，对应的特征金字塔尺寸为$\{h_3^x=32, h_4^x=16, h_5^x=8 \}$, $\{h_3^z=10, h_4^z=5, h_5^z=3 \}$。</p>
<h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/49-SiamTPN/20211123215218.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/49-SiamTPN/20211123215233.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/49-SiamTPN/20211123215552.png" alt=""></p>
<p>Attention可视化。图6中的第二、三列对比了有无TPN的响应映射。如果没有TPN来学习区分特征，相关结果将变得分散，并且更容易漂移到干扰项。最后三列说明了金字塔特征之间的注意力图。低层级(P3到P4，P4到P4)之间的注意力在整个搜索区域提取了更多的局部信息，而高层级(P5到P4)的注意力更集中在目标的语义上。</p>
<h2 id="SOTA-Comparison"><a href="#SOTA-Comparison" class="headerlink" title="SOTA Comparison"></a>SOTA Comparison</h2><p>表3中shufflenet版本的速度在CPU上达到32.1 FPS，alexnet版本在gpu上速度105FPS，且精度超过了使用resnet50的SiamRPN++和HiFT。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/49-SiamTPN/20211123220034.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/49-SiamTPN/20211123220047.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/49-SiamTPN/20211123220108.png" alt=""></p>
<p>最后作者在真实场景中进行测试，不是很了解就不介绍了，感兴趣可以阅读原文。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>transform</tag>
        <tag>WACV2022</tag>
      </tags>
  </entry>
  <entry>
    <title>Explicitly Modeling the Discriminability for Instance-Aware Visual Object Tracking</title>
    <url>/tracking/50-IAT/</url>
    <content><![CDATA[<p><img src="https://github.com/kongbia/picgo/raw/master/blog/50-IAT/20211124104316.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2110.15030">论文</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>作者认为当前方法的局限在于，跟踪目标的特征仅被表达学习，而没有做判别性的建模（即网络只学会了如何从表观上描述一个目标的特征，但并未学到不同目标特征之间的差异）。为了解决这一问题，本文引入对比学习（contrastive learning）构建实例级的跟踪器 Instance-Aware Tracker (IAT)，确保每个训练样本都能被唯一建模，并与其他大量样本高度区分。提出的IAT包括video-level和object-level两种形式，前者提高了从背景中识别目标的能力，后者提高了区分目标和干扰物的判别能力。</p>
<span id="more"></span>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>首先举一个例子，当人类在跟踪目标时，比如一只猫，我们不但会尽力记住它的眼睛、颜色、耳朵的形状等几个显著特征；同时利用大脑中的强先验知识还会知道这是一只特定的猫，知道这只猫和其他猫有什么不同，甚至知道和我们脑海中所有其他的东西有什么不同。这就是作者的出发点，一个强大的表征不仅应该<strong>表达性</strong>(expressively)地描述目标的特征，还需要<strong>区分性</strong>(discriminatively)地描述目标的特征。</p>
<p>对于判别性建模当前已经有一些工作：DaSiamRPN增加语义负样本来提升判别性，但一个模板一次只能看到一个负样本，因此效果有限。SINT++提出生成难正样本进行鲁棒跟踪，但没有考虑负样本的重要性。<a href="https://ieeexplore.ieee.org/document/9324950">Yao</a>在在线更新时选择难负样本来提高模型的适应性，但会降低推理时的速度。DiMP充分利用目标-背景差异达到了较强的性能，但同样面对负样本数量有限的问题。这些方法均不完全或隐式地模拟了判别性建模，因此阻碍了性能提升。</p>
<p>因此，本文提出实例感知跟踪器 (Instance-Aware Tracker, IAT) 显式地建模视觉跟踪的判别性，首次将基于对比学习的实例级分类任务集成到跟踪中。作者借鉴了经典的MoCo算法（算法结构如下图），可以参考<a href="https://www.bilibili.com/video/BV1av411874e?spm_id_from=333.999.0.0">MoCo论文简析</a>。将每个目标看成一个特定的类别，在memory bank中存储大量的类内和类间负样本。然后，对网络进行训练，使其从大量的负样本中对比区分目标对象，从而提升判别能力。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/50-IAT/20211124162015.png" alt=""></p>
<p>作者提出了两种不同级别的实例感知，视频级和对象级。前者将整个视频看成一个样本，从全局的视角提高了从背景中识别目标的能力；后者将每个目标看成一个实例，强化了目标和干扰物的区分能力。提出的方法仅在训练过程中使用，因此不会增加推理时间，作者在PrDiMP上进行实验，速度可以达到30 FPS。</p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/50-IAT/20211125165645.png" alt=""></p>
<p>整体框架如图2所示，包括特征提取、分类分支、回归分支和实例分类分支。整个上半部分就是PrDiMP的结构，增加的实例分类分支将每个目标与大量其他样本进行分类，以提高跟踪器的判别力。下面对实例分类分支进行详细介绍。</p>
<h2 id="Instance-Classification-Branch"><a href="#Instance-Classification-Branch" class="headerlink" title="Instance Classification Branch"></a>Instance Classification Branch</h2><p>为了引入大量负样本，作者借鉴了自监督学习方法MoCo，将该分支定义为字典查找任务，将来自同一视频的帧作为正样本，将来自其他视频的帧作为负样本。如图2所示，该分支包括memory encoder $\omega$ 和instance boosting module $\psi$，其中$\omega$的网络<strong>结构</strong>和f1/f2相同用于提取字典中的特征，$\psi$调整来自$\omega$和f1的特征用于后续对比学习。</p>
<p>给定query q和字典$\{k_+, \{k_i\}^K_{i=1} \}$，字典中包含一个正样本$k_+$和K个负样本，对比学习的任务是从字典中查找到对应q的正样本$k_+$。在本文的跟踪任务中，q为模板特征，$k_+$为搜索特征，而$\{k_i\}^K_{i=1}$是来自其他视频的历史搜索特征。训练时，给定模板帧$I_t$和搜索帧$I_s$，实例分类分支的前向传播如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/50-IAT/20211125095342.png" alt=""></p>
<p>而字典是一个大的队列，每次迭代都会将当前的$k_+$送入队列，把历史最早的特征移出队列，这样无需重复计算就能得到大量历史搜索特征作为负样本，这其中的负样本同时包含了类内和类间的负样本。memory encoder $\omega$ 采用动量更新的方式逐步逼近query的encoder f1/f2：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/50-IAT/20211125103154.png" alt=""></p>
<p>样本query和key的相似性通过点积来衡量，最终的对比损失类似InfoNCE：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/50-IAT/20211125103535.png" alt=""></p>
<h2 id="Two-Variants-of-IAT"><a href="#Two-Variants-of-IAT" class="headerlink" title="Two Variants of IAT"></a>Two Variants of IAT</h2><p>上面介绍的对比学习框架中的loss是比较不同实例之间的相似性，但输入是完整的图片，因此下面我们来看如何将输入的图片转化成实例的概念，即instance boosting module $\psi$。作者提出了两种不同粒度的实例的概念，包括视频级别的实例(IAT-V)和对象级别的实例(IAT-O)，如图3所示。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/50-IAT/20211125112935.png" alt=""></p>
<p>IAT-V将每个视频视为一个实例，它使用了包含目标和背景的整个输入帧的特征，结构如图3左半部分所示。经过一层卷积后全局池化成$1 \times 1$大小，再过两层FC得到了对应的特征向量。IAT-V可以学习视频之间的差异，某一视频中的目标将有机会从其他负样本视频的目标和背景中学习目标和干扰物之间的差异。因此，IAT-V可以从全局的角度提高从包括语义对象和非语义杂波的各种背景中识别目标的能力。</p>
<p>IAT-O将每个目标边界框视为一个实例，如图3右半部分所示。相比IAT-V增加了一个ROI Pooling层来提取目标框内的特征。因此，IAT-O中的所有样本均为语义样本，可以从更具体的角度来区分不同物体之间的特征。</p>
<h2 id="Training-Loss"><a href="#Training-Loss" class="headerlink" title="Training Loss"></a>Training Loss</h2><p>训练损失包含三部分：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/50-IAT/20211125114405.png" alt=""></p>
<p>其中$L_{cls}, L_{reg}$来自PrDiMP的分类和回归损失，$L_{ins}$是公式5的实例分类损失。整个训练流程如算法1所示，和MoCo的流程是一样的。注意该过程只用在训练中，推理时只有分类和回归分支在工作。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/50-IAT/20211125115134.png" alt=""></p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>实验选择了8个数据集进行验证：OTB50, OTB100, GOT-10k, LaSOT, NFS, UAV123, TrackingNet, VOT2019。负样本数量K=1000，数量远超DaSiamRPN和DiMP中的负样本个数。</p>
<h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/50-IAT/20211125115114.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/50-IAT/20211125115232.png" alt=""></p>
<p>表1分析了memory bank中负样本K的数量，可以看到，增加负样本哪怕只有10个都是有益的，差不多到1000性能饱和了，再增加反而加大了区分的难度。</p>
<p>表2分析了IAT-O中不同的ROI池化尺寸，3最佳。</p>
<p>表3对比了IAT-V，IAT-O以及将它们结合的效果。shared表示二者共享同一个instance classification branch并且将得到的特征向量相加，separated表示不共享instance classification branch但同样相加特征向量。这两种结合方式均未带来明显提升，因此作者认为单独使用即可。</p>
<h2 id="SOTA-Comparison"><a href="#SOTA-Comparison" class="headerlink" title="SOTA Comparison"></a>SOTA Comparison</h2><p>在大多数数据集上，IAT-O结果优于IAT-V。可能是该模型更细粒度的区分，进一步提高了克服干扰的能力。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/50-IAT/20211125120055.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/50-IAT/20211125120135.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/50-IAT/20211125120202.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/50-IAT/20211125120217.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/50-IAT/20211125120230.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/50-IAT/20211125120242.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/50-IAT/20211125120254.png" alt=""></p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>arxiv</tag>
        <tag>contrastive learning</tag>
      </tags>
  </entry>
  <entry>
    <title>轻量化目标跟踪</title>
    <url>/tracking/51-lighttrack/</url>
    <content><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>近些年的目标跟踪算法都在往做大做强的方向发展，比如更深的网络和更复杂的模块。尽管性能越刷越高，但是却很少考虑效率问题，以至于几乎无法在边缘设备上实时运行部署，实用性较低，因此研究轻量化的目标跟踪算法是非常必要的（另外一个原因也可能是做大做强上能水论文的点越来越不好找了 /狗头保命）。本篇博客总结了三篇最近研究跟踪模型轻量化的工作。</p>
<ul>
<li><a href="https://arxiv.org/abs/2104.14545">LightTrack: Finding Lightweight Neural Networks for Object Tracking via One-Shot Architecture Search</a></li>
<li><a href="https://arxiv.org/abs/2112.09686">Efficient Visual Tracking with Exemplar Transformers</a></li>
<li><a href="https://arxiv.org/abs/2112.07957">FEAR: Fast, Efficient, Accurate and Robust Visual Tracker</a></li>
</ul>
<span id="more"></span>
<h1 id="LightTrack"><a href="#LightTrack" class="headerlink" title="LightTrack"></a>LightTrack</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222211440.png" alt=""></p>
<p>详细解读：<a href="https://www.bilibili.com/video/BV1r54y1V7EC?from=search&amp;seid=5643544047018487388&amp;spm_id_from=333.337.0.0">【极市直播】严彬：CVPR 2021-LightTrack：基于网络结构搜索的超轻量级跟踪模型设计</a></p>
<p>LightTrack使用神经架构搜索（NAS）来设计更轻量级和高效的目标追踪器。实验表明，LightTrack与手工设计的 SOTA 跟踪器（如SiamRPN++和Ocean）相比，可以实现更优越的性能，而需要的计算量和参数要少得多。此外，当部署在资源受限的移动芯片上时，也能以更快的速度运行。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222114947.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222115004.png" alt=""></p>
<p>LightTrack采用one-shot NAS的方法搜索结构，流程如图2所示。整个过程训练与搜索是解耦的，首先训练超网（随机采样路径进行训练），然后用进化算法从超网中寻找最优子结构。</p>
<p>最后搜索到的LightTrack-Mobile结构如图3所示，具有如下特点：</p>
<ol>
<li>backbone中有将近一半使用$7 \times 7$的卷积核大小，可能是因为这样能在较浅的backbone中尽量提升感受野；</li>
<li>搜索架构选择了倒数第二个block作为特征输出，可能是因为跟踪网络并不倾向太高级的语义特征；</li>
<li>分类分支需要的网络层数比回归分支要少，可能是因为粗目标定位比精确的边框回归更容易。</li>
</ol>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222120151.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222120211.png" alt=""><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222120235.png" alt=""></p>
<p>实验可以看到三个版本mobile，largeA，largeB在性能、计算量和参数量上都具有优势。在骁龙845中，LightTrack运行速度比Ocean快12倍，参数量减少13倍，计算量减少38倍。作者称这种改进可能会缩小学术模型和工业部署在物体跟踪任务中的差距。</p>
<h1 id="Exemplar-Transformer"><a href="#Exemplar-Transformer" class="headerlink" title="Exemplar Transformer"></a>Exemplar Transformer</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222211457.png" alt=""></p>
<p>本文对transformer架构进行轻量化，提出了一种高效的Exemplar Transformer来替代卷积。E.T.Track在CPU上速度达到47FPS，比其他基于transformer的跟踪器快8倍，作者称这是目前唯一的实时transformer-based的跟踪器。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222213402.png" alt=""></p>
<h2 id="Exemplar-Transformers"><a href="#Exemplar-Transformers" class="headerlink" title="Exemplar Transformers"></a>Exemplar Transformers</h2><p>transformer中self-attention计算如公式2：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222160932.png" alt=""></p>
<p>Q,K的序列长度均为图像尺寸，公式2计算复杂度为图像尺寸的平方，这样带来较大计算负担。作者认为，对所有特征之间的关联在机器翻译中是必要的，但是在视觉任务中是不必要的。因为机器翻译中每个特征都代表一个特定的单词或标记，而视觉任务中相邻的空间通常表示相同的物体。因此在视觉任务中，可以减少特征向量的数量，构建一个更粗略更具描述性的视觉表达，从而显著降低计算复杂度。</p>
<p>作者首先提出了两个假设：</p>
<ol>
<li>一个小的exemplar value集合可以在一个数据集之间共享；</li>
<li>一个粗略的查询具有足够的描述性来利用这些exemplar value。</li>
</ol>
<p>为此，作者在构建query时首先将输入特征图$X \in \mathbb{R}^{H \times W \times C}$通过平均池化压缩成空间维度为S的大小，再经过线性映射得到Q。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222203001.png" alt=""></p>
<p>其中S=1，即将query映射成一个向量，这样可以使效率最大化。作者认为对于单目标跟踪，一个查询就足够了。</p>
<p>对于Key，作者学习了一小组捕获数据集信息的范例表示，而不是一个细粒度的特征映射和仅仅依赖于样本内部的关系。 Exemplar keys可以表示成$K=\hat{W}_K \in \mathbb{R}^{E \times D} $，数量从HW降为E。这个Key与输入特征是无关的，是从整个训练数据集中学习出来的一个变量。</p>
<p>对于Value，采用卷积操作在局部层面进行操作。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222203750.png" alt=""></p>
<p>其中$W_V \in \mathbb{R}^{E \times K \times K}$。整个exemplar attention可以表示为：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222203939.png" alt=""></p>
<p>exemplar attention和传统的self-attention对比如图2和3所示，本文方法利用卷积处理局部特征，利用相似性度量处理全局特征。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222204310.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222204324.png" alt=""></p>
<p>计算复杂度分析如表1所示，其中Key的数量E=4，所以Exemplar Attention的计算量和卷积是同一个量级的。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222210355.png" alt=""></p>
<h2 id="E-T-Track-Architecture"><a href="#E-T-Track-Architecture" class="headerlink" title="E.T.Track Architecture"></a>E.T.Track Architecture</h2><p>上述提出的Exemplar Transformer layer可以作为卷积的替代，作者将LightTrack的预测头分支所有卷积换成了Exemplar Transformer，构建新的跟踪器E.T.Track如图4所示。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222211042.png" alt=""></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222211237.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222211248.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222211303.png" alt=""></p>
<h1 id="FEAR"><a href="#FEAR" class="headerlink" title="FEAR"></a>FEAR</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222213350.png" alt=""></p>
<p>本文的目的是设计快速、高效、准确、鲁棒的跟踪器，提出两个轻量化模型，dual-template module和pixel-wise fusion block。前者使用一个可学习的参数集成了时域信息，而后者使用更少的参数编码了更有判别性的特征。使用复杂的backbone，本文方法FEAR-M和FEAR-L在速度和精度上超过大多数算法；而使用轻量backbone的版本FEAR-XS比目前的Siamese跟踪器快10倍以上的跟踪速度，同时保持接近的精度。FEAR-XS比LightTrack小2.4倍，快4.3倍，且具有更高的精度。此外，本文引入能耗和速度来扩展模型效率的定义。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222214236.png" alt=""></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222214256.png" alt=""></p>
<p>整体方法如图2所示，跟经典siamese方法的差别在于输入增加了一个动态模板，将静态和动态模板进行线性插值后再与搜索特征进行融合。</p>
<p>特征提取部分使用轻量的FBNet，特征融合部分设计了像素级别的融合，如图3所示，这个和PGNet，CGACD等方法的操作是一样的。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222214656.png" alt=""></p>
<p><strong>Dynamic Template Update</strong></p>
<p>初始模板$F_T$和动态模板$F_d$通过可学习的参数$\omega$进行线性融合。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222215132.png" alt=""></p>
<p>动态模板的选择如图4所示</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222215336.png" alt=""></p>
<p>将搜索特征与分类分数相乘后池化得到向量$e_s$，动态模板进行池化得到向量$e_t$，计算二者的余弦相似度。推理阶段从每N个历史搜索帧中选择相似度最大的帧裁剪更新动态模板。训练时还额外增加了负样本$e_T$构建三元损失。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222220117.png" alt=""></p>
<h2 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222220456.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222220504.png" alt=""></p>
<p>实验除了常规的benchmark性能测试还专门做了效率分析。作者引入了FEAR Benchmark来评估跟踪算法对移动设备电池和热状态的影响，以及随着时间的推移对处理速度的影响。如图5所示，随着运行时间的增加，其他算法均出现了速度下降、电量下降，温度升高的现象，但本文的FEAR-XS在这些指标上均能保持稳定（高速、耗电少、温度低）。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222220712.png" alt=""></p>
<p>速度上在多款手机处理器上均大幅超过了LightTrack。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/51-lighttrack/20211222220858.png" alt=""></p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>综述</tag>
        <tag>轻量化</tag>
      </tags>
  </entry>
  <entry>
    <title>SBT: Correlation-Aware Deep Tracking</title>
    <url>/tracking/53-SBT/</url>
    <content><![CDATA[<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524105019.png" alt=""></p>
<p>Correlation-Aware Deep Tracking <a href="https://arxiv.org/abs/2203.01666">论文</a></p>
<p>写在开头：本文的写作值得学习，实验极其详尽。本篇博文按照作者的写作思路过一遍摘要和引言，对于我们大多数人写文章按照这个套路都没什么问题。</p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>切入点：鲁棒性和判别性都很重要</p>
<p>现有问题：孪生网络无法判别性的建模目标和干扰</p>
<p>提出新方法：target-dependent feature network </p>
<p>做法：通过attention，将跨图像的特征相关性嵌入特征网络的多个层中。</p>
<p>好处：</p>
<ol>
<li>在多个层进行匹配，压制非目标特征，得到实例感知的特征提取；</li>
<li>输出的搜索特征可以直接用于预测定位，无需互相关操作；</li>
<li>可以在大量不成对数据上预训练，加速收敛</li>
</ol>
<span id="more"></span>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p><strong>指出跟踪中两个竞争的需求：</strong>1. 在剧烈形变下识别目标（鲁棒性）；2. 区分目标和背景干扰物（判别性）</p>
<p><strong>当前方法：</strong>提取更具有表达力的Siamese特征；设计更鲁棒的相关操作。</p>
<p><strong>当前方法的问题：</strong>很少注意到两个竞争的需求会使得网络陷入目标-干扰物困境，主要体现在三个方面</p>
<ol>
<li><p>Siamese特征编码过程无法感知template和search中的的实例级信息</p>
</li>
<li><p>backbone没有明确建模区分两个竞争的决策边界，陷入次优结果</p>
</li>
<li><p>每个训练视频只标注一个目标，但是跟踪过程中为包含干扰物的任意目标。这个差距进一步扩大了问题2</p>
</li>
</ol>
<p><strong>本文观点：</strong>特征提取应该具有动态感知实例变化的能力，为跟踪生成适当的embeddings。具体包括：为视频中的同一对象生成连贯的特征，以及对具有相似外观的目标和干扰物生成对比特征</p>
<p><strong>本文提出方法：</strong></p>
<p>一个基于注意力的动态特征网络 Single Branch Transformer (SBT) network，允许模板和搜索图像的特征在特征提取的每个阶段进行深度交互。</p>
<p>如图2所示，传统的孪生网络堆叠卷积，最后接一个互相关层进行特征交互。而SBT堆叠注意力模块Extract-or-Correlation (EoC) blocks，其中EoC-SA融合同一图像的特征，EoC-CA交互模板和搜索图的特征。最后将搜索图像特征直接输出给预测头。</p>
<p>直观地，交叉注意力<strong>逐层</strong>过滤掉与目标无关的特征，自注意力增强了目标的特征表示。由于特征提取过程是target-dependent且非对称的，因此SBT将目标和干扰物进行区分的同时保持不同目标之间的连贯特征。如图2d所示，属于目标的特征(绿色)与背景(粉色)和干扰物(蓝色)越来越分离，而孪生网络提取的搜索特征是target-unaware的。</p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524111925.png" alt=""></p>
<p><strong>关键创新</strong></p>
<p>为模板和搜索图像对的处理引入一个单一流，通过注意力模块联合特征提取和特征关联。</p>
<p>SBT本质上还是一个特征提取的backbone，因此可以在ImageNet等非成对数据上进行预训练，在跟踪数据中微调快速收敛。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524112053.png" alt=""></p>
<p>整体结构如图3所示，包含三个stage。前两个stage通过patch embedding降维，后面接自注意力EoC-SA；第三个satge交替使用自注意力EoC-SA和交叉注意力EoC-CA。该框架没有额外的互相关层，通过在在网络的不同阶段使用交叉注意力实现了信息交互融合。最后将融合后的搜索特征直接输入预测头做分类和回归。</p>
<h2 id="Patch-Embedding"><a href="#Patch-Embedding" class="headerlink" title="Patch Embedding"></a>Patch Embedding</h2><p>使用$7 \times 7$步长为4的卷积+LN层，降低空间分辨率。</p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524112939.png" alt=""></p>
<h2 id="Extract-or-Correlation-Block"><a href="#Extract-or-Correlation-Block" class="headerlink" title="Extract-or-Correlation Block"></a>Extract-or-Correlation Block</h2><p>EoC模块如图3b，c所示，包括两个LN层一个MLP层以及一个注意力，注意力包含自注意力(SA)和交叉注意力(CA).。SA输入的ij是同源的，CA输入的ij分别来自模板z和搜索图x。</p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524114016.png" alt=""></p>
<p>作者为了计算高效，尝试了 Vanilla Global attention (VG)；Spatial-Reduction Global attention(SRG)，对k，v降分辨率；Vanilla Local window attention (VL) ，计算局部块的注意力；Shift window vanilla Local attention (SL)，即Swin Transformer。最后使用SRG。</p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524114738.png" alt=""></p>
<center>Spatial-Reduction Global attention(SRG)</center>

<h2 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h2><p>尝试了绝对/相对位置编码，和CVPT中的条件位置编码，即一个$3 \times 3$的depth-wise卷积。</p>
<h2 id="Direct-Prediction"><a href="#Direct-Prediction" class="headerlink" title="Direct Prediction"></a>Direct Prediction</h2><p>分类和回归分支均由两个MLP层构成，分别在空间维度$\varphi_{sp}$和通道维度$\varphi_{cn}$做MLP，其中RS表示reshape。</p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524115757.png" alt=""></p>
<h1 id="SBT设计准则"><a href="#SBT设计准则" class="headerlink" title="SBT设计准则"></a>SBT设计准则</h1><p>作者做了大量实验来探究如何设计网络框架，每张实验图里都包含大量信息，微软有卡任性。这里只总结结论，具体实验可以参考原文。</p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524120305.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524120323.png" alt=""></p>
<ol>
<li>多尺度分层结构的性能比单级结构好 （表1 setting A）</li>
<li>不同位置编码影响不大 （表1 setting A）</li>
<li>卷积得到的patch embedding比手工划分的更具表现力 （表1 setting A）</li>
<li>全局注意力SRG效果和效率综合最佳 （表1 setting A）</li>
<li>EoC-CA越多一定范围上可以提升性能  （图4d）</li>
<li>越早使用EoC-CA生成与目标相关的特征越好，但是放在太前面如stage1，2的计算开销较大，而带来的提升较小（图4d，b）</li>
<li>交替使用EoC-SA和EoC-CA （图4f）</li>
<li>浅层stage1和2不要设置过多参数和计算量（表1 setting B）</li>
<li>网络步长一定，3个stage比4个stage好（表1 setting B）</li>
<li>空间分辨率越大，性能越高，但计算量大（表1 setting B）</li>
<li>平衡block的数量和channel维度（表1 setting B）</li>
<li>SBT可以通过预训练中受益，比基于transformer的跟踪器（如transt，stark）收敛得更快（图4e）</li>
</ol>
<h1 id="SBT跟踪的理论分析"><a href="#SBT跟踪的理论分析" class="headerlink" title="SBT跟踪的理论分析"></a>SBT跟踪的理论分析</h1><ul>
<li>SBT克服了孪生跟踪器平移不变性的限制</li>
</ul>
<p>padding破坏平移不变性，而对于SBT，padding只存在于PaE中，attention没有引入padding。此外，attention的token本身就具有排列不变性，即目标在图像上移动多少，则其所在的token也移动同样的距离。</p>
<ul>
<li>交叉注意力相当于执行了两次互相关</li>
</ul>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524163539.png" alt=""></p>
<ul>
<li>在串行管道中嵌入了分层特征利用</li>
</ul>
<p>经典的孪生跟踪器如SiamRPN++是并行的的分层聚合，即对多个特征分别做互相关再叠加。SBT探索了串行的多层次的特征相关性，即将前一层相关（交叉注意力）的输出送给下一次相关的输入。</p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524164926.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524164541.png" alt=""></p>
<h1 id="4种SBT网络"><a href="#4种SBT网络" class="headerlink" title="4种SBT网络"></a>4种SBT网络</h1><p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524165005.png" alt=""></p>
<p>Imagenet预训练用4个stage，跟踪微调只用前3个stage。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="Comparison-to-State-of-the-Art-Trackers"><a href="#Comparison-to-State-of-the-Art-Trackers" class="headerlink" title="Comparison to State-of-the-Art Trackers"></a>Comparison to State-of-the-Art Trackers</h2><p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524165305.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524165320.png" alt=""></p>
<p>作者将本文提出的Correlation-Aware backbone 用在几个主流的跟踪器上，得到SiamFCpp-CA. DiMP-CA.STARK-CA. STM-CA，观察性能提升巨大，甚至部分计算量还下降了。</p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524165514.png" alt=""></p>
<h2 id="Exploration-Study"><a href="#Exploration-Study" class="headerlink" title="Exploration Study"></a>Exploration Study</h2><p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524165758.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524170547.png" alt=""></p>
<p><strong>Correlation-embedded structure</strong> 将特征交互嵌入特征提取中的效果比单独在最后使用互相关效果要好。并且本文使用的交叉注意力的交互形式比DCF和DW-Corr要好。</p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524170333.png" alt=""></p>
<p><strong>Target-dependent feature embedding</strong> (a) Correlation-embedded tracker. (b) Siamese correlation with SBT. (c) Siamese correlation with ResNet-50 </p>
<p>本文提出的方法最具有 target-dependent的性质，即目标和干扰分的最开。</p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524170608.png" alt=""></p>
<p><strong>Benefits from pre-training</strong> 本文的方法可以预训练，提升性能加速收敛。</p>
<p>补充材料里还有一些关于计算量和结构设计方面的实验，就不一一介绍了。作者团队是真的有耐心做了这么多对比实验。</p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524171043.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524171151.png" alt=""></p>
<p>最后提下本文方法的局限性，就是对严重遮挡或者出视野不友好，这也是所有孪生跟踪器的通病了。虽然attention的逐点匹配一定程度上可以克服遮挡，但是对于图13中这些有相似属性的遮挡物还是存在很多误判。另一个局限在于本文使用了很多快速注意力，这些并不能直接通过直接调包实现。</p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/53-SBT/20220524171328.png" alt=""></p>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>本文首次提出了一种target-dependent的跟踪特征网络。SBT大大简化了跟踪管道，比最近基于Transformer的跟踪器收敛更快。从实验和理论两方面对SBT跟踪进行了系统的研究。此外，实现了四个版本的SBT网络，可以改进其他流行的VOT和VOS跟踪器。大量实验表明，该方法取得了良好的效果，可以作为动态特征网络应用于其他跟踪管道。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>transform</tag>
        <tag>CVPR2022</tag>
      </tags>
  </entry>
  <entry>
    <title>MixFormer: End-to-End Tracking with Iterative Mixed Attention</title>
    <url>/tracking/54-mixformer/</url>
    <content><![CDATA[<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/54-MixFormer/20220524172159.png" alt=""></p>
<p>MixFormer: End-to-End Tracking with Iterative Mixed Attention <a href="https://arxiv.org/abs/2203.11082">论文</a> <a href="https://github.com/MCG-NJU/MixFormer">代码</a></p>
<p>核心：用transfrom架构整合特征提取和特征融合</p>
<span id="more"></span>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/54-MixFormer/20220524172748.png" alt=""></p>
<p>主流的跟踪框架分三步：特征提取、特征融合、预测头分类回归</p>
<p>其中特征融合是关键，下图展示了不同的融合方法。（摘自 <a href="https://www.bilibili.com/video/BV1L541127MD?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click">【VALSE论文速览-68期】MixFormer:更加简洁的端到端单目标跟踪器</a>）</p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/54-MixFormer/20220524172920.png" alt=""></p>
<p>最近的研究使用transformer进行融合，但仍然依赖CNN提取特征，这其中存在一些局限：</p>
<ol>
<li>attention只作用在高层抽象的特征表示空间，忽略了浅层特征；</li>
<li>CNN对通用对象识别进行预训练，可能会忽略用于跟踪的更精细的结构信息；</li>
<li>CNN的表征能力是局部的，缺乏长距离建模的能力</li>
</ol>
<p>解决方案：</p>
<p>提出一个通用的transformer结构同时进行特征提取和特征融合。</p>
<p>具有如下好处:</p>
<ol>
<li>使特征提取更具体到相应的跟踪目标，并捕获更多目标特定的判别特征;</li>
<li>让目标信息更extensive的融合进搜索区域；</li>
<li>结构更加紧凑简洁。</li>
</ol>
<p>主要创新点：</p>
<ul>
<li>提出了 MAM 模块，应用 attnetion 机制同时进行特征提取与信息交互</li>
<li>提出SPM模块进行模板更新</li>
</ul>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/54-MixFormer/20220524213439.png" alt=""></p>
<p>网络整体框架如图3所示，包括两部分：模板和搜索图像经过基于mixed attention module(MAM)的backbone进行特征提取和融合，再通过预测头输出结果。backbone部分包含3个stage，每个stage输入特征首先经过patch embedding变成一系列token，然后送入MAM模块提取并融合特征。预测头部分直接将融合后的搜索区域的token输入进行预测。</p>
<h2 id="Mixed-Attention-Module-MAM"><a href="#Mixed-Attention-Module-MAM" class="headerlink" title="Mixed Attention Module (MAM)"></a>Mixed Attention Module (MAM)</h2><p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/54-MixFormer/20220524220836.png" alt=""></p>
<p>本文的核心模块MAM，目的是同时提取并融合模板和搜索图像的特征，因此设计了dual attention分别用于二者。具体来说，MAM输入模板和搜索特征拼接成的 token序列，首先会将输入分开并reshape 成二维的模板和搜索特征，经过$3\times3$ DW卷积编码局部上下文和线性映射生成q，k，v后，同时进行 self-attention和 cross-attention。</p>
<p>注意MAM 是一个非对称的attention，删去了target-to-search的cross-attention。如图2所示，模板的q只会和模板自己的k，v计算attention（黄色虚线）；而搜索图的q会同时和模板和搜索图的k，v计算attention（蓝色虚线）。用公式表达为：</p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/54-MixFormer/20220524222647.png" alt=""></p>
<p>这样做可以使得模板的token在跟踪过程中保持不变，避免被动态的搜索区域影响。为后续引入多个在线模板做铺垫，无需每帧重新计算模板token。</p>
<h2 id="Localization-Head"><a href="#Localization-Head" class="headerlink" title="Localization Head"></a>Localization Head</h2><p>采用类似stark的角点预测模式。作者也额外尝试了类似detr的采用一个query进行预测的方式。均无需后处理。</p>
<h2 id="Template-Online-Update"><a href="#Template-Online-Update" class="headerlink" title="Template Online Update"></a>Template Online Update</h2><p>在线更新模板能够很好的利用时序信息处理一些形变和外观变化，然而低质量的更新模板可能使得结果变差。本文设计了一个score prediction module (SPM)，根据预测置信度得分来选择可靠的在线模板，如图4所示。</p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/54-MixFormer/20220524224125.png" alt=""></p>
<p>SPM由两个attention和一个三层的MLP组成，该模块接在backbone最后一个stage后，和预测头是并行的。首先输入一个可学习的score token，与search ROI token计算attention，对搜索图中挖掘的目标信息进行编码。然后将score token与第一帧的模板token做attention，隐式地将挖掘的目标与初始目标进行比较。最后过一个MLP预测出置信度得分，小于0.5判断为不可靠。</p>
<h2 id="Training-and-Inference"><a href="#Training-and-Inference" class="headerlink" title="Training and Inference"></a>Training and Inference</h2><p>作者设计了两种网络架构MixFormer 和 MixFormer-L ，分别基于CVT-21 和 CVT24-W，也就是说可以使用CVT在Imagenet上预训练的权重来初始化backbone（虽然原始的CVT并没有两个输入，计算attention的方式也不一样，但是每个block的参数是一样的）。</p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/54-MixFormer/20220524225610.png" alt=""></p>
<p>训练过程分为两步，首先用500个epoch训练backbone和head；最后用40个epoch单独训练SPM，冻结其他部分参数。这个训练流程和stark类似。</p>
<p>推理阶段每隔200帧更新一次模板，选择区间中得分最高的模板替换先前的模板。本文的框架允许输入任意张数的模板，代码实现中只包含两张模板，一张初始模板，一张在线更新模板。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="SOTA比较"><a href="#SOTA比较" class="headerlink" title="SOTA比较"></a>SOTA比较</h2><p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/54-MixFormer/20220524234306.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/54-MixFormer/20220524234318.png" alt=""></p>
<p>SOTA性能就一个字：恐怖！</p>
<h2 id="探究实验"><a href="#探究实验" class="headerlink" title="探究实验"></a>探究实验</h2><p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/54-MixFormer/20220524234504.png" alt=""></p>
<ul>
<li><h1 id="1-2-3-8-统一特征提取和融合的MAM比先提特征SAM再融合CAM要好，因为耦合的方式可以互相促进。"><a href="#1-2-3-8-统一特征提取和融合的MAM比先提特征SAM再融合CAM要好，因为耦合的方式可以互相促进。" class="headerlink" title="1 #2 #3 #8 统一特征提取和融合的MAM比先提特征SAM再融合CAM要好，因为耦合的方式可以互相促进。"></a>1 #2 #3 #8 统一特征提取和融合的MAM比先提特征SAM再融合CAM要好，因为耦合的方式可以互相促进。</h1></li>
<li><h1 id="4-5-6-7-8-MAM的数量越多越好，因为这样可以获得更extensive-的目标感知特征提取和分层融合。"><a href="#4-5-6-7-8-MAM的数量越多越好，因为这样可以获得更extensive-的目标感知特征提取和分层融合。" class="headerlink" title="4 #5 #6 #7 #8 MAM的数量越多越好，因为这样可以获得更extensive 的目标感知特征提取和分层融合。"></a>4 #5 #6 #7 #8 MAM的数量越多越好，因为这样可以获得更extensive 的目标感知特征提取和分层融合。</h1></li>
<li><h1 id="8-9-corner-head比query-head效果更好"><a href="#8-9-corner-head比query-head效果更好" class="headerlink" title="8 #9 corner head比query head效果更好"></a>8 #9 corner head比query head效果更好</h1></li>
</ul>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/54-MixFormer/20220524235111.png" alt=""></p>
<ul>
<li>使用非堆成结构效果略有下降，但是速度提升了</li>
<li>从固定间隔中随机采样更新模板效果变差了，加上预测得分后才能提升效果</li>
<li>pretrain的规模越大，对效果也是有提升的。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/kongbia/picgo/master/blog/54-MixFormer/20220524235732.png" alt=""></p>
<p>Attention可视化</p>
<ul>
<li>背景中的干扰物逐层受到抑制</li>
<li>在线模板更适应外观变化并有助于区分目标</li>
<li>多个模板的前景可以通过交叉注意力来增强</li>
<li>某个位置倾向于与周围的局部块相互作用。</li>
</ul>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>transform</tag>
        <tag>CVPR2022</tag>
      </tags>
  </entry>
  <entry>
    <title>相关滤波和孪生网络目标跟踪综述（Martin团队）</title>
    <url>/tracking/52-survey/</url>
    <content><![CDATA[<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211222231118.png" alt=""></p>
<p>Visual Object Tracking with Discriminative Filters and Siamese Networks: A Survey and Outlook 马丁团队的新作，强烈推荐。</p>
<p><a href="https://arxiv.org/abs/2112.02838">论文</a></p>
<p>精确和鲁棒的视觉目标跟踪是计算机视觉中最具挑战性和最基本的问题之一。它需要在只给定目标初始状态的条件下，准确估计图像序列中目标的轨迹及状态。相关滤波和孪生网络已经成为当下的主流跟踪算法，本文选取了90多个DCF和Siamese跟踪器进行系统和全面的回顾。首先，介绍了DCF和Siamese跟踪核心公式的背景理论。然后，区分和全面回顾了这两种跟踪范式中共享的和各自特定的挑战。此外，深入分析了DCF和Siamese跟踪器在9个benchmark上的性能，涵盖了视觉跟踪的不同方面的实验：数据集、评估指标、性能和速度比较。在此分析的基础上，提出了对视觉跟踪开放挑战的建议。</p>
<span id="more"></span>
<h1 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h1><p>这项工作系统的回顾流行的DCF和Siamese的跟踪范式。两种模型都有一个共同的目标，即学习一个准确的目标外观模型，能够有效地从背景中区分出目标物体。尽管在解决上述目的方面出现了不同的潜在范式，但深度学习的出现给这两种范式带来了一些重要的相似之处和共同的挑战。</p>
<p>相同的挑战包括：</p>
<ol>
<li><strong>特征表达</strong> 从预训练网络中提取深度特征表示是两种范式共同的趋势。然而，深度网络架构和特征层次结构的选择仍然是一个开放的问题；</li>
<li><strong>目标状态估计</strong> 两种范式的核心公式只解决了如何估计目标对象的平移，但都没有提供一个显式的方法来估计完整的目标状态（边界框参数）；</li>
<li><strong>离线训练</strong> 最初只有Siamese跟踪器可以端到端离线训练，但最近的DCF也可以利用大规模离线学习，将其与高效、可微的在线学习模块集成，以实现鲁棒和准确的跟踪。</li>
</ol>
<p>各自特定的问题包括：</p>
<ol>
<li><strong>边界效应</strong> DCF通常利用训练样本的周期性循环位移来学习在线分类器，这引入了不良的边界效应，严重降低了目标模型的质量；</li>
<li><strong>优化问题</strong> DCF的损失函数优化是一个挑战，特别是在岭回归中加入了空间或时间正则化等目标约束条件时变得更加困难；</li>
<li><strong>模型在线自适应</strong> 当目标外观发生变化时，模型需要能够应对这些变化。DCF可以通过损失函数更新外观模型，但Siamese跟踪器并没有固有的在线模型更新机制。因此，在线适应性是Siamese跟踪器的一个重要问题。</li>
</ol>
<p>下面将分别介绍这两种跟踪范式的背景理论以及回顾它们针对上述挑战做出的相关工作。图4整体展现了两种跟踪范式的发展过程。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211225112755.png" alt=""></p>
<h1 id="DCF"><a href="#DCF" class="headerlink" title="DCF"></a>DCF</h1><p>DCF是一种有监督的线性回归技术。DCF成功的关键是通过循环移动训练样本实现稠密采样，这允许在学习和应用相关滤波器时使用快速傅里叶变换(FFT)，大幅提升计算效率。通过利用傅里叶变换的特性，DCF在线学习相关滤波器，有效地最小化岭回归误差来定位连续帧中的目标对象。为了估计下一帧的目标位置，将学习的滤波器应用到感兴趣的区域，其中最大响应的位置估计目标位置，然后以迭代的方式更新滤波器。</p>
<h2 id="DCF基本公式"><a href="#DCF基本公式" class="headerlink" title="DCF基本公式"></a>DCF基本公式</h2><p>早期的DCF如MOOSE和CSK均采用单通道的灰度特征，从KCF之后基本都是应用多通道特征，所以这里直接列出多通道的形式，关于单通道的详细推导可以查看原论文。多通道DCF优化目标如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211223103910.png" alt=""></p>
<p>其中$d \in \{1,…,D\}$表示特征维度，$m$是样本个数，$x, y$分别对应训练样本和标签，$\omega$表示是滤波器参数。通过利用FFT转到频域求解：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211223104338.png" alt=""></p>
<p>详细推导可以参考<a href="https://blog.csdn.net/u011285477/article/details/53861850">KCF论文阅读笔记_图像研究猿的专栏</a></p>
<h2 id="DCF跟踪流程"><a href="#DCF跟踪流程" class="headerlink" title="DCF跟踪流程"></a>DCF跟踪流程</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211223104623.png" alt=""></p>
<p>DCF跟踪流程如图2所示，首先在第一帧学习滤波器$\omega$，然后在后续帧进行检测，并更新滤波器参数。第m帧的检测公式计算如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211223105745.png" alt=""></p>
<p>其中$z$为根据上一帧的检测结果裁剪的patch，$\omega_{m-1}$为从初始帧一直递归更新到上一帧的滤波器，二者通过卷积运算得到每个位置的目标分数$s$。其中$s$中响应最大的位置为当前检测的目标位置，并在此位置裁剪新的patch用于更新滤波器参数。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211223114556.png" alt=""></p>
<p>其中$\omega_m^{num}, \omega_m^{den}$分别为分子和分母，即滤波器$\hat{\omega}=\frac{\omega_m^{num}}{\omega_m^{den}}$。</p>
<h2 id="DCF存在的挑战"><a href="#DCF存在的挑战" class="headerlink" title="DCF存在的挑战"></a>DCF存在的挑战</h2><p>DCF框架中存在的问题包括特征表达、边界效应、损失优化和目标状态估计，下面分别进行讨论。</p>
<h3 id="特征表达"><a href="#特征表达" class="headerlink" title="特征表达"></a>特征表达</h3><p><strong>Handcrafted  Features</strong> 手工特征包括：灰度/RGB/LAB等颜色强度特征，Color  Names  (CN)  特征以及HOG特征。</p>
<p><em>由于其速度和有效性，这些特性已经成为手工方法中的首选。此外，HOG特征也被有效地与CN特征结合起来，以利用形状和颜色信息。</em></p>
<p><strong>Deep Features</strong> 随着CNN的发展，许多DCF方法将高维非线性的卷积特征用于跟踪，如HCF、HDT、CCOT、ECO、ASRCF和RPCF等。它们均使用在ImageNet上离线预训练的特征，虽然是用于分类任务的特征，但这种深度表示方法适用于广泛的视觉任务。</p>
<p>一些流行的预训练深度网络，如VGG-19、imagenet-vgg-m-2048、VGG-16、ResNet50和googlenet被用来提取深度特征表示。其中浅层特征包含高分辨率的低层信息，这对精确定位目标非常重要。更深层的特征对复杂的形变具有较高的不变性，可以提高跟踪鲁棒性，同时在很大程度上不受小的平移和尺度变化的影响。因此，在DCF框架中融合浅卷积层和深卷积层的精确策略一直是人们感兴趣的话题。CCOT中提出了DCF框架的连续域公式，实现了多分辨率特征的融合。ECO研究降低CCOT计算成本的策略，并降低过拟合的风险。其他跟踪器如HDT、HCFTs、MCCT、MCPF、MCPF、LMCF、STRCF、TRACA、DRT、UPDT和GFS-DCF使用后期融合策略集成深度特征。该策略是在每个单独的特征表示上训练一个分类器，然后聚合特征响应图。</p>
<p><strong>End-to-End Features Learning</strong> 上述方法依赖离线预训练，而后续研究关注如何在跟踪数据集上端到端优化DCF框架，这样可以学到任务特定的深度特征表示来改善跟踪性能。CFNet以离线方式对相关滤波器进行端到端学习，CREST和ACFN也采用了相同的在线策略。最近，ATOM额外加入了端到端的尺度估计分支，DiMP和PrDiMP则改进了经典的DCF模型，提升了判别能力。</p>
<p><em>最近DCF跟踪器（ATOM, DiMP, PrDiMP）中端到端特征学习的趋势导致了在多个基准上的优秀跟踪性能，为探索DCF范式中更复杂的端到端特征学习铺平了道路。</em></p>
<h3 id="边界效应"><a href="#边界效应" class="headerlink" title="边界效应"></a>边界效应</h3><p>DCF的循环位移操作引入了对跟踪不利的边界效应。具体来说，由于循环位移，训练样本中的负样本并不是真实的背景内容，而是一个较小图像块的不断位移合成的重复。因此，模型在训练过程中看到的背景样本较少，严重限制了其判别能力。此外，由于周期性重复所造成的失真，预测的目标分数只在图像块的中心附近是准确的，搜索区域的大小因此受到限制。尽管可以通过窗口函数相乘来对其进行预处理。然而，这种技术并不试图解决上述问题，而只是为了消除边界区域的不连续性。针对这一问题，许多方法在DCF的目标公式中加入了各种目标特定的空间、时空和平滑约束。</p>
<p><strong>空间正则化</strong> SRDCF提出了一种空间正则化来控制滤波器的空间扩展，以缓解边界问题，公式如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211224163041.png" alt=""></p>
<p>其中空间权重函数$f(n) &gt; 0$，在背景像素处取大值，在目标区域内取小值，对背景滤波系数进行惩罚。这样可以在一个更大的搜索图像上学到一个聚焦于目标区域的紧凑型滤波器。空间正则化策略已被应用于各种跟踪器中，包括ARCF、ASRCF和AutoTrack。为了提高SRDCF的效率，Li等人提出了STRCF，它只使用单一的训练样本，而引入了时间正则化项来整合历史信息。</p>
<p><strong>约束优化</strong> SRDCF的目的是惩罚目标区域外的滤波系数，而Kiani等人提出引入硬约束（BACF）。该策略强制滤波系数$\omega$在目标区域外为零。优化公式可以通过引入二进制掩码P来表示：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211224165451.png" alt=""></p>
<p>其中$P \cdot \omega$ 掩盖了背景特征对于滤波器参数的影响，目标函数采用ADMM迭代优化。公式(13)和(14)中所研究的特定于目标的约束通常针对不同的对象是固定的，并且在跟踪过程中不会发生变化。最近，Dai等人通过引入自适应正则化项来扩展BACF和SRDCF。</p>
<p><strong>隐式方法</strong> GFS-DCF提出了一种联合特征选择模型，该模型同时学习三个正则化项:空间正则化用于特征选择，通道正则化用于特征通道选择，低秩时间正则化项用于增强滤波权值的平滑性。Mueller等人提出CACF对每个目标patch的上下文信息进行正则化。在每一帧中，CACF对几个上下文补丁进行采样，作为负样本。</p>
<p><strong>空域形式</strong> 最近，ATOM和DiMP采用低分辨率（16倍降采样）的深度特征，可以以小尺寸卷积核（$4 \times 4$）的形式在空间域中直接学习滤波器。这种方法完全绕开了边界效应，因为不需要周期性地扩展训练样本。</p>
<p><em>基于正则化的(SRDCF， STRCF)和基于约束的(CFLB/BACF)方法都取得了巨大成功，并被广泛应用于跟踪器中。最近的深度学习方法(ATOM/DiMP)通过直接在空域中优化滤波器，已经完全避开了边界效应问题。因此，虽然傅里叶域对高分辨率特征的计算具有吸引力，但在使用强大的低分辨率深度特征时，高效的空域优化方法在在线学习中占优。因此，目前基于DCF的SOTA方法研究采用了空域优化的形式，不需要额外的策略来缓解边界效应。</em></p>
<h3 id="优化问题"><a href="#优化问题" class="headerlink" title="优化问题"></a>优化问题</h3><p>标准的DCF优化利用循环矩阵在傅里叶域可对角化的性质来求解岭回归的闭式解。这套方法无法处理有额外正则项或约束条件的优化问题，下面总结几种有效的模型优化方法。</p>
<p><strong>Gauss-Seidel Method</strong> 代表方法：DeepSRDCF，采用Gauss-Seidel迭代速度非常慢，大概只有每秒几帧。</p>
<p><strong>Conjugate Gradient Based Method</strong> 共轭梯度最早用于CCOT，可作用于任意一组满秩的正规方程$A\tilde{\omega}=b$。它通过寻找共轭方向$p^{i}$和最优步长$\beta^i$ 来更新滤波器$\tilde{\omega}^i=\tilde{\omega}^{i-1}+\beta^ip^{i}$。该算法在有限次迭代次数中收敛到解，但在实际应用中，算法在固定次数的迭代后或当误差降低到令人满意的水平时停止。CG在处理D维特征时，复杂度从Gauss-Seidel的二次复杂度$O(D^2)$降为线性的$O(D)$，因此可以用于高维的深度特征优化。</p>
<p>针对非线性最小二乘问题，Gauss-Newton法也被用于许多跟踪器，包括ECO、ATOM和UPDT。该方法对误差使用泰勒级数展开，找到一个二次逼近的目标。由此产生的二次问题可以用迭代方法处理，例如上面描述的CG方法。在ECO和ATOM中，采用Gauss-Newton结合CG对滤波器$\omega$和降维矩阵进行联合优化。DiMP使用Gauss-Newton和Steepest Descent迭代来学习滤波器。优化步骤本身是可微分的，这进一步支持端到端的学习。PrDiMP进一步使用更一般的牛顿近似来处理凸且非线性KL散度目标函数。</p>
<p><strong>Alternating Direction Method of Multipliers (ADMM) Method</strong> ADMM方法近年来被广泛应用于DCF中，特别是引入额外正则项的优化上。ADMM将大的全局问题分解为多个较小、较容易求解的局部子问题，并通过协调子问题的解而得到大的全局问题的解。基于ADMM的优化方法为每个子问题提供了闭式解，并且在非常少的迭代内收敛。BACF、DRT、AutoTrack、ARCF和RPCF等跟踪器都采用ADMM来实现高效的求解。</p>
<h3 id="目标状态估计"><a href="#目标状态估计" class="headerlink" title="目标状态估计"></a>目标状态估计</h3><p><strong>Multiple Resolution Scale Search Method</strong> 多尺度金字塔搜索是最简单粗暴的方法，首先按不同的比例因子调整图像大小，然后在每个尺度进行估计，选择分数最大的位置和尺度作为最终结果。在一定程度上可以提升尺度估计的准确性，但是多尺度采样带来较大的计算负担，且只能等比例缩放边界框。</p>
<p><strong>Discriminative Scale Space Search Method</strong> 即DSST为代表的方法，将目标状态估计分两步进行。由于两帧之间的尺度变化通常较小或中等，首先通过在当前的尺度估计上应用通常的平移滤波器来找到目标平移。然后，在尺度维度上应用单独的一维滤波器来更新目标尺寸。好处有两方面：1）通过减小搜索空间来提高计算效率；2）对尺度滤波器进行训练，区分不同尺度下目标的外观，从而得到更准确的估计。后续的fDSST跟踪器通过应用PCA和子网格插值[27]降低了DSST的计算开销。</p>
<p><strong>Deep Bounding Box Regression Method</strong> 上述方法依赖于比例因子参数和在线精确相关滤波器响应，没有以离线方式利用强大的深度特征表示。此外，这些在线方法不执行任何边界框回归。因此，这些方法在尺度突然变化的情况下表现出性能下降。精确估计目标边框是一项复杂的任务，需要高层次的先验知识，不能被建模为一个简单的图像变换(例如统一的图像缩放)。</p>
<p>边界框回归在目标检测中有广泛的应用，ATOM就借鉴了检测中的IOUNet来进行状态估计。考虑到检测网络具有class-specific的特性，不适合target-specific的跟踪任务，Martin设计了一个调制网络嵌入参考帧中的目标外观信息从而得到target-specific的IOU预测。在跟踪过程中，通过简单地最大化每帧的预测IOU来找到目标框。结果表明，与传统的多尺度搜索方法相比，该方法的性能有了显著提高。后续的DiMP, PrDiMP和KYS都采用了这种策略，其中PrDiMP使用基于能量的模型来预测边界框的非归一化概率密度，而不是预测IOU，通过最小化KL散度和高斯标签来训练的。</p>
<h2 id="基于分割的DCF"><a href="#基于分割的DCF" class="headerlink" title="基于分割的DCF"></a>基于分割的DCF</h2><p>目标分割为跟踪提供了可靠的目标观测，解决了旋转目标框、遮挡、变形、缩放等跟踪问题。Bertinetto等人使用了一种基于颜色直方图的分割方法来改进在不同光照变化、运动模糊和目标变形下的跟踪。Lukezic等人提出了一种使用基于颜色的分割方法来正则化滤波学习的空间可靠性图。Kart等人将CSR-DCF跟踪器扩展到基于颜色和深度分割的RGB-D跟踪，深度线索提供了更可靠的分割图。Lukezic等人提出了一种single shot分割跟踪器来解决联合框架内的VOT和VOS问题，采用两种判别模型进行编码，用于联合跟踪和分割任务。最近，Robinson等人利用ATOM的快速优化方案，将一种功能强大的判别模型用于视频对象分割任务。Bhat等人也使用了目标模型区分能力来实现更鲁棒的视频对象分割。</p>
<h1 id="SIAMESE-TRACKERS"><a href="#SIAMESE-TRACKERS" class="headerlink" title="SIAMESE  TRACKERS"></a>SIAMESE  TRACKERS</h1><p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211225121835.png" alt=""></p>
<p>深度学习模型成功的关键是特征在大量数据上的离线学习能力，能够从大量标注的数据中学习复杂而丰富的关系。孪生网络将目标跟踪看成一个相似性学习问题，通过端到端的离线训练来学习目标图像和搜索区域之间的相似性。Siamese跟踪器由模板分支和检测分支组成，模板分支输入初始目标图像块，检测分支输入当前帧图像块。这两个分支共享CNN参数，使得两个图像块编码了适合跟踪的相同变换。Siamese跟踪框架如图3所示，其主要目标是克服预训练CNN的局限性，充分利用端到端学习。离线训练视频用于指导跟踪器处理旋转、视点变化、光照变化和其他复杂的挑战。Siamese跟踪器能够学习物体运动和外观之间的一般关系，并可以用来定位训练中未见过的目标。</p>
<p><strong>Training Pipeline</strong> 以SiamFC为例，输入一对训练图像(x,  z)，x表示感兴趣的对象（第一帧裁剪的图像块）和z表示后续帧的搜索图像区域。将这些图像对输入CNN中，以获得两个特征图，然后使用互相关进行匹配，</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211226102603.png" alt=""></p>
<p>其中星号表示互相关，$f_{\rho}(.)$表示CNN，如AlexNet，模型参数为$\rho$。$g_{\rho}(x,z)$表示x和z之间相似性的响应映射，$b$是常数标量。</p>
<p>训练目标是使得响应图$g_{\rho}(x,z)$的最大值与目标位置相对应，因此采用logistic loss：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211226103514.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211226103550.png" alt=""></p>
<p>其中$v$是预测值，$c \in \{-1,1\}$是标签。</p>
<p>同期的其他一些工作，如：SINT使用欧式距离作为相似度量，而不是互相关；GOTURN预测的是边框回归的结果；CFNET将相关滤波器作为匹配函数中的一个独立模块加入到x中，使该网络更浅更高效。</p>
<p><strong>Testing Pipeline</strong> 测试过程就是利用学到的匹配函数度量x在z上每个匹配区域的相似度，用得分最高的位置预测目标的新位置。最初的SiamFC只是将每一帧与目标的初始外观进行比较，并在GPU上以140FPS的速度实时跟踪。Siamese跟踪器在推理和离线学习中都具有极高的计算效率，并且跟踪性能卓越，因此在跟踪社区中受到了很多关注。</p>
<h2 id="Siamese跟踪存在的挑战"><a href="#Siamese跟踪存在的挑战" class="headerlink" title="Siamese跟踪存在的挑战"></a>Siamese跟踪存在的挑战</h2><p>经典的SN在准确性和效率上都优于DCF跟踪器。然而，SN在backbone提取网络、离线训练时需要大量的标注图像对、缺乏在线适应性、损失函数、目标状态估计等方面也存在一定的局限性。下面将讨论这些问题并概述近年来发展的解决方案。</p>
<h3 id="Backbone结构"><a href="#Backbone结构" class="headerlink" title="Backbone结构"></a>Backbone结构</h3><p>早期的Siamese跟踪器均使用AlexNet作为特征提取，包括SiamFC, GOTURN, SINT, FlowTrack, MemTrack, EAST和SiamRPN。然而，这些跟踪器在性能上仍然有限，因为AlexNet是一个相对较浅的网络，并没有产生非常强的特征表示。直接替换更深的网络并不能取得性能上的提升。</p>
<p>为了解决这一问题，SiamRPN++研究发现，在SNs中，网络中的padding使得学习到的特征表示不满足空间平移不变性约束。因此，提出了一种有效的采样技术来满足这种空间不变性约束。利用强大的深层ResNet架构，许多Siamese跟踪器的性能得到了改善。Zhang等人也研究了同样的问题，并提出了SiamDW，其中浅骨干AlexNet被包括Inception、VGG-19和ResNet在内的深网络所取代。研究发现，除了padding外，感受野和网络步长也是深层网络不能直接替代浅层网络的主要原因。有了这些基础，最近的跟踪器包括SiamCAR、Ocean和SiamBAN等都使用了更深的网络结构。</p>
<p><em>由于ResNet的简单性和强大的性能，ResNet已经成为Siamese跟踪的首选方案。然而，Transformer的最新进展预计将在未来几年对跟踪社区产生重大影响。</em></p>
<h3 id="离线训练"><a href="#离线训练" class="headerlink" title="离线训练"></a>离线训练</h3><p>当前流行的跟踪训练数据集包括：ImageNet ILSVRC2014、ILSVRC2015、COCO、YouTube-BB、YouTube-VOS、LaSOT、GOT-10k和TrackingNet。这些数据集充分覆盖了大量的语义，并且不关注特定的对象，否则在Siamese训练中，调优后的网络参数会过拟合到特定的对象类别。</p>
<p>与DCF范式不同，标准的Siamese范式不能利用跟踪过程中的已知干扰物。因此，当与目标本身相似的物体出现时，Siamese跟踪器通常不能很好地处理。早期的方法只在训练过程中对同一视频中的训练图像进行采样，这种抽样策略不关注具有语义相似干扰物的情况。为了解决这个问题，Zhu等人在DaSiamRPN中引入了难负样本挖掘技术，通过在训练过程中引入更多的语义负样本对来克服数据不平衡。构建的负样本对由相同和不同类别的标记目标组成。该技术通过更多地关注细粒度表示，帮助DaSiamRPN克服漂移。Voigtlaender (SiamRCNN)等人利用嵌入网络和最近邻近似提出了另一种难负样本挖掘技术。对于每个真实目标框，使用预训练网络为相似的目标外观提取嵌入向量。 然后使用索引结构估计近似最近邻，并使用它们估计嵌入空间中目标对象的最近邻。</p>
<p><em>近年来，利用更多的训练数据和设计数据挖掘技术的趋势在多个基准上显示出了良好的跟踪性能。</em></p>
<h3 id="在线模型更新"><a href="#在线模型更新" class="headerlink" title="在线模型更新"></a>在线模型更新</h3><p>在SiamFC中，目标模板在第一帧中初始化，然后在视频的剩余部分中保持固定。跟踪器不进行任何模型更新，因此其性能完全依赖于SN的泛化能力。然而当外观变化很大时，不更新模型往往导致跟踪失败。下面将介绍模型更新方向的潜在解决方案。</p>
<p><strong>Moving Average Update Method</strong> 最简单的线性更新策略，使用固定学习率的对模板滑动平均更新。虽然它提供了一种集成新信息的简单方法，但由于恒定的更新速率和简单的线性模板组合，导致跟踪器无法从漂移中恢复。</p>
<p><strong>Learning Dynamic SN Method</strong> DSiam设计了两个动态变换矩阵，包括目标外观变化和背景抑制。这两个矩阵都在傅里叶域中用闭式解进行求解。DSiam提供了有效的在线学习，但它忽略了历史目标变化，这对于更平滑地适应模板非常重要。 </p>
<p><strong>Dynamic Memory Network Method</strong> MemTrack可以动态写入和读取以前的模板，以应对目标外观的变化。使用LSTM作为存储器控制器，输入是搜索特征图，输出存储器读写过程的控制信号。这种方法使跟踪器能够记忆长期目标外观。然而，该算法只关注目标特征，忽略了背景杂波中的鉴别信息，在目标变化剧烈的情况下，会导致精度下降。 </p>
<p><strong>Gradient-Guided Method</strong> Li等人提出了GradNet，对梯度信息进行编码，通过前向和后向操作更新目标模板。跟踪器利用梯度信息更新当前帧中的模板，然后加入自适应过程，简化基于梯度的优化过程。与上述方法不同的是，该方法充分利用了反向传播梯度中的判别信息，而不是仅仅集成之前的模板。这样可以提高算法性能，但代价是以反向传播的方式计算梯度引入了计算负担。</p>
<p><strong>UpdateNet Method</strong> UpdateNet利用一个CNN整合了初始模板、历史累计模板、以及当前帧模板，进行自适应更新。基于现有模板与累积模板的差异，可以适应现有框架的具体更新要求。此外，在每帧中还考虑了初始模板，提供了高度可靠的信息，增加了对模型漂移的鲁棒性。结果表明，与SiamFC和DaSiamRPN相比，该方法具有优异的性能。</p>
<p><em>虽然已经提出了许多模型更新的技术，但简单地不使用更新仍然是一种鲁棒且流行的选择。在这个方向上的进一步研究需要开发简单的、通用的、端到端可训练的技术，从而进一步提高Siamese跟踪的鲁棒性。</em></p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>损失函数主要包括分类和回归两种任务。</p>
<p><strong>Logistic Loss</strong> 如上述公式16-17所示，早期方法均采用逻辑损失，包括SiamFC, DSiam, RASNET, SA-SIAM, CFNET, SiamDW和GradNet。该训练方法利用图像对上的成对关系，在正样本对上相似性分数最大化，在负样本对上相似性分数最小化。</p>
<p><strong>Contrastive Loss</strong> </p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211227102746.png" alt=""></p>
<p>其中$\epsilon$为距离阈值，$D$代表两个样本特征的欧氏距离，$y_{x z} \in \{0,1\}$表示$x$和$z$是否属于同一个目标。SINT采用了contrastive loss，而GOTURN采用了预测和真实目标框之间的L1 loss。</p>
<p><strong>Triplet Loss</strong> 上面的损失只利用了图像之间的两两关系，忽略了正样本对和负样本对之间的结构联系。Yan等人提出了SPLT跟踪器，在训练过程中使用Triplet Loss，定义如下：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211227103412.png" alt=""></p>
<p>其中$x^p$是正例，$x^n$是负例。Triplet Loss可以进一步挖掘目标、正实例和负实例之间的潜在关系，而且包含了更鲁棒的相似结构。SiamFC-Tri，SiamRCNN都利用了该损失函数。</p>
<p><strong>Cross Entropy Loss</strong> SNs中的分类分支借鉴了目标检测中常用的交叉熵损失，定义为：</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211227110305.png" alt=""></p>
<p>其中$p_o$是预测值，$p_o^*$是标签。SiamRPN最早将检测思想引入SNs并使用了交叉熵损失。后续在其基础上，SiamRPN++, SiamAttn, Ocean, CLNET, SPM, C-RPN均使用交叉熵损失用于训练分类分支。</p>
<p><strong>Regression Loss</strong> 回归损失使用较多的为smooth L1 loss (SiamRPN, SiamRPN++, SiamAttn, CLNET, SPM, C-RPN)和IOU loss (SiamBAN, Ocean, SiamFC++)。</p>
<p><strong>Multi-Task Loss</strong> 对于分类分支和回归分支的联合训练，需要多任务损失。如交叉熵+smooth L1 loss或交叉熵+IOU loss。</p>
<p><strong>Regularized Linear Regression</strong> 为了将相关滤波作为一个单独的层加入SNs，需要使用公式9-10的线性回归损失。将岭回归问题通过闭式解的形式解决，并以端到端方式嵌入整个训练框架，代表方法有：CFNET, TADT, RTINET, DSiam, FlowTrack, UDT, UDT++。</p>
<p><em>目前，关于损失函数的研究还没有普遍的共识。相反，最近的SOTA方法采用了不同的替代方法。在上述方法中，交叉熵损失仍然是最近的跟踪器一个普遍的选择</em></p>
<h3 id="目标状态估计-1"><a href="#目标状态估计-1" class="headerlink" title="目标状态估计"></a>目标状态估计</h3><p>与DCF类似，SNs也面临尺度变化的挑战。相似函数只能学习图像之间深层结构关系，没有考虑尺度变化问题。下面将讨论这一方面的发展。</p>
<p><strong>Multiple Resolution Scale Search Method</strong> 最早期的方法仍然是简单粗暴的多尺度搜索，如如RASNET、SA-Siam、StructSiam、UDT、UDT++、TADT、GradeNet、RTINET、FlowTrack。</p>
<p><strong>Deep Anchor-based Bounding Box Regression Method</strong> 基于锚框的边框回归利用目标检测中的RPN网络来预测具有多种尺度和宽高比的proposal。RPN是一个全卷积网络，它同时预测每个位置的分类分数和边框回归。RPN以端到端方式进行训练，以生成高质量的proposal。最早运用RPN的跟踪器是SiamRPN，包括一个分类分支和回归分支，相比传统方法取得了显著的进步。之后的DaSiamRPN， SiamRPN++， SiamDW ， SPLT， C-RPN， SiamAttn， CSA， SPM等也建立在相同的概念上。</p>
<p><strong>Deep Anchor-free Bounding Box Regression Method</strong> 上述基于锚框的方法需要启发式知识精心设计锚箱，引入了许多超参数和计算复杂度。因此目标检测中提出了无锚框的方法，避免了设计与锚框相关的超参数，更加灵活通用。无锚框方法分为keypoint-based和center-based两种，前者首先定位预定义的关键点，然后在目标上执行边框回归；后者则预测目标中心正样本区域到边界的四个距离。无锚检测器能够消除与锚框相关的超参数，性能与基于锚框的检测器相似，具有更强的泛化能力。典型的无锚框孪生跟踪算法包括SiamBAN, Ocean, SiamCAR等。</p>
<p><em>目标检测技术在目标状态估计方面取得了显著的进展。最近使用RPN和无锚框回归结构的趋势表明，可以在端到端范例中进一步探索这些技术</em></p>
<h2 id="基于分割的Siamese跟踪"><a href="#基于分割的Siamese跟踪" class="headerlink" title="基于分割的Siamese跟踪"></a>基于分割的Siamese跟踪</h2><p>Siamese跟踪也可以和分割结合起来，处理一些形变的问题（比如手掌张开的人、旋转或轴对齐的目标框）。Wang等人提出SiamMask，可以同时估计二值蒙版、边界框和相应的背景-前景得分。该网络能够联合处理视觉跟踪和目标分割，以提高鲁棒性。Lu等人采用无监督视频对象分割任务，该任务基于SN内的联合注意机制提出了一种新颖的体系结构。</p>
<h1 id="EXPERIMENTAL-COMPARISON"><a href="#EXPERIMENTAL-COMPARISON" class="headerlink" title="EXPERIMENTAL COMPARISON"></a>EXPERIMENTAL COMPARISON</h1><p>实验比较了59个DCF和33个Siamese trackers在9个跟踪数据集上的性能，包括OTB100, TC128, UAV, VOT2014, VOT2016, VOT2018, TrackingNet, LaSOT和GOT-10k。图5显示了来自不同跟踪数据集的示例帧。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211227153554.png" alt=""></p>
<h2 id="跟踪数据集"><a href="#跟踪数据集" class="headerlink" title="跟踪数据集"></a>跟踪数据集</h2><p>为了提供一个标准和公平的目标跟踪器性能评估，许多benchmarks被提出。除了短期跟踪外，一些最近的数据集提供了短期和长期跟踪序列。公开的基准数据集包含各种跟踪挑战，包括尺度变化(SV)，出视野(OV)，形变(DEF)，低分辨率(LR)，光照变化(IV)，平面外旋转(OPR)，遮挡(OCC)，背景杂波(BC)，快速运动(FM)，平面内旋转(IPR)，运动模糊(MB)、部分遮挡(POC)、摄像机突然运动(CM)、长宽比变化(ARC)、全遮挡(FOC)、视点变化(VC)、相似物体(SOB)、物体颜色变化(OCC)、绝对运动(SOB)、目标旋转(ROT)、场景复杂度(SCO)、快速摄像机运动(FCM)、低分辨率物体(LRO)、运动变化(MOC)。表1给出了我们在实验比较中使用的每个数据集的描述。接下来，简要描述每个跟踪数据集。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211227154029.png" alt=""></p>
<p><strong>OTB100</strong> 22个对象类别的100个视频组成，与OTB50相同的11个跟踪属性。OTB100数据集的平均分辨率为356 × 530，视频长度在71 - 3872帧之间。</p>
<p><strong>TC128</strong> TC128用于评价颜色对视觉跟踪的影响。它包含27个对象类别的128个完整标注的彩色视频序列。在128个序列中，有78个序列不同于OTB100，而剩下的50个序列在两个数据集中是共同的。TC128也包含11个跟踪属性，类似于OTB100(表1)，平均分辨率为461  × 737，最小帧数71帧，最大帧数3872帧。</p>
<p><strong>UAV123</strong> UAV是低空无人机捕获的真实和合成高清视频序列，分为两个子集，UAV123和UAV20L。UAV123包含9种对象类别的123个短序列，最小帧数109帧，最大帧数3085帧。UAV20L由飞行模拟器生成的5个对象类的20个长视频组成。这些序列包含最小1717帧和最大5527帧。两个数据集子集都包含1231 ×  699的平均分辨率，并带有12个跟踪属性。</p>
<p><strong>VOT2016</strong> 包含60个序列，每个序列每帧都由不同的属性标注，包括OCC、IV、MOC、ARC、SCO和FCM。序列的平均分辨率为757×480，最小帧数为48，最大帧数为1507。</p>
<p><strong>VOT2018</strong> 该数据集由短期和长期挑战组成。VOT2018 ShortTerm (VOT2018-st)挑战由24个对象类别的60个序列组成。短期挑战序列的平均分辨率为758 ×  465，最小帧数为41帧，最大帧数为1500帧。长期分割由35个长期序列组成。序列的长期平均分辨率为896 ×  468，最小帧数为1389，最大帧数为29700。</p>
<p><strong>VOT2020</strong> 由五个子集组成，我们使用VOT2020短期(VOT2020-st)数据集来评估跟踪器的性能。VOT2020-ST与VOT2018-ST在视频数量、类数量和属性数量方面相同。区别在于标注由mask编码，并且重新定义了A,R,EAO。可以参考我之前写的<a href="https://www.bilibili.com/read/cv7654043?from=search&amp;spm_id_from=333.337.0.0">VOT2020 测评指标</a>。</p>
<p><strong>TrackingNet</strong> 由60643个序列和超过1400万个密集的包围框注释组成。它涵盖了27个不同的对象类。序列也由15个跟踪属性表示。数据集被划分为训练、验证和测试部分。训练集包含30643个序列，而测试集包含511个视频。在测试集中，序列的平均分辨率为591  × 1013，最小帧数为96，最大帧数为2368，帧数为30fps。</p>
<p><strong>LaSOT</strong> 由1120个训练序列(2.8M帧)和280个测试序列(685K帧)组成。在每一帧中，所有序列都用边界框标注。对象类别是从ImageNet中选择的，包含70个不同的对象类别，每个类别包含20个目标序列。根据ARC、BC、FCM、DEF、POC、ROT和VC等14个属性对序列进行分类。序列的平均分辨率为632  × 1089。此外，该数据集包含非常长的序列，范围在1000到11,397帧之间。</p>
<p><strong>GOT -10K</strong> 由WordNet语义层次结构中的10,000个视频组成。目的是为开发具有丰富运动轨迹的类无关跟踪器提供统一的训练和测试平台。这些序列被分类为563类运动物体、6种跟踪属性和87类运动，以覆盖现实世界中尽可能多的具有挑战性的模式。GOT-10K分为训练、验证和测试三部分。训练集包含9340个序列，480个物体类别；测试集包含420个视频，83个物体类别，每个序列平均长度为127帧。在测试集中，序列的平均分辨率为929  × 1638，最小帧数为51，最大帧数为920，帧数为10fps。</p>
<h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><p><strong>Precision Plot</strong> 精度曲线基于中心位置误差，中心位置误差定义为目标物体的预测中心与真实目标框中心之间的平均欧氏距离。距离精度，定义为目标物体的中心定位误差在T像素范围内的帧数所占的百分比。通过绘制阈值范围内的距离精度来生成精度曲线，使用T=20的精度对跟踪器进行排名。</p>
<p><strong>Success Plot</strong> 精度只度量跟踪器的定位性能，对于测量目标尺度变化不准确。因此考虑用IOU来衡量这一指标，成功率是预测IOU大于阈值T的百分比。通过将IOU阈值从0改变到1来生成成功率曲线，使用曲线下的面积对跟踪器进行排名。</p>
<p><strong>Normalized Precision Plot</strong> 由于距离精度对目标尺度敏感，因此引入归一化精度，它计算相对于目标大小的误差，而不是考虑绝对距离。然后在0到0.5的范围内绘制相对误差曲线，这条曲线下的面积称为归一化精度，用于对跟踪器进行排序。</p>
<p><strong>Average Overlap</strong> 度量预测和真实框的重叠率的平均值。</p>
<p><strong>$SR_{0.50}$和$SR_{0.75}$</strong> 阈值为0.50和0.75时的成功率。</p>
<p>在OTB100、TC128、UAV123和LaSOT数据集上，使用One pass评估标准，从精度和成功率方面衡量跟踪性能。这些数据集上的跟踪器是通过在第一帧上初始化并让它运行到序列的末尾来计算的。</p>
<p>在VOT系列中，跟踪器一旦偏离目标就会被重置。VOT评估协议根据准确性(A)，鲁棒性(R)和期望平均重叠(EAO)对跟踪器进行比较。A是成功跟踪期间预测和真值的平均重叠。R表示跟踪器在跟踪过程中丢失目标(失败)的次数。一旦跟踪器丢失目标对象，复位机制将在某些帧后启动。EAO是一种估计跟踪器期望在与给定数据集具有相同视觉特性的大量短期序列上获得的平均重叠。</p>
<h2 id="定量分析"><a href="#定量分析" class="headerlink" title="定量分析"></a>定量分析</h2><p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211227162203.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211227162224.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211227162351.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211227162405.png" alt=""></p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211227162415.png" alt=""></p>
<p>LaSOT和UAV123包含长序列和多个干扰，在这些数据集上性能较高的跟踪器具有较强的鲁棒性和重检测能力。我们观察到，最近的跟踪器DiMP和PrDiMP取得了很强的效果，SiamR-CNN中的干扰感知跟踪提高了此类场景中的鲁棒性。与LaSOT相比，TrackingNet和GOT10k包含短序列，鲁棒性和重检测能力的重要性要小得多。相反，这些数据集更看重高精度的预测框，比如SiamR-CNN和PrDiMP。此外，SiamR-CNN和SiamAttn在几个数据集均实现了良好的结果。除了SiamAttn在LaSOT和SiamRCNN在VOT上效果欠佳。在基于DCF的方法中，PrDiMP在所有评估的数据集中一致地实现SOTA结果。</p>
<p><img src="https://github.com/kongbia/picgo/raw/master/blog/52-survey/20211227162909.png" alt=""></p>
<p>图6展示了近年来不同benchmark上跟踪性能的提升趋势。可以看到OTB几乎饱和了，而最近推出的LaSOT、GOT10K和TrackingNet都显示了类似的趋势，性能还在持续上涨，且仍有较大空间。这表明，这些新的具有挑战性的benchmark对于SOTA跟踪器来说仍然是非常具有挑战性的，它们的引入显著地推动了视觉跟踪研究的上界。</p>
<h2 id="速度比较"><a href="#速度比较" class="headerlink" title="速度比较"></a>速度比较</h2><p>KCF和STAPLE追踪器的速度明显最好，而DeepSRDCF和HCF等借助离线深度学习特征的跟踪器速度较慢。</p>
<h1 id="DISCUSSION-AND-CONCLUSIONS"><a href="#DISCUSSION-AND-CONCLUSIONS" class="headerlink" title="DISCUSSION  AND CONCLUSIONS"></a>DISCUSSION  AND CONCLUSIONS</h1><p><strong>Importance of end-to-end tracking framework</strong> 端到端训练不管对DCF还是Siamese跟踪都非常管用。随着大规模训练数据集的引入，这在过去几年才成为可能。</p>
<p><strong>Importance of robust target modeling</strong> 虽然基于Siamese的方法在许多领域都表现出色，但基于端到端DCF的方法在挑战长期跟踪场景(如LaSOT)方面仍然显示出优势。这说明了通过在网络结构中嵌入判别学习模块来实现鲁棒在线目标外观建模的重要性。这些方法有效地整合了背景外观线索，并且在使用在线学习的跟踪过程中易于更新。</p>
<p><strong>Target state estimation</strong> 基于Siamese的方法通过利用目标检测技术，推动了更精确的边框回归的进步。最近的基于单阶段(无锚框)的方法，如Ocean，实现了简单、准确和高效的边框回归。此外，这些策略是通用的，可以很容易地集成到任何视觉跟踪架构中。</p>
<p><strong>Role of segmentation</strong> 分割可以提供更精确的像素级预测，此外，分割还可以改善跟踪本身的潜力，例如辅助目标模型更新。因此，未来的工作应着眼于将准确的分割集成到跟踪框架中。</p>
<p><strong>Backbone architectures</strong> ResNet在视觉跟踪中仍然是最常用的特征提取方法，但在计算资源受限的平台ResNet的计算成本仍然较高。因此未来研究移动平台的高效backbone仍然是一个有趣的方向。</p>
<p><strong>Estimating geometry</strong> 将DCF和Siamese方法往3D方向扩展。</p>
<p><strong>Role of Transformers</strong> Transformer最近在各种视觉任务上都取得了成功，也被应用到了跟踪。未来还需要做更多的工作来进一步分析Transformer的有效性，以及它与DCF和Siamese的联系。</p>
<p><strong>Future directions</strong> 1）与分割结合；2）与SLAM结合；3）与多目标跟踪结合。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>综述</tag>
        <tag>DCF</tag>
        <tag>孪生网络</tag>
      </tags>
  </entry>
</search>
