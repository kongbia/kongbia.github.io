<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>目标检测框架在目标跟踪中的应用</title>
    <url>/tracking/2-detection-in-tracking/</url>
    <content><![CDATA[<p>从SiamRPN将跟踪问题定义为one-shot detection任务之后，出现了大量将检测组件由于跟踪的研究。不过Siamese系列一个很大的问题在于其本质仍然是一个模板匹配问题，网络关注的是寻找与target相似的东西，而忽视了区分target和distractor的判别能力，这正是目标检测任务所擅长的。目标检测和目标跟踪的关键差异在于检测是一个class-level的任务，而跟踪是一个instance-level的任务（即检测只关注类间差异而不重视类内差异，跟踪需要关注每一个实例，同时跟踪的类别是不可知的）。</p>
<p>本篇笔记关注如何将目标检测框架应用在跟踪中，主要介绍其思想，细节部分不做过多描述，记录论文包含：</p>
<span id="more"></span>
<ul>
<li><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Bridging_the_Gap_Between_Detection_and_Tracking_A_Unified_Approach_ICCV_2019_paper.pdf">Bridging the Gap Between Detection and Tracking: A Unified Approach</a></li>
<li><a href="https://arxiv.org/abs/1912.08531">GlobalTrack: A Simple and Strong Baseline for Long-term Tracking</a></li>
<li><a href="https://arxiv.org/abs/1910.11844">Learning to Track Any Object</a></li>
<li><a href="https://arxiv.org/abs/1911.12836">Siam R-CNN: Visual Tracking by Re-Detection</a></li>
<li><a href="https://arxiv.org/abs/2004.00830">Tracking by Instance Detection: A Meta-Learning Approach</a></li>
</ul>
<h1 id="Bridging-the-Gap-Between-Detection-and-Tracking-A-Unified-Approach"><a href="#Bridging-the-Gap-Between-Detection-and-Tracking-A-Unified-Approach" class="headerlink" title="Bridging the Gap Between Detection and Tracking: A Unified Approach"></a>Bridging the Gap Between Detection and Tracking: A Unified Approach</h1><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210424234243.png" alt=""></p>
<p>从结构图可以很直观的看出这就是Faster RCNN的框架，作者将目标跟踪任务看成是one-shot object detection 和 few-shot instance classification 的组合。前者是一个class-level的子任务用来寻找和目标相似的候选框，后者是instance-level的任务用来区分目标和干扰物。主要有两个模块：Target-guidence module(TGM) 和 few-shot instance classifier。</p>
<p>TGM对目标和搜索区域的特征以及它们在主干中的相互作用进行编码，相当于让网络更关注于与目标相关的instance，后面几篇文章也用了不同的方法来实现这个目的。</p>
<p>TGM虽然使检测器聚焦于与目标相关的物体，但忽略了周围的背景干扰。为了弥补这一点，提出了few-shot instance classifier。 然而，直接从头开始训练耗时且容易导过拟合。因此作者通过Model-Agnostic Meta Learning (MAML)进行few-shot finetune，增强判别性进一步消除distractors。</p>
<p>MAML的目的是训练一组初始化参数，<strong>通过在初始参数的基础上进行一或多步的梯度调整，来达到仅用少量数据就能快速适应新task的目的，</strong>示意图如下：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210424234331.png" alt=""></p>
<p>域自适应的检测器整体训练流程如下图：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210424234353.png" alt=""></p>
<p>输入是三元组，包括examplar, support, query，训练分为inner and outer optimization loops。对于Inner optimization loop中，在support image上计算的loss用来微调meta-layers即detector heads的参数，然后用微调后的参数计算meta-loss其梯度用于更新outer optimization loop。具体公式可以参考原文。</p>
<p>在线跟踪中将之前帧的检测结果作为训练样本在线更新detector head的参数。</p>
<p>作者称这是第一篇将目标检测框架应用到跟踪上的通用框架，检测模型可以用Faster RCNN，SSD等，速度上SSD模型为10FPS   Faster RCNN模型为3FPS。</p>
<hr>
<h1 id="GlobalTrack-A-Simple-and-Strong-Baseline-for-Long-term-Tracking"><a href="#GlobalTrack-A-Simple-and-Strong-Baseline-for-Long-term-Tracking" class="headerlink" title="GlobalTrack: A Simple and Strong Baseline for Long-term Tracking"></a>GlobalTrack: A Simple and Strong Baseline for Long-term Tracking</h1><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210424234652.png" alt=""></p>
<p>这篇文章构建了一个global instance search tracker，主要思想是利用target来引导网络搜索特定instance，与上一篇的TGM模块思想类似，不过这里对在RPN阶段和分类回归阶段都加入了target信息进行引导。对应的就是Query-guided RPN 和 Query-guided RCNN。</p>
<p><strong>Query-guided RPN</strong></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210424234747.png" alt=""></p>
<p>$z \in [k,k,c]$ 是query的ROI特征，$x \in [h,w,c]$ 是搜索图像的特征。$f<em>z$ 用 $k \times k$ 0-padding的卷积将 $z$ 转换为 $1 \times 1$ 的核作用于搜索区域，$f_x$ 使用 $3 \times 3$ 1-padding的卷积。$f</em>{out}$ 是 $1 \times 1 \times c$  的卷积将通道数变回为c，这个过程不使用正则化和激活函数。</p>
<p><strong>Query-guided RCNN</strong></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210424235530.png" alt=""></p>
<p> $z$ 定义同上，$x<em>i \in [k,k,c]$ 是提取的proposal的特征。$h_z$ 和 $h_x$ 均为 $3 \times 3$ 1-padding的卷积，$h</em>{out}$ 为$1 \times 1 \times c$  卷积。</p>
<p>GlobalTrack 对视频每一帧的跟踪完全不依赖相邻帧，没有累计误差使得它在长期跟踪问题中准确率保持稳定。速度为6FPS。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210426092832.png" alt=""></p>
<center>车牌在长期跟踪过程中消失了一段时间，当车牌再次出现的时候，其他跟踪算法就再也无法恢复跟踪了，而没有累计误差的 GlobalTrack不受前面的影响立刻跟踪到了目标。</center>

<hr>
<h1 id="Learning-to-Track-Any-Object"><a href="#Learning-to-Track-Any-Object" class="headerlink" title="Learning to Track Any Object"></a>Learning to Track Any Object</h1><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210426093105.png" alt=""></p>
<center>图1 (a)从基于图像的数据集学习一个通用对象先验，(b)通过计算一个封闭形式的目标和背景之间的线性判别器使其适应于一个感兴趣的特定对象(例如左上角的总线)。这允许跟踪物体通过显著的变形，而不捕获干扰物</center>

<p>本文重点在于将category-specific object detector 变成 category-agnostic, object-specific detector来做跟踪。想达到这个目的，需要处理如下两个关键的问题，如图1所示：</p>
<ol>
<li>如何将 category specific prior 改为 generic objectness prior？</li>
<li>如何进一步的将这种 generic prior 改为 particular instance of interst？</li>
</ol>
<p>针对问题1，作者构建了 a joint model for category-specific object detection and category-agnostic tracking。和之前类似，也是添加了目标特征的检测框架（基于 Mask R-CNN）如下图2所示 。其将目标模板作为输入，计算 feature embedding。然后该模板特征与测试图像计算相似性得到attention mask。attention mask又被用于重新加权空间特征，以检测感兴趣的物体。另外这个框架可以同时用于检测、跟踪和分割。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210426100348.png" alt=""></p>
<p>针对问题 2，本文计算一个线性分类器来区分第一帧的感兴趣目标和其他目标，通过最小二乘方法得到闭式解从而可以学习到一个更关注感兴趣instance的鲁棒特征。下图3通过一个例子说明，左下是直接用feature embedding计算的attention map，右下是用线性分类器计算的attention map，显然右下效果更好。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210426100433.png" alt=""></p>
<p>最后运行速度为7FPS。</p>
<hr>
<h1 id="Siam-R-CNN-Visual-Tracking-by-Re-Detection"><a href="#Siam-R-CNN-Visual-Tracking-by-Re-Detection" class="headerlink" title="Siam R-CNN: Visual Tracking by Re-Detection"></a>Siam R-CNN: Visual Tracking by Re-Detection</h1><p>这个就是用重检测的思想做跟踪，也是基于RCNN框架的，同时使用Tracklet Dynamic Programming Algorithm去跟踪所有潜在的目标。</p>
<p>具体解读见前一篇笔记</p>
<p><a href="https://kongbia.github.io/2021/04/24/tracking/1-siamrcnn/">Siam R-CNN: Visual Tracking by Re-Detection | CV home (kongbia.github.io)</a></p>
<hr>
<h1 id="Tracking-by-Instance-Detection-A-Meta-Learning-Approach"><a href="#Tracking-by-Instance-Detection-A-Meta-Learning-Approach" class="headerlink" title="Tracking by Instance Detection: A Meta-Learning Approach"></a>Tracking by Instance Detection: A Meta-Learning Approach</h1><p>同样是域自适应方法将检测器转化成跟踪器，此篇更像是第一篇Bridging the Gap Between Detection and Tracking: A Unified Approach的进阶，不同的是本文没有额外添加模板引导分支，而是直接用标准的检测器通过元学习的方式做域自适应。避免了冗余结构使得速度大幅提升，达到40FPS。另外就是训练的时候加入了很多来自MAML++喝MetaSGD的技巧，效果更好。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210426100802.png" alt=""></p>
<p>具体的解读可以参考我b站的笔记，后续会搬运到博客。</p>
<p><a href="https://www.bilibili.com/read/cv5521209/">[Note7] Tracking by Instance Detection: A Meta-Learning Approach - 哔哩哔哩专栏 (bilibili.com)</a></p>
<hr>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这几篇文章的一个共同思路都是融合了Siamese架构和目标检测框架，将目标实例信息以各种形式加入待检测图像中，从而将class-level的通用检测转变成instance-level的实例检测（跟踪）。借助目标检测对尺度，形变等复杂条件的优越性来解决跟踪中的问题，同时将跟踪转变成one-shot的检测任务也避免了更新带来的漂移（第一篇里面使用了MAML进行更新，主要原因猜测是单纯往RPN中融合目标信息还不够work，像globaltracker在head上也添加了instance，而第三篇则是构建一个分类器增强鲁棒性）。当然引入检测框架带来的计算开销也是很大的，最后一种方法避免了额外的模板分支相当于跳出了Siamese框架，给实时带来了可能。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title>Anchor Free的孪生目标跟踪</title>
    <url>/tracking/3-anchorfree-siamese/</url>
    <content><![CDATA[<p>Anchor-free+孪生网络做跟踪在2020年非常火爆，相关笔记在b站记录。本文主要对其整合进行简单归纳。</p>
<p><a href="https://www.bilibili.com/read/cv4987634/?from=readlist">[Note3] Anchor Free的目标跟踪 - 哔哩哔哩专栏 (bilibili.com)</a></p>
<p><a href="https://www.bilibili.com/read/cv6518222/?from=readlist">[Note17] Anchor-free的目标跟踪(下) - 哔哩哔哩专栏 (bilibili.com)</a></p>
<p>跟踪任务可以看成是分类任务与状态估计任务的结合。分类任务的目的是精确定位目标的位置，而状态估计获得目标的姿态（即目标框）。SiamFC++一文将当前的跟踪器按照不同<strong>状态估计</strong>的方法分为三类：</p>
<span id="more"></span>
<ol>
<li>以DCF和SiamFC为主的跟踪器，构建多尺度金字塔，将搜索区域缩放到多个比例，选择最高得分对应的尺度，这种方式是最不精确的同时先验的固定长宽比不适合现实任务；</li>
<li>以ATOM为主的跟踪器，借鉴IOUNet，通过IOU的梯度迭代来细化box，提升精度的同时带来了较多的超参数以及时间上的消耗；</li>
<li>以SiamRPN为主的追踪器，通过RPN预设anchor来回归框，这类方法虽然很高效，但是anchor的设定不但会引入模糊的相似性得分，而且anchor的设置需要有大量的数据分布先验信息，与通用跟踪的目的不符合。</li>
</ol>
<p>本文主要记录用Anchor Free的思想来解决上述目标跟踪状态估计中存在的问题。目前比较主流的都是基于FCOS和CenterNet两种无锚框方式展开的。</p>
<h1 id="FCOS类"><a href="#FCOS类" class="headerlink" title="FCOS类"></a>FCOS类</h1><h2 id="SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines"><a href="#SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines" class="headerlink" title="SiamFC++: Towards Robust and Accurate Visual Tracking with Target Estimation Guidelines"></a>SiamFC++: Towards Robust and Accurate Visual Tracking with Target Estimation Guidelines</h2><p><a href="https://arxiv.org/abs/1911.06188">论文</a> <a href="https://github.com/MegviiDetection/video_analyst">代码</a></p>
<p>针对siam网络分析了之前的工作不合理的地方，提出了4条guidelines：</p>
<p>G1：decomposition of classification and state estimation：跟踪任务可以分解为分类与状态估计。分类影响鲁棒性，状态估计影响精确性。多尺度金字塔的方式忽略了状态估计所以精确性很低；</p>
<p>G2：non-ambiguous scoring：分类得分应该直接表示为目标在视野中存在的置信度分数，而不是像预定义的anchor那样匹配anchor和目标，这样容易产False positive；</p>
<p>G3：prior knowledge-free：跟踪器不应该依赖过多的先验知识（如尺度/长宽比）。现有的方法普遍存在对数据分布先验知识的依赖，阻碍了其泛化能力；</p>
<p>G4：estimation quality assessment：不能直接使用分类置信度来评价状态估计，需要使用独立于分类的质量评估方式。（如RPN系列直接就是选择分类置信度最高的位置进行边框预测，而ATOM，DIMP则另外加入了IOU信息来指导边框调整）</p>
<p>作者依据这4条guidelines设计了SiamFC++，将目标检测中的Anchor Free的FCOS应用到Siamese框架中，整体结构如下，细节部分可以去开头我在b站的专栏。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426104215.png" alt=""></p>
<hr>
<h2 id="SiamCAR-Siamese-Fully-Convolutional-Classification-and-Regression-for-Visual-Tracking"><a href="#SiamCAR-Siamese-Fully-Convolutional-Classification-and-Regression-for-Visual-Tracking" class="headerlink" title="SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking"></a>SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking</h2><p><a href="https://arxiv.org/abs/1911.07241v2">论文</a> <a href="https://github.com/ohhhyeahhh/SiamCAR">代码</a></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426111123.png" alt=""></p>
<p>这一篇和SiamFC++很类似，这里仅标注一些实践细节的差异。</p>
<ul>
<li>backbone采用了改造的resnet50；</li>
<li>multi-stage融合对相关结果拼接用1*1卷积降维/融合，而不是像siamrpn++那样对相关后的分类预测响应图加权相加；</li>
<li>分类和回归均由一个相关引出，而不是每个分支对应一个相关。这样计算量更小效率更高，而性能差不多；</li>
<li>inference阶段为了避免抖动取了中心点周围top-k的均值作为最终结果。</li>
</ul>
<p>细节同样参照开头b站专栏。</p>
<hr>
<h2 id="Siamese-Box-Adaptive-Network-for-Visual-Tracking"><a href="#Siamese-Box-Adaptive-Network-for-Visual-Tracking" class="headerlink" title="Siamese Box Adaptive Network for Visual Tracking"></a>Siamese Box Adaptive Network for Visual Tracking</h2><p><a href="https://arxiv.org/abs/2003.06761">论文</a> <a href="https://github.com/hqucv/siamban">代码</a> <a href="https://www.bilibili.com/read/cv5400217">解读</a></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426112506.png" alt=""></p>
<p>同样是FCOS的应用，比较insight的地方是打标签的时候使用椭圆标签，两个椭圆，小椭圆E2内的点是positive，大椭圆E1外的点是negative，两个椭圆中间的部分为ignore。椭圆标签能够更紧凑地标注正负样本，并且设置了缓冲(ignore)以忽略模棱两可的样本。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426112854.png" style="zoom:67%;" /></p>
<hr>
<h2 id="Fully-Conventional-Anchor-Free-Siamese-Networks-for-Object-Tracking"><a href="#Fully-Conventional-Anchor-Free-Siamese-Networks-for-Object-Tracking" class="headerlink" title="Fully Conventional Anchor-Free Siamese Networks for Object Tracking"></a>Fully Conventional Anchor-Free Siamese Networks for Object Tracking</h2><p><a href="https://www.researchgate.net/publication/335467780_Fully_Conventional_Anchor-Free_Siamese_Networks_for_Object_Tracking">论文</a></p>
<p>将FCOS与级联结构结合，另一个就是分配GT到AFPN层时采用了FCOS一样的思路（划分[0,64], [64,128], [128,∞]）</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426114147.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426114239.png" style="zoom: 50%;" /></p>
<hr>
<h2 id="Ocean-Object-aware-Anchor-free-Tracking"><a href="#Ocean-Object-aware-Anchor-free-Tracking" class="headerlink" title="Ocean: Object-aware Anchor-free Tracking"></a>Ocean: Object-aware Anchor-free Tracking</h2><p><a href="https://arxiv.org/abs/2006.10721">论文</a> <a href="https://github.com/researchmm/TracKit">代码</a> <a href="https://www.bilibili.com/read/cv6615313">解读</a></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426115451.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426115537.png" alt=""></p>
<ul>
<li>anchor-base方法对于弱预测的修正能力较差，因为训练时只考虑了IOU大于阈值的anchor的回归，对于跟踪过程中如果出现overlap很小的anchor很难去refine。而anchor-free可以针对每个点进行预测；</li>
<li>作者设计了一个feature alignment module来从预测框中学习object-aware feature（图2c），从而对物体尺度敏感；</li>
<li>特征融合上采用xy轴膨胀系数不同的膨胀卷积进行融合，不同膨胀的卷积可以捕获不同尺度的特征，提高最终融合特征的尺度不变性。</li>
</ul>
<hr>
<h1 id="CenterNet类"><a href="#CenterNet类" class="headerlink" title="CenterNet类"></a>CenterNet类</h1><h2 id="Siamese-Attentional-Keypoint-Network-for-High-Performance-Visual-Tracking"><a href="#Siamese-Attentional-Keypoint-Network-for-High-Performance-Visual-Tracking" class="headerlink" title="Siamese Attentional Keypoint Network for High Performance Visual Tracking"></a>Siamese Attentional Keypoint Network for High Performance Visual Tracking</h2><p><a href="https://arxiv.org/abs/1904.10128v2">论文</a></p>
<p>这篇将CenterNet和CornerNet结合到跟踪中，分别预测中心点和两个角点，以及运用了CBAM注意力机制强化上下文信息，应该是第一个将CenterNet/CornerNet用进来的，遗憾的是性能没有刷的很高。细节同样参照开头b站专栏。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426164747.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426164810.png" alt=""></p>
<hr>
<h2 id="Accurate-Anchor-Free-Tracking"><a href="#Accurate-Anchor-Free-Tracking" class="headerlink" title="Accurate Anchor Free Tracking"></a>Accurate Anchor Free Tracking</h2><p><a href="https://arxiv.org/abs/2006.07560">论文</a></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426165201.png" alt=""></p>
<p>这篇就是比较典型的CenterNet模式了，预测中心点，中心偏移以及宽高。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426165235.png" alt=""></p>
<p>作者另外设计了backbone，最后在VOT2018性能虽然比siamrpn++略低但是速度是它的3.9倍（136FPS v.s. 35FPS）。</p>
<hr>
<h2 id="Siamese-Keypoint-Prediction-Network-for-Visual-Object-Tracking"><a href="#Siamese-Keypoint-Prediction-Network-for-Visual-Object-Tracking" class="headerlink" title="Siamese Keypoint Prediction Network for Visual Object Tracking"></a>Siamese Keypoint Prediction Network for Visual Object Tracking</h2><p><a href="https://arxiv.org/abs/2006.04078">论文</a> <a href="https://github.com/ZekuiQin/SiamKPN">代码</a></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426171705.png" alt=""></p>
<p>这一篇将casscade的思想结合在centernet类的siamese跟踪器中，看上面图2结构已经很清晰了，KPN结构如下：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426171748.png" style="zoom:80%;" /></p>
<p>还有一个需要关注的就是每个stage训练的时候分类标签的高斯方差不一样，遵循的原则就是越高的stage峰值越收束。目的即随着级联的进行，监管信号越来越严格。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426171845.png" alt=""></p>
<hr>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="Correlation-Guided-Attention-for-Corner-Detection-Based-Visual-Tracking"><a href="#Correlation-Guided-Attention-for-Corner-Detection-Based-Visual-Tracking" class="headerlink" title="Correlation-Guided Attention for Corner Detection Based Visual Tracking"></a>Correlation-Guided Attention for Corner Detection Based Visual Tracking</h2><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Du_Correlation-Guided_Attention_for_Corner_Detection_Based_Visual_Tracking_CVPR_2020_paper.pdf">论文</a> <a href="https://www.bilibili.com/read/cv6647311">解读</a></p>
<p>作者为了解决跟踪中回归框估计不准确的问题，引入角点检测来得到更紧致的回归框。分析了之前一些角点检测方法在目标跟踪中无法取得好性能的原因，并提出了两阶段的correlation-guided attentional corner detection (CGACD)方法。第一阶段使用siamese网络得到目标区域的粗略ROI，第二阶段通过空间和通道两个correlation-guided attention来探索模板和ROI之间的关系，突出角点区域进行检测。速度可以达到70FPS。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426173132.png" alt=""></p>
<hr>
<h2 id="RPT-Learning-Point-Set-Representation-for-Siamese-Visual-Tracking"><a href="#RPT-Learning-Point-Set-Representation-for-Siamese-Visual-Tracking" class="headerlink" title="RPT: Learning Point Set Representation for Siamese Visual Tracking"></a>RPT: Learning Point Set Representation for Siamese Visual Tracking</h2><p><a href="https://arxiv.org/abs/2008.03467">论文</a> <a href="https://github.com/zhanght021/RPT">代码</a> <a href="https://zhuanlan.zhihu.com/p/257854666">原作者解读</a></p>
<p>现有的跟踪方法往往采用矩形框或四边形来表示目标的状态（位置和大小），这种方式忽略了目标自身会变化的特点（形变、姿态变化），因此作者受启发自Reppoints检测方法，采用表示点（Representative Points）方法来描述目标的外观特征，学习表示点的特征，根据表示点的分布确定目标的状态，实现更精确的目标状态估计。</p>
<p>具体可以参考原作者在知乎的解读，该方法取得了VOT2020-ST的冠军。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426173720.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426173734.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426173759.png" style="zoom:80%;" /></p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>综述</tag>
        <tag>Anchor-free</tag>
      </tags>
  </entry>
  <entry>
    <title>Transform与目标跟踪</title>
    <url>/tracking/33-transform-tracking/</url>
    <content><![CDATA[<p>Transform在视觉领域遍地开花，终于目标跟踪也没能逃过。并行的长距离依赖（空间和时间皆可）对于目标跟踪似乎有着天然的优势，本篇笔记简要概述今年CVPR2021关于Transform在目标跟踪中的应用，主要介绍动机和结构，细节和实验部分以后有空再补充。</p>
<span id="more"></span>
<p>论文列表：</p>
<ul>
<li>Transformer Tracking</li>
<li>Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking</li>
<li>Learning Spatio-Temporal Transformer for Visual Tracking</li>
<li>Target Transformed Regression for Accurate Tracking</li>
</ul>
<h1 id="Transformer-Tracking"><a href="#Transformer-Tracking" class="headerlink" title="Transformer Tracking"></a>Transformer Tracking</h1><p><a href="https://arxiv.org/abs/2103.15436">论文</a><br><a href="https://github.com/chenxin-dlut/TransT">代码</a><br><a href="http://naotu.baidu.com/file/0348e011f8d04a784134e3329a328076?token=53a180e3aa5251ab">代码框架解析</a></p>
<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>跟踪中常用的correlation存在问题：<br>是一个局部线性匹配过程，没有利用全局上下文，容易陷入局部最优；<br>得到的相似图丢失一定程度的语义信息，导致对目标边界预测不准。</p>
<p>利用transform的attention有效融合模板特征和ROI特征，相比correlation能产生更多的语义特征。作者提出了基于self-attention的ego-context augment module (ECA)和基于cross-attention的cross-feature augment module (CFA)<br><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/1.png" style="zoom:80%;" /></p>
<h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p>重复N=4次fusion layer最后再接一个CFA<br><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/2.png" alt=""></p>
<center>整体跟踪框架</center>

<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/3.png" style="zoom:80%;" /></p>
<center>ECA和CFA结构</center>

<p>transform工作过程</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/4.png" style="zoom:80%;" /></p>
<ul>
<li>n=1 self search 没有来自模板的信息，因此会看到所有目标，而self template关注模板的关键信息（蚂蚁上的红点）；cross search和template同时具有目标和搜索的特征，因此可以更关注重要信息；</li>
<li>n=2 每一个attention输入都同时包含目标和搜索特征，self search对相似物的响应被抑制了，而cross search此时非常确信其预测。template的注意力此时开始关注目标边界；</li>
<li>n=3 进一步强化，模板特征成为包含大量目标边界信息的信息库，而搜索区域特征保留了目标的空间信息；</li>
<li>n=4 模板的分布变得混乱，这可能是因为，在目标确定之后，模板分支的特征不再需要保留模板本身的信息，而是存储了大量目标的边界信息，成为一个为回归服务的特征库。</li>
</ul>
<hr>
<h1 id="Transformer-Meets-Tracker-Exploiting-Temporal-Context-for-Robust-Visual-Tracking"><a href="#Transformer-Meets-Tracker-Exploiting-Temporal-Context-for-Robust-Visual-Tracking" class="headerlink" title="Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking"></a>Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking</h1><p><a href="https://arxiv.org/abs/2103.11681">论文</a><br><a href="https://github.com/594422814/TransformerTrack">代码</a></p>
<h2 id="动机-1"><a href="#动机-1" class="headerlink" title="动机"></a>动机</h2><p>现有的跟踪器常常忽略连续帧之间的 temporal contexts</p>
<ol>
<li><p>单帧独立检测方法： 对时域信息的利用只有运动先验（余弦窗）</p>
</li>
<li><p>模型更新方法：视频帧是独立的，没有相互推理关系；噪声会污染模型更新</p>
</li>
</ol>
<p>transform中的注意机制，能够建立跨帧的像素对应关系，在时间域内自由传递各种信号。</p>
<p>本文将各个独立的视频帧进行桥接，并通过 transformer 架构来探索它们之间的 temporal contexts，以实现鲁棒的目标跟踪。与经典的 transformer 的结构不同，作者将其编码器和解码器分离成两个平行的分支，并在 Siamese-like 跟踪管道中对其精心设计。</p>
<h2 id="结构-1"><a href="#结构-1" class="headerlink" title="结构"></a>结构</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/5.png" style="zoom:80%;" /></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/10.png" style="zoom:80%;" /></p>
<p>编码器通过基于注意力的特征强化来促进目标模板，有利于高质量的跟踪模型生成；</p>
<p>解码器将之前模板中的跟踪线索传播到当前帧，有利于目标搜索过程。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/6.png" style="zoom:80%;" /></p>
<p>与经典transform结构的差异：</p>
<ol>
<li>Encoder-decoder Separation. 没有将编码器和解码器级联，而是将编码器和解码器分离为两个分支，以适应Siamese-like跟踪方法；</li>
<li>Block Weight-sharing. 编码器和解码器中的self-attention(图4中的黄色方框)共享权值，将模板和搜索转换到同一特征空间中，便于进一步cross-attention；</li>
<li>Instance Normalization. 将Layer Norm换成Instance Norm；</li>
<li>Slimming Design. 移除FFN，并且使用single-head attention。</li>
</ol>
<p>图4编码器解码器结构细节：</p>
<p>编码器： 输入模板特征 $T \in [N_T, C], N_T=n \times H \times W $, $n$为模板数量；</p>
<p>解码器： 输入搜索特征 $S \in [N_S, C], N_S=H \times W $</p>
<p>高斯Mask     $M \in [N_T, 1] $</p>
<p>Mask Transformation 关注空间注意力，Feature Transformation 关注上下文信息</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/7.png" style="zoom:80%;" /></p>
<h3 id="跟踪框架"><a href="#跟踪框架" class="headerlink" title="跟踪框架"></a>跟踪框架</h3><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/8.png" style="zoom:80%;" /></p>
<ul>
<li><p>Siamese框架将编码器特征crop后和解码器特征做相关；</p>
</li>
<li><p>DCF框架用编码器特征训练Dimp的kernel，作用于解码器特征；</p>
</li>
<li><p><center>将上一帧中所有目标框的最小外接矩形bm扩大一定倍率得到搜素区域bs。<center></p>
</li>
<li><p>模板池每5帧更新一次，先入先出。</p>
</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>严重（完全）遮挡，出视野，高计算量</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/9.png" style="zoom:80%;" /></p>
<hr>
<h1 id="Learning-Spatio-Temporal-Transformer-for-Visual-Tracking"><a href="#Learning-Spatio-Temporal-Transformer-for-Visual-Tracking" class="headerlink" title="Learning Spatio-Temporal Transformer for Visual Tracking"></a>Learning Spatio-Temporal Transformer for Visual Tracking</h1><p><a href="https://arxiv.org/abs/2103.17154">论文</a><br><a href="https://github.com/researchmm/Stark">代码</a></p>
<h2 id="动机-2"><a href="#动机-2" class="headerlink" title="动机"></a>动机</h2><p>卷积只处理空间或时间上的局部关系，不擅长建立长距离的全局依赖关系。因此在面对目标发生较大形变或频繁进出视野时容易失败。另外，当前的方法将空间和时间分离处理，并没有明确建模空间和时间之间的关系。</p>
<p>考虑到transform在建模全局依赖方面的优势，作者利用它整合空间和时间信息进行跟踪，生成判别的时空特征用于目标定位。</p>
<p>编码器对目标对象和搜索区域之间的全局时空特征依赖关系进行建模，而解码器学习一个查询嵌入来预测目标对象的空间位置。该方法将目标跟踪作为一个直接的边框预测问题（角点预测），没有后处理。</p>
<h2 id="结构-2"><a href="#结构-2" class="headerlink" title="结构"></a>结构</h2><h3 id="Baseline-spatial-only"><a href="#Baseline-spatial-only" class="headerlink" title="Baseline (spatial-only)"></a>Baseline (spatial-only)</h3><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/11.png" style="zoom:80%;" /></p>
<p><strong>编码器</strong>输入将模板和搜索特征拉平拼接；</p>
<p><strong>解码器</strong>中query可以注意到模板和搜索区域的所有位置的特征，从而学习鲁棒表示，以进行边框预测；</p>
<p><strong>预测头</strong>将Encoder输出中的搜索特征和decoder输出经过图3的结构，通过概率预测两个角点，最后输出唯一的框，用L1和IOU loss优化。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/12.png" style="zoom:80%;" /></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/13.png" style="zoom:80%;" /></p>
<h3 id="Spatio-Temporal-Transformer-Tracking"><a href="#Spatio-Temporal-Transformer-Tracking" class="headerlink" title="Spatio-Temporal Transformer Tracking"></a>Spatio-Temporal Transformer Tracking</h3><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/14.png" style="zoom:80%;" /></p>
<p>相比baseline的改变：三元输入、增加分数预测头、训练&amp;推理策略</p>
<p>训练分为两阶段：第一阶段不训练score head，搜索图像全部包含目标；第二阶段固定其他参数单独训练score head，搜索图像中有一半不包含目标（训练时只要搜索图像包含目标则认为可以更新）；</p>
<p>推理时达到更新间隔且分数大于阈值更新模板</p>
<h3 id="本文结构与DETR的区别"><a href="#本文结构与DETR的区别" class="headerlink" title="本文结构与DETR的区别"></a>本文结构与DETR的区别</h3><ol>
<li>任务不同，检测vs跟踪</li>
<li>输入不同，detr输入整个图像，本文输入三元组，一个search和两个template；</li>
<li>query和训练策略，detr有100个query并且每个都需要匈牙利匹配gt，而本文只有一个query和唯一gt；</li>
<li>预测头不同，detr三层感知器，本文基于角点预测</li>
</ol>
<hr>
<h1 id="Target-Transformed-Regression-for-Accurate-Tracking"><a href="#Target-Transformed-Regression-for-Accurate-Tracking" class="headerlink" title="Target Transformed Regression for Accurate Tracking"></a>Target Transformed Regression for Accurate Tracking</h1><p><a href="https://arxiv.org/abs/2104.00403">论文</a><br><a href="https://github.com/MCG-NJU/TREG">代码</a></p>
<h2 id="动机-3"><a href="#动机-3" class="headerlink" title="动机"></a>动机</h2><p>如何将目标信息整合到回归分支中，<strong>保留精确的边界信息</strong>并<strong>及时处理各种目标变化</strong>对于跟踪是至关重要的。</p>
<p>dw-corr将整个目标当成滤波器，只有目标的全局信息，面对物体变形时难以准确反映边界；</p>
<p>pix-corr忽略了目标模板中的少量背景会对目标外部区域赋予较大的注意力权重。</p>
<p>作者利用transform的交叉注意力来建模模板和搜索区域的每个元素之间的pair-wise关系，并用其增强原始特征。这种特征表达能够增强目标相关信息，帮助精确定位边界，并由于其局部和密集匹配机制，在一定程度上适应目标变形。</p>
<p>此外，设计了一个简单的在线模板更新机制来选择可靠的模板，提高了对目标外观变化和几何变形的鲁棒性。</p>
<h2 id="结构-3"><a href="#结构-3" class="headerlink" title="结构"></a>结构</h2><p>设计准则：</p>
<ol>
<li>目标集成模块，保留充足的目标信息以生成精确目标边界；</li>
<li>像素级的上下文建模，以增强目标相关的特征和处理形变；</li>
<li>高效的在线机制，以处理连续序列中的外观变化。</li>
</ol>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/15.png" alt=""></p>
<center>TREG整体结构，核心是黄色的target-aware transformer，其余结构参考FCOT</center>

<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/16.png" alt=""></p>
<center>Online Target-aware Transformer for Regression. (a) Target-aware transformer (b) Online template update mechanism</center>

<p>将搜索特征看成query，目标被编码成key和value，对每一个query，都利用所有key和value为其提供加权聚合响应。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/17.png" style="zoom:80%;" /></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/18.png" style="zoom:80%;" /></p>
<p>$x_i$是搜索特征，$t_j$是目标特征，$Ω_k$表示目标模板的所有位置，$k$表示模板池的序号；</p>
<p>$\theta<em>{x_i}, \phi</em>{t<em>j}, \omega</em>{t_j}$ 分别表示 query, key, value；</p>
<p>注意这里归一化使用1/N而不是softmax。</p>
<blockquote>
<p>The reason lies in that some positions in background and distractors of the search region are expected to have low dependency with target, while Softmax function will amplify this noise influence as the sum of attention weights between the query and all the keys is always 1.</p>
</blockquote>
<p>在线更新模板，构建模板序列，包含3个静态模板和4个动态模板，静态的由第一帧变换增广生成，动态的取每n帧中得分最高的。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/20.png" style="zoom:80%;" /></p>
<h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/19.png" style="zoom:80%;" /></p>
<p>图4展示物体在序列发生了变化，本文的transform增强了目标包括头部和脚在内的边界。</p>
<p>表1 TAT-Cls表示将transform用于分类，效果稍微下降，因为pixel-to-pixel的匹配方法往往忽略了目标的整体信息，不适合区分相似的对象。</p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>本文的结构和CVPR2021另外一篇文章也有些类似，即Graph Attention Tracking，可以参考我在b站的<a href="https://www.bilibili.com/read/cv8692025">笔记</a>。作者将模板和搜索特征的每个位置看成节点，使用图注意力构建局部密集的匹配关系用于加强原始特征。实现方式也和transform的交叉注意力类似，可以说是殊途同归。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>综述</tag>
        <tag>transform</tag>
        <tag>CVPR2021</tag>
      </tags>
  </entry>
  <entry>
    <title>Real-Time Visual Object Tracking via Few-Shot Learning</title>
    <url>/tracking/35-FSL-tracking/</url>
    <content><![CDATA[<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427173306.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2103.10130">论文</a></p>
<p>跟踪可以看成是一个特殊的 few-shot learning (FSL) 问题，本文提出了一个通用的两阶段框架，它能够使用大量的FSL算法并且保持较快的速度。第一阶段通过SiamRPN生成若干潜在候选框，第二阶段通过少样本分类的思想对候选框进行分类。按照这种coarse-to-fine结构，第一阶段为第二阶段提供稀疏的样本，在第二阶段可以更方便、高效地进行多种FSL算法。作者选取了几种基于优化的少样本学习方法进行证明。此外，该框架可将大多数FSL算法直接应用到视觉跟踪中，使研究人员能够在这两个领域相互交流。</p>
<span id="more"></span>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>跟踪任务要求在有限的时间内通过少量数据学习对目标和背景的分类，这与FSL任务非常相似。在少样本学习中，我们假设训练任务和新任务之间存在共享的元知识；在视觉跟踪领域，这可以解释为模型在序列中跟踪任何未见过对象的适应性。</p>
<p>已经有一些方法将FSL的概念引入跟踪了，比如DiMP和一些MAML的方法，它们将在线更新纳入离线训练阶段作为内环 (inner loop)，使得在线更新可以由手工设计转变成数据驱动。然而，这些方法大多局限于对<strong>整个图像</strong>的<strong>特定卷积核</strong>进行优化设计，而不是像在FSL中，使用<strong>稀疏样本</strong>进行<strong>更定制化的权值学习</strong>(例如矩阵乘法因子)，这限制了直接引入各种新的FSL算法，因为直接应用各种FSL算法，将整个图像的所有位置作为输入样本，必然会牺牲其跟踪速度，而且大量简单负样本会在学习中占主导导致模型判别力下降。</p>
<p>因此作者提出这个通用的两阶段级联结构，在第一阶段过滤掉大量简单负样本，从而使得第二阶段可以应用各种FSL算法在信息丰富的稀疏样本上实现高效的跟踪。作者选取了几种具有不同目标函数，优化方法，或解空间的少样本学习方法进行验证，速度在40-60 FPS。</p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Cascaded-Tracking"><a href="#Cascaded-Tracking" class="headerlink" title="Cascaded Tracking"></a>Cascaded Tracking</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427203740.png" alt=""></p>
<p>整体跟踪框架如图1所示，第一阶段利用SiamRPN++生成若干个候选框，经过ROI Align后送入第二阶段进行少样本分类。第二阶段是一个N-shot-2-way的分类任务，所有候选使用在线生成的伪标签进行标记。</p>
<h2 id="Second-Stage-as-Few-Shot-Learning"><a href="#Second-Stage-as-Few-Shot-Learning" class="headerlink" title="Second Stage as Few-Shot Learning"></a>Second Stage as Few-Shot Learning</h2><p>首先用数学形式定义任务，公式比较多直接贴图：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427205448.png" style="zoom:80%;" /></p>
<p>其中 $\theta$ 是meta-training stage (inner loop)中使用base loss $ L<em>{base} $ 优化得到的sequence-specific参数；然后用$\theta$ 去计算meta loss $ L</em>{meta} $，用于 meta-testing (outer loop) stage 来更新few-shot learner 的参数 $\rho$ 。而$\varphi $ 表示feature embedding的参数，就是前面一大堆特征提取之类的。</p>
<p>这里的meta loss是用于分类，所以采用focal loss：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427211809.png" style="zoom:80%;" /></p>
<p>其中 $\gamma$ 是可学习的缩放因子，$\theta^T$ 是前景和背景的权重。$\alpha, \beta$ 是focal loss的超参数。</p>
<p>few-shot learner $\Lambda$ 的选择对于公式1的影响巨大，它会被用来分类第一阶段得到的稀疏候选样本，下面介绍几种常用的基于优化（optimization-based）的方法。</p>
<h2 id="Optimization-Based-Few-Shot-Learners"><a href="#Optimization-Based-Few-Shot-Learners" class="headerlink" title="Optimization-Based Few-Shot Learners"></a>Optimization-Based Few-Shot Learners</h2><p>首先默认目标是一个线性凸优化问题。</p>
<p><strong>RR-prim-itr</strong> 就是在原空间求解岭回归问题，base loss $ L_{base} $ 为L2 loss，可以看成MAML的一种特例。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427215927.png" style="zoom:80%;" /></p>
<p>其中 $\omega_n$ 是每个样本的权重，为了加速优化，借鉴DiMP中的最速梯度下降：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427220212.png" style="zoom:80%;" /></p>
<p><strong>RR-dual-itr</strong> 迭代求解对偶空间中的岭回归，$\theta$ 当成训练集的特征向量的线性组合。对偶变量 $a$ 作为权重因子。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427220526.png" style="zoom:80%;" /></p>
<p>将公式5带入公式3，得到优化目标</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427220643.png" style="zoom:80%;" /></p>
<p>用二次规划的方式求解对偶变量 $a$ ，同样可以利用GPU加速。此外，特征向量在训练集中进行线性组合可以缓解过拟合问题。</p>
<p><strong>RR-dual-cls</strong> 在对偶空间中开发了岭回归的封闭解用于分类。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427221514.png" style="zoom:80%;" /></p>
<p>优势就是把原始闭式解形式中的 $ \Phi_T\ \Phi$ 变成了 $ \Phi \Phi_T$，使得计算量由 $O(Nd^2)$ 下降到了$O(N^2d)$ ，样本量N远小于特征维数d。</p>
<p><strong>SVM-dual-itr</strong> 在对偶空间迭代求解稀疏核用于线性分类。主要解决岭回归容易过拟合以及对噪声不鲁棒的问题。用SVM替换了最小二乘，细节不太明白，要去看原文。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427222444.png" style="zoom:80%;" /></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427222555.png" style="zoom:80%;" /></p>
<h2 id="Online-Tracking"><a href="#Online-Tracking" class="headerlink" title="Online Tracking"></a>Online Tracking</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427230231.png" alt=""></p>
<p><strong>Candidate Selection</strong> 候选框根据第一阶段的检测结果用阈值为0.2的NMS进行筛选，再通过ROI Align作用于第二阶段。初始化时选择24个样本，IOU最高的作为正样本，其余为负样本，并通过数据增广生成额外的8个正样本，共同用于初始化few-shot learner。跟踪过程中，选择NMS后M=8个样本送入few-shot learner进行判断输出最终结果。用融合得分（两个阶段的得分加权相加）top-k个候选框更新support set中最老的样本，其中最高的为正样本，其余为负样本，k=4。</p>
<p><strong>Support Set Maintenance</strong> 先入先出（FIFO），对于每个样本赋予权重 $\omega$，随着与当前帧的间隔而指数衰减。原空间中的求解存储1000个样本，对偶空间只有60个，一方面是对CPU资源要求高，另外60个的性能已经很好。此外，第一帧的正样本永远会留在support set中。</p>
<p><strong>Few-Shot Learner Update</strong> 原空间用公式4更新，对偶空间用滑动平均更新</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427231851.png" style="zoom:80%;" /></p>
<p><strong>Stage Fusion</strong> 将两个阶段的分类和回归结果融合，第二阶段除了得到分类得分，还会进行类似RCNN的回归refine。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427232137.png" style="zoom:80%;" /></p>
<h1 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h1><p>训练类似DiMP，每个task从一个视频中选择3个support和2个query。对于第二阶段，NMS后采样16个样本，IOU大于0.8为正，小于0.2为负。每张图片总共8个样本，其中最多两个正样本。meta-training和meta-test均按照上述准则。第一阶段的SiamRPN++用ATSS进行label assign。</p>
<p>训练loss如下：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427233344.png" style="zoom:80%;" /></p>
<p>两个阶段都需要分类回归，分类为focal loss，回归为L1 loss。注意这里rcnn的回归是在第一阶段的正样本基础上进行的。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427234324.png" alt=""></p>
<p>比较了baseline（第一栏），不使用FSL的基于全连接的距离度量方法（第二栏），metric-based少样本学习方法（第三栏），以及本文使用的四种optimization-based少样本学习方法（第四栏）。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427234748.png" alt=""></p>
<p>只用第一阶段方法，SiamRPN++远不如DiMP；加入第二阶段后，这个差距被消除了，且速度将近领先了3倍。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427234945.png" alt=""></p>
<p>support set数量的影响，对于不同数据集最佳的数量是不一样的。但是即使是最低限度的M=40鲁棒性依然不错。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210428093340.png" alt=""></p>
<p>最后可视化了候选框及置信度，可以看到第二阶段对于区分目标和干扰物非常有效。</p>
<h2 id="Comparison-with-State-of-the-Arts"><a href="#Comparison-with-State-of-the-Arts" class="headerlink" title="Comparison with State-of-the-Arts"></a>Comparison with State-of-the-Arts</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427235535.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427235554.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427235609.png" alt=""></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文的创新点主要集中在两阶段和少样本学习。这两个东西单独来看都分别有人做过了，像SPM，SPLT就是先检候选框再分类的两阶段方法；而DiMP还有一些用了MAML的跟踪方法都属于少样本学习。作者将二者结合，最大的好处在于第二阶段得到的稀疏样本更适合FSL的任务设置，可以不用局限于任何特定类型的FSL算法，大幅加强了跟踪和FSL的联系。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>FSL</tag>
        <tag>arxiv</tag>
      </tags>
  </entry>
  <entry>
    <title>Learning Target Candidate Association to Keep Track of What Not to Track</title>
    <url>/tracking/36-learning-target-candidate-association/</url>
    <content><![CDATA[<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210428110851.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2103.16556">论文</a> <a href="https://github.com/visionml/pytracking">代码</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>Martin参与的新作，出发点和之前的<a href="https://arxiv.org/abs/2003.11014">KYS</a>类似，均指出仅外观模型不足以区分目标和干扰物，因此需要对所有潜在目标保持跟踪。不同的是KYS是通过一个传播模块隐式地跟踪所有对象，最后作用于外观模型的输出来抑制干扰响应；而本文则是借助SuperGlue显式地匹配帧间所有的候选对象，构建跟踪链，有点多目标跟踪的意思，可解释性也更强。</p>
<ul>
<li>主流跟踪方法大多聚焦于建立强大的外观模型，然而仅依靠外观模型对于干扰物的鲁棒性较差；</li>
<li>作者提出另一种思路，即对干扰物也保持跟踪。为此构建一个可学习的关联网络（受启发自SuperGlue），允许在帧与帧之间传播所有候选目标；</li>
<li>针对跟踪数据集没有对干扰物标注的问题，提出了一种结合部分标注和自监督的训练策略。</li>
</ul>
<span id="more"></span>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210428113928.png" alt=""></p>
<p>整体框架如上图，base tracker使用SuperDiMP预测出分数图$s$，选择高得分的几个位置用于生成目标候选 $v_i$。然后对每个候选提取一系列特征，包括分类分数$s_i$，位置$c_i$，和 backbone提取的外观模型$f_i$，将这些特征编码编码成一个单独的特征向量$z_i$。将当前帧和前一帧的所有候选送入candidate embedding network，一起处理得到每个候选对象的丰富嵌入$h_i$。最后利用这些特征来计算相似矩阵S，并使用最优匹配策略估计两个连续帧之间的候选分配矩阵A。</p>
<p>有了这个分配概率矩阵，就可以建立当前帧的所有候选目标$O$与上一帧所有识别出来的目标$O’$之间的关联，包括消失和新出现的目标。用这种传播策略来推断当前帧中的真实目标对象$\hat{o}$。此外，在在线更新目标分类器时，通过计算目标检测置信度$\beta$来管理存储和控制样本权重。</p>
<h2 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h2><p>首先将问题公式化，定义候选目标集合 $V = {v<em>i} </em> {i=1}^N$，N为候选个数，$V’$和$V$分别表示前一帧和当前帧的候选集合。两帧的候选关联问题就是寻找$V’$和$V$之间的分配矩阵$A$。若$v’ <em> {i}$和$v</em>{j}$关联，那么$A<em>{i,j}=1$，否则$A</em>{i,j}=0$。</p>
<p>实际操作中，并不是每个候选都能找到相应的匹配，因此引入dustbin来处理未匹配的节点。大致的想法就是额外增加一行一列来存放未匹配的节点。比如候选$v<em>j$只在集合$V$中出现，则$A</em>{N’+1,j}=1$；类似的，若$v’ <em> i$ 在集合$V$中没有匹配对象，则$A</em>{i,N+1}=1$。</p>
<p>作者设计了一种可学习方法来预测这个分配矩阵A，首先需要提取候选对象的特征表示，接下来讨论。</p>
<h2 id="Target-Candidate-Extraction"><a href="#Target-Candidate-Extraction" class="headerlink" title="Target Candidate Extraction"></a>Target Candidate Extraction</h2><p>目标候选对象需要满足两个条件，响应得分是局部最大，且要超过一定阈值。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210428170210.png" style="zoom:;" /></p>
<p>实现时使用$5 \times 5$ max-pooling找到局部极值，阈值$\tau=0.05$。</p>
<p>接下来为目标候选构建特征编码</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210428170144.png"  /></p>
<p>包括响应得分$s_i$，位置$c_i$，和 backbone在该位置提取的外观模型$f_i$。$\psi$ 表示MLP，将 s 和 c 变换到和 f 一样的维度。</p>
<h2 id="Candidate-Embedding-Network"><a href="#Candidate-Embedding-Network" class="headerlink" title="Candidate Embedding Network"></a>Candidate Embedding Network</h2><p>为了进一步丰富编码特征，特别是便于提取特征的同时又能识别邻近的候选特征，作者引入了候选嵌入网络。这里借鉴了SuperGlue中的self-attention和cross-attention交换不同节点的信息。最后经过一个线性变换，得到每个候选$v_i$的编码$h_i$。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210428171233.png" alt=""></p>
<h2 id="Candidate-Matching"><a href="#Candidate-Matching" class="headerlink" title="Candidate Matching"></a>Candidate Matching</h2><p>用特征点积来度量候选$v’ <em> {i} \in V’$和$v_j \in V$之间的相似性，$S</em>{i,j}=&lt;h’_i,h_j&gt;$，h是对应的特征编码。</p>
<p>有了相似性得分矩阵S，接下来需要构建分配矩阵A，通过最大化总体得分$\Sigma<em>{i,j} S</em>{i,j} A_{i,j} $ 可得到A，这是一个最优传输问题。</p>
<p>在此之前，还需要考虑dustbin，它是一个虚拟概念，没有对应的特征编码h，因此不能直接预测相似性得分。只有当一个候选对象与所有其他候选对象的相似度分数足够低时，它才属于dustbin。上面得到的相似度矩阵S仅代表了不考虑dustbin的候选对象之间的初始关联预测。本文没有提及如何处理dustbin的相似性得分，参照SuperGlue是给所有dustbin赋予一个相同的可学习参数。</p>
<p>下面就是给这个最优匹配问题设计约束条件：</p>
<ul>
<li>当$v’ <em> i$和$v_j$匹配时，需要同时满足 $\Sigma</em>{i=1}^{N’} A<em>{i,j}=1$ 和 $\Sigma</em>{j=1}^{N} A_{i,j}=1$，确保一对一的匹配，即(i, j)所在行和列仅有唯一的匹配位置；</li>
<li>所有未匹配到其他候选的候选必须匹配到dustbin，数学表达为$\Sigma<em>{j} A</em>{N’+1,j}=N-M$ 和$\Sigma<em>{i} A</em>{i,N+1}=N’-M$，其中$M = \Sigma<em>{(i\le N’,j \le N)} A</em>{i,j}$表示候选之间匹配上的数量。</li>
</ul>
<p>最后通过Sinkhorn算法（迭代）解出最优分配矩阵。</p>
<p>这一套流程也是借鉴了SuperGlue，这里贴出它的示意图。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210428181327.png" alt=""></p>
<h2 id="Learning-Candidate-Association"><a href="#Learning-Candidate-Association" class="headerlink" title="Learning Candidate Association"></a>Learning Candidate Association</h2><p>上述流程的训练需要知道所有候选之间的匹配关系作为标签，但跟踪数据集只有唯一目标的标注。所以作者提出了部分监督和自监督结合的方式。</p>
<p><strong>部分监督损失：</strong>只对有标注的那对目标候选计算loss，另外为了模拟遮挡和重检测，人为地排除一些候选并将其替换为dustbin</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210428182311.png"  /></p>
<p>其中$(l’,l)=(i,j)$，$(l’,l)=(N’+1,j)$, $(l’,l)=(i,N+1)$.</p>
<p><strong>自监督损失：</strong>自监督的一个候选$V$是由另外的候选$V’$增广变换而来的，因此它们可以构建一一对应关系$C={(i,i)}^N_{i=1}$，增广策略包括随机平移位置$c_i$，增减响应得分$s_i$，以及变换图像对特征$f_i$进行变换。此外，也像上面部分监督一样模拟了遮挡和重检测。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210428202146.png"  /></p>
<p>最后将二者相加得到最终loss。</p>
<p><strong>Data Mining:</strong> 有价值的训练样本是那些出现干扰物导致跟踪失败或者跟踪置信度很低的子序列，这样才能包含更多候选供网络学习。因此先使用base tracker跑一遍找到这些困难样本。</p>
<p><strong>Training Details:</strong> 先训练SuperDiMP（这里不训练在线分类的参数，可能是因为想要获取更多失败的样本），跑一遍数据找到难样本。然后冻结base tracker的参数只训练后半部分网络。</p>
<h2 id="Object-Association"><a href="#Object-Association" class="headerlink" title="Object Association"></a>Object Association</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210428205314.png" alt=""></p>
<p>在线跟踪时对所有潜在对象保持跟踪，并构建跟踪链，如图3所示。若当前的候选对象都与该对象没有关联，那么该对象就会从场景中消失。若有新的对象出现则新增链并赋予新的对象id。对于关联上的对象，将得分$s_i$添加到历史分数中。此外，当候选对应关系不确定，即分配概率小于$\tau=0.75$时，我们删除旧的对象并创建一个新的对象。</p>
<p>这样所有候选对象都与一个已经存在的或新创建的对象相关联，若上一帧检测到的真实目标在当前帧有关联对象，则暂定该关联对象是目标。为了避免该关联对象是干扰，还需将它的历史得分与当前其他候选对象的得分进行比较，如果另一个对象在当前帧中获得的分类分数高于当前选择对象在过去获得的所有分数，我们就选择这个对象作为目标；否则还是保持当前选择对象不变。</p>
<p>若上一帧检测到的真实目标在当前帧没有关联对象，则进行重检测，需满足：1. 分类得分最高；2. 分类得分大于阈值$\eta =0.25$。</p>
<p>整个算法流程如下图：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210429161216.png" alt=""></p>
<center>'.' 表示访问对象的属性</center>

<h2 id="Memory-Sample-Confidence"><a href="#Memory-Sample-Confidence" class="headerlink" title="Memory Sample Confidence"></a>Memory Sample Confidence</h2><p>base tracker在线更新的训练样本采用先入先出的策略进行替换，并仅基于存在的寿命时间对样本加权。本文则是将分类得分也考虑进来，计算在线优化损失时综合考虑了寿命时间和分类得分对样本进行加权：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210429165636.png"  /></p>
<p>$\alpha$ 表示样本存在的时间，越早的样本值越小；$\beta$表示分类得分。此外，训练样本更新时替换的是$\alpha \beta$最小的样本而不是简单替换最早的样本。</p>
<p><strong>Inference details</strong></p>
<p>base tracker搜索区域取决于当前估计的边界框大小。当目标物体被遮挡或出视野时，跟踪器通常只检测到目标的一小部分，并估计出比之前帧更小的边界框，相应的搜索区域也会变小，这对于跟踪失败后的重捕是不利的。因此，如果在目标丢失前搜索区域大幅缩小，我们将搜索区域重置为以前的大小，以便于重新检测。</p>
<p>此外，如果只有一个高分目标出现在前一帧和当前帧中。我们选择这个候选对象作为目标，省略运行目标候选关联网络来加速跟踪。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210429165050.png" alt=""></p>
<p>memory sample confidence 在LaSOT提升比较大，NFS和UAV123没啥影响，说明它在长时跟踪中的作用更大。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210429165254.png" alt=""></p>
<p>比较了提出的部分监督损失，自监督损失和数据挖掘的作用，每一项都有提升。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210429165349.png" alt=""></p>
<p>表3分析了学习在线分类器过程中加入分类得分的影响。第一列表示增加了分类得分来考虑替换训练样本，第二列表示在计算在线损失时增加分类得分对样本加权，第三列表示在线学习时忽略了得分较低的低质量样本。</p>
<h2 id="State-of-the-art-Comparison"><a href="#State-of-the-art-Comparison" class="headerlink" title="State-of-the-art Comparison"></a>State-of-the-art Comparison</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210429172159.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210429172214.png" alt=""></p>
<p>对LaSOT数据集，$T&lt;0.7$时本文方法指标明显更高，证明其鲁棒性。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210429172329.png" alt=""></p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>arxiv</tag>
      </tags>
  </entry>
  <entry>
    <title>Updatable Siamese Tracker with Two-stage One-shot Learning</title>
    <url>/tracking/38-siamtol/</url>
    <content><![CDATA[<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510102825.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2104.15049">论文</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>本文主要解决孪生跟踪器缺乏在线更新能力的问题。传统的线性模板更新难以处理目标的不规则变化和采样噪声，造成跟踪漂移；而一些像updatenet采用网络进行自适应更新的方法，其更新网络和跟踪器在结构上是分离的，不能从联合训练中受益，也不能以最佳方式合作。</p>
<p>为了实现高质量的自适应更新，作者从 one-shot learning的角度提出一个two-stage one-shot learner，利用不同阶段的目标样本预测分类器的参数。具体来说，除了使用模板分支来学习初始目标特征，作者额外增加了一个输入分支用于捕获后续帧中的目标特征，并设计了一个残差模块来使用这些特征更新初始模板。通过残差学习融合多帧目标特征，跟踪器可以用更合适的模板跟踪当前目标。此外，还设计了一种多方面(multi-aspect)的训练损失来避免过拟合。</p>
<span id="more"></span>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="One-shot-learning-formulation"><a href="#One-shot-learning-formulation" class="headerlink" title="One-shot learning formulation"></a>One-shot learning formulation</h2><p>SiamRPN指出孪生跟踪框架是一个one-shot learner，即通过初始帧的一次学习使得模型能够跟踪到后续帧中的目标，本节首先对其进行公式化定义。</p>
<p>目标跟踪的典型框架是判别分类器$\varphi (x,W)$，目标是在训练数据集上找到能使总损失L最小化的参数W：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510115919.png" alt=""></p>
<p>$n$表示训练样本数，$l_i$是样本$x_i$的标签。尽管分类器在目标跟踪上具有很强的竞争力，但需要大量计算量和样本在线训练学习。</p>
<p>另外一种跟踪框架是孪生网络，目标是学习模板和搜索区域之间的相似性度量：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510152537.png" alt=""></p>
<p>$z_i$和$x_i$分别表示模板和搜索区域，$\varphi ‘$和 $\zeta$ 分别表示特征提取网络和匹配网络。</p>
<p>进一步分析，我们发现公式2的孪生网络模型可以被重新解释为类似公式1的one-shot分类器模型。对于分类器$\varphi (x,W)$，若仅通过一个感兴趣样本$z_i$就能学到分类器参数$W$，那么这就是一个one-shot learner。因此，孪生网络的模板分支可以看成是一个元学习函数$\omega$，它将模板特征映射成分类器参数$W$；而搜索分支和互相关就是一个检测器，整个目标函数定义如下：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510154435.png" alt=""></p>
<p>至此，我们就把孪生框架解释为了one-shot learning，图1直观展示了这种表达方式</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510155519.png" alt=""></p>
<h2 id="Two-stage-One-shot-learner"><a href="#Two-stage-One-shot-learner" class="headerlink" title="Two-stage One-shot learner"></a>Two-stage One-shot learner</h2><p>孪生网络框架只能在初始帧这个stage通过one-shot learning学到目标信息，因此无法在线更新。那自然会想能不能让这个learner在不同的stage（跟踪阶段）去学习目标信息呢？因此作者提出了 two-stage one-shot learner (TOL)，可以结合具有不同属性的样本来预测分类器的参数$W$。目标函数定义如下：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510160306.png" alt=""></p>
<p>相比公式3，就是增加了一个来自后续帧中的样本$u_i$来学习$\omega$ 。</p>
<h2 id="Updatable-Siamese-Network"><a href="#Updatable-Siamese-Network" class="headerlink" title="Updatable Siamese Network"></a>Updatable Siamese Network</h2><p>基于上述的 two-stage one-shot learner，本文提出一个可更新的孪生网络，如下图</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510162506.png" alt=""></p>
<p>相比原始的孪生框架，增加了一个update分支，然后把两个分支的目标特征进行融合。其实抛开上面讲的那些one-shot learning也不妨碍理解这个结构，无非就是把初始帧和跟踪过程中的历史帧通过网络融合生成一个更好的模板以适应跟踪中的变化。</p>
<p>融合模板的过程（或者说元学习函数）定义如下：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510163640.png" alt=""></p>
<p>其中$zf$和$uf$分别表示模板和更新样本的特征，M表示特征融合网络（三层卷积）。</p>
<p><strong>Multi-aspect loss training</strong> 训练网络时在模板和搜索图像之间的间隔图像中额外扣一个更新样本，并且计算损失分别考虑了模板样本-搜索样本，更新样本-搜索样本，融合模板样本-搜索样本三方面损失，如图3所示。这样做是因为网络包括一个基本的孪生跟踪器和一个在线调整的更新器两部分，如果直接用一个整体损失训练，网络可能难以平衡这两部分。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510164536.png" alt=""></p>
<p><strong>Online update</strong> 在线跟踪过程中，更新样本每N=10帧更新一次，且满足置信度大于阈值0.9。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Comparison-with-the-state-of-the-arts"><a href="#Comparison-with-the-state-of-the-arts" class="headerlink" title="Comparison with the state-of-the-arts"></a>Comparison with the state-of-the-arts</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510165043.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510165257.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510165536.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510165551.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510165627.png" alt=""></p>
<h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510165730.png" alt=""></p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>FSL</tag>
        <tag>arxiv</tag>
        <tag>模型更新</tag>
      </tags>
  </entry>
  <entry>
    <title>Siamese Natural Language Tracker: Tracking by Natural Language Descriptions with Siamese Trackers</title>
    <url>/tracking/40-snlt/</url>
    <content><![CDATA[<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519110756.png" alt=""></p>
<p><a href="https://arxiv.org/abs/1912.02048">论文</a></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文研究的课题为tracking by natural language(NL) 。人类的学习过程是视觉和语言共同作用的，而在基于外观的跟踪过程中引入语言描述同样可以使得跟踪器更加精确、灵活和鲁棒（如图1的例子）。因此，本文将孪生跟踪器与语言描述结合，将语言描述编码成一个卷积核嵌入到孪生框架中（SNL-RPN），并将视觉和语言的预测进行动态聚合（Dynamic Aggregation），为tracking by NL任务提供了一个新的baseline。具体贡献总结如下：</p>
<ol>
<li>提出一种新的tracking by NL的baseline，Siamese Natural Language Region Proposal Network (SNL-RPN)；</li>
<li>提出了一种基于视觉和语言预测的动态聚合（Dynamic Aggregation），将SNL-RPN转换为Siamese Natural Language Tracker (SNLT)；</li>
<li>在NL标注的数据集上将孪生跟踪器的性能提升了3-7个百分点，并且性能超过其他NL tracker，速度为50FPS。</li>
</ol>
<span id="more"></span>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519113920.png" alt=""></p>
<center>图1 （摘自CVPR2021 TNL2K） (a) 只利用box时跟踪器可能会混淆跟踪目标是自行车还是人的腿，加入语言描述使得跟踪对象更加清晰；(b) 当目标发生剧烈形变时加入语言描述可以减少漂移；(c) 语言描述可以更灵活地指定目标，如这里需要跟踪持球的运动员，使用传统跟踪器达到这一效果需要反复重新初始化跟踪器。  </center>

<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519115620.png" alt=""></p>
<p>上图展示了SNLT整体框架，将语言描述嵌入到SiamRPN++中。输入包含三部分：模板、搜索区域和查询语言Q。模板和搜索区域经过卷积提取特征$Z$和$X$，查询语言经过语言模型（BERT, GloVe, HGLMM）提取句子嵌入编码$Z_Q$。三元组$(Z,Z_Q,X)$送入SNL-RPN，分别预测视觉和语言的分类和回归响应。最后二者通过Dynamic Aggregation Module进行融合。</p>
<h2 id="Architecture-of-the-SNL-RPN"><a href="#Architecture-of-the-SNL-RPN" class="headerlink" title="Architecture of the SNL-RPN"></a>Architecture of the SNL-RPN</h2><p>SNL-RPN如上图b所示，包括蓝色部分的visual head和红色部分的NL head，都是通过DW-XCorr生成相应的的分类和回归响应。类似SiamRPN++，同样在ResNet的三个stage上进行预测，并将结果融合，融合权重通过离线训练得到。visual head和NL head各包含两个分支，总共有四组融合参数：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519154224.png" alt=""></p>
<p>$S<em>{VIS}, S</em>{NL}$表示分类分支的视觉和语言预测，$B<em>{VIS},B</em>{NL}$表示回归分支的两种预测。</p>
<h2 id="Aggregation-of-the-SNL-RPN-Predictions"><a href="#Aggregation-of-the-SNL-RPN-Predictions" class="headerlink" title="Aggregation of the SNL-RPN Predictions"></a>Aggregation of the SNL-RPN Predictions</h2><p>上一节对不同stage的预测融合，本节作者对视觉和语言的结果再次进行融合：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519160117.png" alt=""></p>
<p>$\omega<em>{VIS},\omega</em>{NL}$ 可以简单像上面一样离线训练得到，但是融合对象来自两个不同的输入，用固定的权重融合并不是最优的。因此，作者提出一个基于预测响应图熵的动态融合方式：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519160727.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519160737.png" alt=""></p>
<p>其中$\sigma$表示softmax函数，$\alpha$表示缩放因子。公式4的含义是<strong>熵越大的项赋予更小的权重</strong>。熵越大表示预测图分布越混乱，越不准确，所以权重更小。</p>
<h2 id="Training-the-SNL-RPN-and-Loss-Functions"><a href="#Training-the-SNL-RPN-and-Loss-Functions" class="headerlink" title="Training the SNL-RPN and Loss Functions"></a>Training the SNL-RPN and Loss Functions</h2><p>训练设置和siamese框架一样，额外增加了一个语言模型。损失函数分别计算视觉和语言对应的分类回归损失。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519161412.png" alt=""></p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>训练数据集使用的是 MSCOCO, YouTube-BB, VisualGenome, LaSOT, OTB-99-LANG，其中后三个都是有语言标注的，前两个只有类别标注。</p>
<p>测试数据集使用有语言标注的OTB-99-LANG和LaSOT。</p>
<h2 id="Comparison-with-Visual-and-NL-Trackers"><a href="#Comparison-with-Visual-and-NL-Trackers" class="headerlink" title="Comparison with Visual and NL Trackers"></a>Comparison with Visual and NL Trackers</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519161835.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519161846.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519161952.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519162121.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519162142.png" style="zoom:80%;" /></p>
<p>表2最后一行表示只使用语义类别作为语言输入测试LaSOT（原始的LaSOT里语言描述是一句话），这样效果会下降，证明SNLT可以学到比语义类别更多的东西。</p>
<h2 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519162612.png" style="zoom:80%;" /></p>
<p>表1对比了不同语言模型的效果</p>
<p><img src="C:/Users/zjp/AppData/Roaming/Typora/typora-user-images/image-20210519162738919.png" alt="image-20210519162738919"></p>
<p>图6的两个例子，第一个展示了语言描述可以辅助跟踪器避免模型漂移，而第二个例子语言描述不能唯一地描述目标，导致跟踪器漂移。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519163102.png" style="zoom:80%;" /></p>
<p>图8展示了视觉和语言的分类响应图，NL head的响应更加准确，可能是由于遮挡阻碍了视觉模型。通过计算响应图的熵，赋予熵更小的NL响应图更大的权重。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>CVPR2021</tag>
        <tag>Natural Language</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习在目标跟踪中的应用</title>
    <url>/tracking/42-rl-tracking/</url>
    <content><![CDATA[<p>强化学习讨论的问题是智能体(agent) 如何在一个复杂不确定的环境(environment) 里去最大化它能获得的奖励。 今天介绍三篇关于强化学习在目标跟踪中的工作，分别利用强化学习来决策使用的特征，多个跟踪器的切换以及是否更新模板。</p>
<p>论文列表：</p>
<ul>
<li><a href="https://arxiv.org/abs/1708.02973">Learning Policies for Adaptive Tracking with Deep Feature Cascades</a></li>
<li><a href="https://proceedings.neurips.cc//paper/2020/file/885b2c7a6deb4fea10f319c4ce993e02-Paper.pdf">Online Decision Based Visual Tracking via Reinforcement Learning</a></li>
<li><a href="https://arxiv.org/abs/2004.07538">Fast Template Matching and Update for Video Object Tracking and Segmentation</a></li>
</ul>
<span id="more"></span>
<h1 id="强化学习方法"><a href="#强化学习方法" class="headerlink" title="强化学习方法"></a>强化学习方法</h1><p>强化学习包含环境, 动作和奖励三部分, 其本质是agent 通过与环境的交互, 使得其作出的action所得到的总奖励达到最大, 或者说是期望最大。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210608202931.png" style="zoom: 50%;" /></p>
<p>强化学习方法主要可以分为Value-Based，Policy-Based以及二者结合的Actor-Critic方法。Value-Based方法通过Temporal Difference (TD) Learning学习动作价值函数；Policy-Based方法通过Policy Gradient学习策略函数；而Actor-Critic方法将二者结合，actor学习一个策略来得到尽量高的回报，critic对当前策略的值函数进行估计，即评估actor的好坏。</p>
<p>关于强化学习的更多细节可以参考王树森老师的视频课程<a href="https://www.bilibili.com/video/BV12o4y197US">【王树森】深度强化学习(DRL)</a></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210608203415.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210608204616.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210608204711.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210608204741.png" alt=""></p>
<hr>
<h1 id="Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades"><a href="#Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades" class="headerlink" title="Learning Policies for Adaptive Tracking with Deep Feature Cascades"></a>Learning Policies for Adaptive Tracking with Deep Feature Cascades</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>第一篇来自ICCV2017，出发点是不同复杂程度的跟踪场景对特征的需求是不同的，对于简单场景使用浅层特征（甚至像素特征）就能处理，而对于一些复杂场景才需要具有更强语义信息的深度特征。这个<strong>自适应决策</strong>的问题可以通过基于Q-learning的强化学习完成，如图1所示，学习一个agent来判断当前特征是否已经可以以较高的置信度定位目标，还是需要继续计算更深层的特征来寻找目标。</p>
<p>这样对简单目标提前终止的策略可以大幅提升推理速度，相比baseline平均速度提升了大约10倍，GPU速度158.9FPS，并且在cpu上也能以23.2FPS的速度接近实时运行。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210629104958.png" alt=""></p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>首先定义一些公式符号，孪生网络每一层的互相关层定义：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210629121458.png" alt=""></p>
<p>其中$\varphi_l$ 表示第 $l$ 层的特征，$F_l$ 表示第 $l$ 层的互相关结果。</p>
<p>整体框架如图2所示，在每一层互相关结果$F_l$后面接一个Q-Net，用于判断是否在该层停止，或者调整预测框的形状并继续使用下一层特征。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210629121328.png" alt=""></p>
<p>agent采用强化学习的方式训练，基本元素包括状态(state S) ，动作(action A)和奖励(reward R)。在每一个step即第l层中，agent根据当前状态$S_l$ 采取动作$A_l$来决定是否调整预测框或者在该层停止并输出结果，动作$A_l$的目的是减少预测的框的不确定性。训练时根据预测框与GT的IOU给出相应的奖励$R_l$（有正有负），通过最大化期望奖励，agent能学到最好的决策来采取行动，在精度和效率上取得平衡。</p>
<p><strong>Actions:</strong> 包括7个各向异性的的尺度变换和一个stop动作。7个尺度变换里包括2个全局的缩放和4个局部缩放，缩放比例为0.2。还有一个不缩放(no scaling)的动作，这一操作用于在当前响应图不明确或无法做出决策时推迟决策。</p>
<p><strong>States:</strong> 状态是一个包含响应图$F’_l$和历史动作$h_l$的二元组 $(F’_l,h_l)$。$F’_l$使用的是当前层和之前所有层响应图的平均，相当于结合了浅层的细节和深层的语义。$h_l$包含历史4个动作的向量，每个动作是8维的one-hot的向量，所以$h_l$总共是32维。</p>
<p><strong>Rewards:</strong> 奖励函数$R(S<em>{l-1},S_l)$反应了采取动作$A_l$后，从状态$S</em>{l-1}$到状态$S_l$的定位精度提升（或下降），精度采用IOU衡量，奖励函数计算如下：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210629174619.png" style="zoom:80%;" /></p>
<p>当动作不为stop时，若该动作能使IOU增大，则奖励+1，否则惩罚-1。若采取任何尺度变换都不能进一步提升IOU或者已经到达最后一层了，则采取stop动作，此时以IOU阈值0.6来决定奖惩。</p>
<p><strong>Deep Q-learning</strong>：本文使用value-based的DQN来选择动作，该方法需要学习一个动作-价值函数$Q(S_l,A_l)$, 选择能够使得Q最大的动作A。Q函数用网络模拟，如图2虚线框所示，包含两个128维的FC层，输出对应8维动作的回报。训练时采用TD learning进行迭代。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210629181015.png" style="zoom:80%;" /></p>
<p>其中R表示当前奖励，$Q(S‘,A’)$表示未来总的回报，$\gamma$是折扣因子。</p>
<p>测试阶段无需奖励，只根据Q函数调整预测框直到输出stop动作。作者在OTB50上验证平均只需要2.1步输出结果，即只需要两层网络，因此可以大幅提速。</p>
<p>此外，这套策略还可以集成一些简单的特征，比如像素特征和hog特征，计算更快。</p>
<p>图3展示了一些early stop的例子，如跟踪清晰的人脸时只需C1-C2的特征，但跟踪一个模糊的人脸则需要更深层的C5特征。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210629194612.png" alt=""></p>
<hr>
<h1 id="Online-Decision-Based-Visual-Tracking-via-Reinforcement-Learning"><a href="#Online-Decision-Based-Visual-Tracking-via-Reinforcement-Learning" class="headerlink" title="Online Decision Based Visual Tracking via Reinforcement Learning"></a>Online Decision Based Visual Tracking via Reinforcement Learning</h1><h2 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h2><p>第二篇来自NIPS2020。目前主流的跟踪方法有基于检测的和基于模板匹配的，二者各有优劣。基于检测的方法容易受遮挡等影响错误更新网络，但是能适应形变；而基于模板匹配的方法只利用第一帧模板，与上述情况刚好相反。很自然会想到将二者结合，但这是两套完全不同的跟踪原理，直接融合并不能同时收敛到各自的最优解。因此本文提出了一个基于分层强化学习(HRL)的在线决策机制。决策机制实现了一种智能切换策略，其中检测器和模板跟踪器必须相互竞争，以便在它们擅长的不同场景中进行跟踪。</p>
<h2 id="Method-1"><a href="#Method-1" class="headerlink" title="Method"></a>Method</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210629205511.png" alt=""></p>
<p>整体框架如图2所示，包括决策模块和跟踪模块。决策模块是一个Actor-Critic（or Option-Critic？）结构，包括switch network和termination network。首先将初始帧模板和上一帧跟踪结果送入switch network，输出一个二元信号选择跟踪器。跟踪器结果送入termination network，输出终止当前跟踪器的概率。注意这里终止之后并不一定切换到另一个跟踪器，因为并不能保证另一个就更好，而是要经过switch network重新选择。</p>
<p><strong>Decision Module</strong></p>
<p>给定一组状态$S$和动作$A$，马尔可夫选项$\omega \in \Omega$ 包括三部分：intra-option policy $\pi: S \times A \rightarrow [0,1]$, termination condition $\beta: S^{+}  \rightarrow [0,1]$, initiation set $I \subseteq  S$。当option $\omega$选定后，根据$\pi<em>{\omega}$选择相应的动作，直到终止函数 $\beta</em>{\omega}$ 判断终止。</p>
<p>这是一个标准的Option-Critic结构，一大堆公式就省略了。但是最后作者却用Actor-Critic去解释图2，即switch network是option-value函数，作为Critic来评价option，并且为termination network提供更新梯度（参考上面Actor-Critic的ppt）。termination network作为Actor评估正在使用的跟踪器性能，以决定它是否应该在当前帧终止。</p>
<p>switch network的奖励函数定义如下：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210629214943.png" style="zoom:80%;" /></p>
<p>其中$P$表示被选中的跟踪器的跟踪框与GT的IOU，$P^*$表示未被选中的跟踪器的跟踪框与GT的IOU，$D_{IoU}$表示两者的差。按照公式7总共有3种情况：一个成功一个失败，两个均成功，两个均失败。</p>
<p>训练按照Actor-Critic训练，Critic使用贝尔曼方程（TD learning）更新，Actor使用策略梯度更新。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210629215908.png" alt=""></p>
<p>图5是一个可视化结果，其中终止概率1表示终止，0表示保持不变。可以看到初始是SiamFC（黄框）表现较好；当发生形变后，FCT的价值函数更大，终止概率趋近1，跟踪器切换；之后一直都是FCT表现更好，因此终止概率始终在0附近。</p>
<hr>
<h1 id="Fast-Template-Matching-and-Update-for-Video-Object-Tracking-and-Segmentation"><a href="#Fast-Template-Matching-and-Update-for-Video-Object-Tracking-and-Segmentation" class="headerlink" title="Fast Template Matching and Update for Video Object Tracking and Segmentation"></a>Fast Template Matching and Update for Video Object Tracking and Segmentation</h1><h2 id="Motivation-2"><a href="#Motivation-2" class="headerlink" title="Motivation"></a>Motivation</h2><p>第三篇来自CVPR2020。本文针对的任务是<strong>多实例半监督视频目标分割</strong>(VOS)。基于检测的算法被广泛应用于这一任务，难点在于选择匹配方法来预测结果，以及是否更新目标模板。 本文利用强化学习来同时做出这两个决策。具体来说，<strong>agent根据预测结果的质量来决定是否更新目标模板</strong>。 匹配方法的选择则基于agent的动作历史来确定。</p>
<p>目前大部分VOT或VOS方法主要分为三步：</p>
<ol>
<li>对当前帧进行实例分割，生成一系列候选proposal；</li>
<li>将目标模板和所有proposal进行匹配，找到正确的proposal作为最终结果；</li>
<li>使用当前帧的预测结果替换目标模板。</li>
</ol>
<p>针对步骤2，基于外观的匹配方法（siamese）准确但非常耗时，而直接利用候选框与前一帧预测框的IOU进行快速匹配只适用于目标缓慢移动或变化。针对步骤3，现有方法简单地直接用当前结果替换模板，不考虑结果的正确性，会导致误差逐渐累积。因此需要利用强化学习智能切换。</p>
<h2 id="Method-2"><a href="#Method-2" class="headerlink" title="Method"></a>Method</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630113658.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630171122.png" alt=""></p>
<center>将上一帧中所有目标框的最小外接矩形bm扩大一定倍率得到搜素区域bs。</center>

<p>整体方法如图2所示，分为三个步骤：</p>
<ol>
<li>按照图3的方式确定目标搜索区域，采用实例分割网络（如YOLACT, Mask-RCNN）生成候选预测，然后利用基于IOU的匹配方法得到初步匹配结果；</li>
<li>通过agent判断初步结果的正确性和质量，决定是否更新模板；</li>
<li>确定是否需要切换到基于外观的匹配的方法。若连续N帧初步结果都不好（即第二步预测不更新模板），则切换到基于外观的匹配。此时会将整个图像送入网络。</li>
</ol>
<p>下面介绍将Actor-Critic的框架嵌入上述模型</p>
<p><strong>Action</strong></p>
<p>首先定义相关的符号，如图4所示，目标模板包括边界框 $T<em>{box}$，mask $T</em>{mask}$，$T<em>{box}$中的图像内容$T’</em>{box}$，$T<em>{mask}$中的图像内容$T’</em>{mask}$。而预测结果则是类似的$P<em>{box}, P</em>{mask}, P’<em>{box}, P’</em>{mask}$。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630173712.png" style="zoom:80%;" /></p>
<p>第一个决策是是否更新模板，agent的动作 $a_i \in A$ 有两种情况，$a_0$表示用当前结果更新模板，$a_1$表示不更新。</p>
<p>第二个决策是匹配方法选择快速的基于IOU匹配还是精确的基于外观匹配，在精度和速度间取得平衡。前者分别计算模板和候选预测的box IOU和mask IOU，选择IOU最大的作为匹配结果；后者则是计算模板图像块$T’<em>{box}$和候选预测图像块$P’</em>{box}$的相似性，选择最像的作为匹配结果。<strong>注意这里没有另外增加一个agent</strong>，而是根据第一个agent的历史决策来决定。若agent连续N帧预测$a_1$，表示目标很可能丢失，此时需要切换到基于外观的匹配方法。</p>
<p><strong>State</strong> </p>
<p>输入agent的状态$s<em>t$包括两部分，如下式。第一部分$S_T$是模板图像，其中边框$T’</em>{box}$之内的内容保持不变，之外的内容填充黑色；第二部分$S_P$是搜索图像，同样将mask之内的内容保持不变，之外的内容填充黑色。提取这两种图像的特征并相加得到输入状态。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630175114.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630175123.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630175138.png" alt=""></p>
<p><strong>Reward</strong></p>
<p>奖励函数定义：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630175755.png" alt=""></p>
<p>$J<em>t$表示$P</em>{mask}$和GT mask之间的IOU。</p>
<p><strong>Actor-Critic Training</strong></p>
<p>基本元素确定后，按照Actor-Critic框架训练。</p>
<p>critic网络用value-based的方式训练：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630180416.png" alt=""></p>
<p>$\delta_t$ 表示TD error，公式8中梯度下降用加号是因为公式9减法顺序和常规的是反过来的。</p>
<p>actor用policy-based方法训练：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630192837.png" alt=""></p>
<p>公式11减$V(s_t)$表示是带baseline的策略梯度。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630193309.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630193324.png" alt=""></p>
<p>图1和图6表面本文的方法在VOT和VOS任务上均能在速度和精度上取得一个较好的平衡。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上三种方法分别介绍了利用强化学习来决策使用的跟踪特征，多个跟踪器的切换以及是否更新模板。可以发现，应用的方向基本都是把跟踪方法中某些需要启发式设计的模块换成了强化学习进行智能决策。此外，第一篇和第三篇均提到了引入强化学习可以在一定程度上提速，对于某些简单的情况，agent可以决策使用简单的方法进行跟踪。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>综述</tag>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Domain Adaptive SiamRPN++ for Object Tracking in the Wild</title>
    <url>/tracking/44-DASiamRPN/</url>
    <content><![CDATA[<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210701165637.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2106.07862">论文</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>基于孪生网络的跟踪算法均假定训练和测试数据遵循相同的分布，然而在正常图像上训练的跟踪器并不能保证在其他领域的数据上（如雨雾天气的序列）也表现良好，即存在域偏移（domain shift）问题，如图1和图2。作者称本文是首次将域分布差异问题引入视觉跟踪领域。</p>
<p>针对这一问题，本文提出一种域自适应方法，包括Pixel Domain Adaptation (PDA) 和 Semantic Domain Adaptation (SDA)。PDA分别对（不同域的）模板和搜索图像的特征对齐，消除天气、光照等引起的像素级域偏移；SDA将（不同域的）跟踪目标的特征表达对齐，以消除语义级的域偏移。二者均通过对抗训练的方式学习域分类器，域分类器强制网络学习域不变的特征表达，从而实现域自适应。</p>
<p>最后作者在有雾和红外序列两个不同域的数据集上进行了验证。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702104303.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702104337.png" alt=""></p>
<span id="more"></span>
<h1 id="Theoretical-Preliminaries"><a href="#Theoretical-Preliminaries" class="headerlink" title="Theoretical Preliminaries"></a>Theoretical Preliminaries</h1><p>最简单粗暴的方法就是搜集许多具有不同域的标注训练数据，但这显然不现实。因此我们的目标是针对<strong>无监督域自适应场景</strong>（即源域有标记而目标域未标记），使跟踪器在源域和目标域上都表现良好，而不需要额外的标注成本。一种通用的方案就是学习域不变（domain-invariant）的特征表达来缩小不同域之间的差异。作者利用 <em>A</em>-distance理论和概率分析来实现这一目的，下面先简单介绍这些概念。</p>
<h2 id="A-distance"><a href="#A-distance" class="headerlink" title="A-distance"></a><em>A</em>-distance</h2><p>给定源域 $S$ 和目标域 $T$， <em>A</em>-distance可以用于衡量两个域样本分布的差异，定义如下：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702113837.png" alt=""></p>
<p>其中h表示域分类器，$h(x)\rightarrow 0$表示样本x属于源域，$h(x)\rightarrow 1$表示样本x属于目标域。$min \ error(h(x))$表示理想域分类器的预测误差，显然，误差越小（越容易区分）表示域差异越大。现在要最小化域差异$d_A(S,T)$以实现特征对齐，等价于要最大化理想域分类器误差，即</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702114945.png" alt=""></p>
<p>其中 f 表示样本 x 的特征表达。公式(3)是特征提取器 f 和域分类器 h 之间的minimax优化问题。这个怎么理解呢？其实类似GAN，域分类器 h 需要尽可能区分不同域的样本，而特征提取 f 需要欺骗分类器让其难以区分不同域，即让 f 提取到域不变特征。</p>
<p>作者在优化这个问题时采用 Gradient Reversed Layer (GRL)，如下图所示，在梯度从域分类器传到特征提取之前将其取负号反转，希望粉色部分的参数向$L_d$减小的方向优化，绿色部分的参数向$L_d$增大的方向优化，用一个网络一个优化器就实现了两部分有不一样的优化目标，形成<strong>对抗</strong>的关系。（参考<a href="https://www.zhihu.com/question/266710153/answer/1338864403">Gradient Reversal Layer指什么？ - Just4Fan的回答 - 知乎</a> )</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702115615.png" alt=""></p>
<h2 id="Probabilistic-Analysis-for-Object-Tracker"><a href="#Probabilistic-Analysis-for-Object-Tracker" class="headerlink" title="Probabilistic Analysis for Object Tracker"></a>Probabilistic Analysis for Object Tracker</h2><p>作者将跟踪问题看成一个后验概率 $P(S,B|Z,X)$，即给定模板Z和搜索区域X，预测分类得分S和目标框B。由于域偏移的存在，源域的联合概率分布$P_S(S,B,Z,X)$与目标域的联合概率分布$P_T(S,B,Z,X)$是不同的。</p>
<p><strong>Pixel Domain Adaptation</strong> 根据贝叶斯公式，可以将联合概率分布分解成：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702150940.png" alt=""></p>
<p>其中$i \in {S,T}$。条件概率$P(S,B|Z,X)$相当于跟踪器的分类回归分支，我们假设这部分对于不同域是一样，那么域偏移主要来自模板和搜索图像的特征提取$P(Z,X)$。为了消除域偏移，需要另Siamese网络提取域不变的特征映射，即$P_S(Z,X) = P_T(Z,X)$</p>
<p><strong>Semantic Domain Adaptation</strong> 上面PDA解决天气或光照引起的全局域偏移，但不同域的目标还存在外观和类别的变化，因此还需要考虑目标语义的域偏移。类似的，可以将联合概率分解成：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702151847.png" alt=""></p>
<p>同样假设条件概率$P(S | B,Z,X)$对于不同域是一样的，那么域偏移主要来自$P(B,Z,X)$。为了消除偏移，需要$P_S(B,Z,X) = P_T(B,Z,X)$，表示给定了模板、搜索区域以及对应的目标框，跟踪目标的特征表达要是一样的。考虑到目标域是没有真实框标注的，因此这里统一采用RPN的预测框表示B。</p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702152654.png" alt=""></p>
<p>图3是整体方法框架，根据上一节的<em>A</em>-distance理论以及概率分析，作者提出了PDA和SDA两个模块。其中PDA针对的是孪生网络的整体特征，SDA针对的是预测框内的目标特征。</p>
<h2 id="Pixel-Domain-Adaptation"><a href="#Pixel-Domain-Adaptation" class="headerlink" title="Pixel Domain Adaptation"></a>Pixel Domain Adaptation</h2><p>PDA包括模板对齐和搜索区域对齐，目的是通过域分类器和Siamese网络之间的minimax优化来混淆跨域的特征映射。域分类器由Conv+MaxPool+FC组成，FC层对每个像素进行二值分类，损失函数为：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702154430.png" alt=""></p>
<p>m,n为像素位置，D是标签，p是预测结果。然后按照公式3的minimax优化，需要对域分类器参数最小化该损失，对siamese特征提取参数最大化该损失，即</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702154740.png" alt=""></p>
<p>$\omega_{pda}$表示PDA域分类器参数，$\varphi$表示孪生网络参数。域分类器的参数更新方向与减少域分类损失的方向相同，这与普通的训练方法相同；而Siamese网络的参数更新方向被反转（GRL），这正是增加域分类损失的方向，二者形成对抗。</p>
<h2 id="Semantic-Domain-Adaptation"><a href="#Semantic-Domain-Adaptation" class="headerlink" title="Semantic Domain Adaptation"></a>Semantic Domain Adaptation</h2><p>由于不同域的类别、视角和姿态的变化，跟踪目标会发生明显的变化，SDA强制跟踪目标的特征表示在语义上是域不变的。具体过程为，通过ROI Align提取预测框内的multi-layer的ROI特征，域分类器（两层FC）对其进行分类，GRL放在域分类器和ROI Align之间。域分类损失为：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702155824.png" alt=""></p>
<p>同样以对抗的方式训练SDA</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702155913.png" alt=""></p>
<p>$\omega_{sda}$表示SDA域分类器参数，$\varphi$表示孪生网络参数。无论跟踪目标来自源域还是目标域，目标的域不变特征都能在分数图中获得较高的响应。</p>
<p>最后总的训练损失包括孪生跟踪器的损失和域自适应损失</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702160122.png" alt=""></p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>训练时使用LaSOT作为源域数据，Foggy GOT-10k和LSOTB-TIR作为目标域数据。Foggy GOT-10k是作者生成的有雾数据集，LSOTB-TIR是红外数据集，注意二者作为目标域数据训练时是没有标注的。模板和搜索图像的裁剪通过运行现有的SiamRPN++对目标域数据集获取伪标签得到的。</p>
<p>表1-4展示了正常天气到有雾的跨域和RGB到红外的跨域的跟踪结果。这里的比较方式有点迷，作者列出每个epoch的结果证明性能的提升，但如果只关注最好的结果发现的性能提升其实不明显。比如Foggy VOT2018 0.211 v.s. 0.218，LSOTB-TIR 0.543 v.s. 0.547。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702162344.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702162404.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702162413.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702162420.png" alt=""></p>
<p>消融实验也呈现一样的结果，如果只比较最好的性能，单独的PDA和SDA甚至不如baseline。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702161634.png" alt=""></p>
<p>其他的一些可视化结果。图6将特征压缩到平面证明了源域和目标域的特征混淆在一起，证明了域不变特征。图7证明了提出的方法在跨域性能表现良好的同时，不会损失在源域上的性能。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702161754.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702161803.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/44-DASiamRPN++/20210702161810.png" alt=""></p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>Domain adaption</tag>
      </tags>
  </entry>
  <entry>
    <title>HiFT: Hierarchical Feature Transformer for Aerial Tracking</title>
    <url>/tracking/45-HiFT/</url>
    <content><![CDATA[<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/45-HiFT/20210807114323.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2108.00202">论文</a> <a href="https://github.com/vision4robotics/HiFT">代码</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>现有的孪生跟踪算法大多是基于相似得分图对目标对象进行分类和回归，使用单一的相似图会降低复杂场景下的定位精度，而像SiamRPN++那样单独使用多个相似图分别进行预测又会引入较大计算负担，不适用于移动设备。因此，本文提出一种 hierarchical feature transformer (HiFT) 对多个层级的相似图进行融合，既可以捕获全局的依赖关系，又可以高效地学习多层级特征之间的依赖关系。</p>
<p>在介绍本文方法前，我们先分析经典的transformer架构应用于目标跟踪任务中的难点。</p>
<ol>
<li>预定义的(或学习的)解码query在面对任意跟踪对象时很难保持有效性；</li>
<li>transformer难以处理小目标（参考deform DETR）。</li>
</ol>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/45-HiFT/20210807215745.png" alt=""></p>
<p>图2为整体框架，分成特征提取，transformer和预测头三部分。特征提取采用高效的alexnet，最后三层特征输入到transformer中，预测头采用类似FCOS的三分支预测（分类、回归、定位质量）。下面详细介绍本文提出的Hierarchical Transformer。</p>
<h2 id="Hierarchical-Feature-Transformer"><a href="#Hierarchical-Feature-Transformer" class="headerlink" title="Hierarchical Feature Transformer"></a>Hierarchical Feature Transformer</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/45-HiFT/20210807221543.png" style="zoom:80%;" /></p>
<p>HiFT包含高分辨率特征编码和低分辨率特征解码，前者学习不同特征层和空间信息之间的相互依赖关系，以提高对不同尺度(特别是低分辨率)目标的关注；而后者聚合了来自低分辨率深层特征的语义信息。这种全局上下文和层次特征之间的相互依赖大大提升了对复杂跟踪场景的适应能力。</p>
<p>transform的输入是三层不同尺度的互相关相似图，如公式1所示。图3中的$M_3’$ 和 $M_4’$ 则是加上了位置编码。​</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/45-HiFT/20210807225118.png" style="zoom:80%;" /></p>
<p><strong>Feature Encoding</strong></p>
<p>首先对$M_3’$ 和 $M_4’$ 进行相加和归一化的融合，得到$M_E^1 = Norm(M_3’+M_4’)$；然后经过multi-head attention得到 $M_E^2 \in WH \times C$，attention矩阵中同时包含了$M_3’$ 和 $M_4’$的多尺度信息，注意这里MHA中Q, K, V的输入差异；</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/45-HiFT/20210808094324.png" style="zoom:80%;" /></p>
<p>此外还额外增加了一个调制层 (modulation layer)，探索$M_4’$ 和 $M_E^3$ 之间的空间信息。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/45-HiFT/20210808095158.png" style="zoom:80%;" /></p>
<p><strong>Feature Decoding</strong></p>
<p>decoder部分和标准的transformer类似，差别在于输入的查询向量不是预定义的query，而是低分辨率的特征$M_5 \in WH \times C$​​，并且无需位置编码。</p>
<p>作者在实验中堆叠了一个编码和两个解码结构。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Evaluation-on-Aerial-Benchmarks"><a href="#Evaluation-on-Aerial-Benchmarks" class="headerlink" title="Evaluation on Aerial Benchmarks"></a>Evaluation on Aerial Benchmarks</h2><p>本文的应用环境是无人机跟踪，所以测试数据集均在无人机数据集测试。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/45-HiFT/20210808100142.png" style="zoom:80%;" /></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/45-HiFT/20210808100159.png" style="zoom:80%;" /></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/45-HiFT/20210808100237.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/45-HiFT/20210808100251.png" style="zoom:80%;" /></p>
<h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/45-HiFT/20210808100338.png" style="zoom:80%;" /></p>
<p>图5中OT表示标准transformer结构，FT表示用特征取代解码器中的object query，PE表示在解码输入中加入位置编码，RL表示在GT的矩形框内采样正样本（本文用的椭圆采样策略）。可以看到，OT使得性能下降，证明预定义的object query不适用于目标任意的跟踪任务；增加了PE后相比不用PE性能大幅下降；使用RL性能同样大幅下降，这样看上去似乎label assign策略的影响都要大于HFT了。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/45-HiFT/20210808102502.png" style="zoom:80%;" /></p>
<p>图5展示了本文方法在快速运动、低分辨率和遮挡等场景均可以更聚焦目标。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/45-HiFT/20210808102632.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/45-HiFT/20210808102731.png" alt=""></p>
<p>速度一骑绝尘，大于100FPS，并且使用alexnet的性能超过了很多resnet50的算法。并且作者在嵌入式平台NVIDIA AGX Xavier中实验也达到了31.2FPS(未使用tensorrt)，非常适合应用。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>transform</tag>
        <tag>ICCV2021</tag>
      </tags>
  </entry>
  <entry>
    <title>Learn to Match: Automatic Matching Network Design for Visual Tracking</title>
    <url>/tracking/46-AutoMatch/</url>
    <content><![CDATA[<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210808103150.png" alt=""></p>
<p><a href="https://arxiv.org/abs/2108.00803">论文</a> <a href="https://github.com/JudasDie/SOTS">代码</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>本文聚焦于孪生跟踪算法的匹配过程，目前主流的互相关操作是启发式设计的，严重依赖人工经验，并且单一的匹配方法无法适应各种复杂的跟踪场景。因此，本文引入了6种新的匹配算子来替代互相关。通过分析这些算子在不同跟踪挑战场景下的适应性，作者发现可以将它们结合起来进行互补，并借鉴NAS思想提出一种搜索方法 binary channel manipulation (BCM) 探索这些匹配算子的最优组合。</p>
<h1 id="Analysis-of-Matching-Operators"><a href="#Analysis-of-Matching-Operators" class="headerlink" title="Analysis of Matching Operators"></a>Analysis of Matching Operators</h1><p>首先介绍本文采用的6种匹配算子，Concatenation, Pointwise-Addition , Pairwise-Relation , FiLM , Simple-Transformer 和Transductive-Guidance。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210808161330.png" alt=""></p>
<p><strong>Concatenation</strong> 图2(a)，将模板特征$F_z$ pool成 $f_z \in 1 \times 1 \times C$，再与搜索特征$F_x$拼接。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210808161411.png" style="zoom:80%;" /></p>
<p><strong>Pointwise-Addition</strong> 图2(b)，将上面的拼接换成了对应元素相加。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210808161701.png" style="zoom:80%;" /></p>
<p><strong>Pairwise-Relation</strong> 图2(c)，类似non-local，将$F_x, F_z$分别reshape成 $H_x W_x \times C$ 和 $C \times H_z W_z $​，然后做矩阵乘法。相当于将模板中的每个元素都与搜索特征的所有元素衡量相似性。​</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210808192301.png" style="zoom:80%;" /></p>
<p><strong>FiLM</strong> 图2(d)，这个思路源于视觉推理，通过对神经网络的“中间特征”应用仿射变换来自适应地影响网络的输出。如公式5所示，对$f_Z$卷积变换后得到系数$\gamma$和偏置$\beta$，将其作用于搜索特征$F_x$​</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210808202649.png" style="zoom:80%;" /></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210808202908.png" style="zoom:80%;" /></p>
<p><strong>Simple-Transformer</strong> 图3(a)，就是multi head attention。</p>
<p><strong>Transductive-Guidance</strong>  图3(b)，源于VOS中的掩码传播机制，即用前一帧的掩码引导当前帧的预测。首先像Pairwise-Relation一样计算模板和搜索特征之间的对应关系(affinity)（公式7），然后用第一帧的伪掩码进行调制（公式8），生成的$G$中的每个位置表示前景的概率，最后将$G$与原始搜索特征$F_x$相加（公式9）。整个过程可以参考该作者另一篇文章<a href="https://arxiv.org/abs/2008.02745v2">OceanPlus</a>。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210808221525.png" style="zoom:80%;" /></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210808222703.png" style="zoom:80%;" /></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210808222743.png" style="zoom:80%;" /></p>
<p><strong>Analysis</strong></p>
<p>作者将上述6种算子与传统的互相关匹配分别在OTB100上进行测试，比较其性能，如表1所示。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210808223451.png" alt=""></p>
<p>对于单一的匹配算子，除了6和7，其他的均取得了和dw-corr相当的性能，甚至2（直接拼接）的性能更好。</p>
<p>虽然2的性能最优，但并不能保证其在所有跟踪挑战中都是最好的，比如在SV，OPR，OV和LR上就被其他算子超过。作者在图4进一步可视化了响应图，可以看到depthwise cross-correlation (a), Pairwise-relation (d), and Transductive-Guidance (g)更关注目标本身；而the concatenation (b), Pointwise-Addition (c), Simple-Transformer (e), and FiLM (e)则包含了更多上下文信息。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210808225804.png" style="zoom:80%;" /></p>
<p>上述结果表明不同的匹配算子在不同跟踪场景下具有不同的可靠性，由此想到是否可以将它们结合起来，利用这些特征进行互补。因此，作者接下来提出一种自适应学习的自动选择和组合匹配算子的方法。</p>
<h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210809101847.png" alt=""></p>
<p>图5为整体框架，首先将模板和搜索特征送入多个匹配算子构成的搜索空间，生成m个响应特征 ${r_1, r_2, … , r_m}$，给每个响应特征的每个通道都赋予一个可学习的调制器(manipulator) $\omega_i^j$​​，表示该通道的贡献。然后引入二元Gumbel-Softmax来离散化这些调制器进行二元决策，训练过程参考DARTS的两层优化。最后根据学好的调制器保留两个匹配算子构建新的跟踪器再次训练。</p>
<p>下面分别介绍二元的通道调制器(Binary Channel Manipulation)和两层优化(Bilevel Optimization)</p>
<h2 id="Binary-Channel-Manipulation"><a href="#Binary-Channel-Manipulation" class="headerlink" title="Binary Channel Manipulation"></a>Binary Channel Manipulation</h2><p>Binary Channel Manipulation (BCM) 用于决定匹配算子对目标状态预测的贡献，它给每个匹配响应特征的每个通道都赋予一个可学习的调制器(manipulator) $\omega_i^j$​​，将所有调制后的特征拼接聚合成一个大的特征（有点类似NAS里的supernet）。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210809111736.png" style="zoom:80%;" /></p>
<p>其中$r_i^j$​ 表示第 i 个响应特征的第 j 个通道。对于每个匹配算子，我们将其对应的所有通道调制器相加得到该算子的potential $p_i$​，这个在后面选择匹配算子时会用到。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210809112703.png" style="zoom:80%;" /></p>
<p>接下来，将连续的$\omega_i^j$​转换为离散的，离散后的类别是二值分类，概率向量为$\pi = { \pi_1 = \sigma(\omega_i^j), \pi_2 = 1 - \sigma(\omega_i^j) }$​。为了能够可微的反向传播，这里利用<a href="https://www.cnblogs.com/initial-h/p/9468974.html">Gumbel-Softmax</a>进行训练。对概率向量$\pi$​对应的随机变量添加Gumbel噪声$g_k$​后随机采样</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210809114815.png" style="zoom:80%;" /></p>
<p>然后将argmax替换为Softmax定义一个连续可微的近似</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210809114947.png" style="zoom:80%;" /></p>
<p>因为这只是一个二值概率分布，公式14带入$\pi$的表达式后可以被简化成</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210809115114.png" style="zoom:80%;" /></p>
<p>推导如下：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210809120120.png" alt=""></p>
<p>作者这里令$\tau=1, g_k=0$​​，对于离散采样的样本d，前向传播时使用硬值(0或1)，反向传播采用软值获得梯度。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210809120823.png" style="zoom:80%;" /></p>
<h2 id="Bilevel-Optimization"><a href="#Bilevel-Optimization" class="headerlink" title="Bilevel Optimization"></a>Bilevel Optimization</h2><p>优化的目标有两部分，一个是调制器参数$\omega$，另一个是匹配网络的卷积层参数$\theta$。这里借鉴DARTS的优化方法，利用训练集优化$\theta$，利用验证集优化$\omega$，记作</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210809154925.png" style="zoom:80%;" /></p>
<p>为了加速收敛，将其简化成</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210809155027.png" style="zoom:80%;" /></p>
<p>按上述方式训练后，保留potential $p_i$最高的两个匹配算子构成新的跟踪器重新训练。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h2><p>训练过程分成匹配网络的搜索和新跟踪器的训练两个阶段，第一阶段用Bilevel Optimization搜索最优的匹配网络组合，第二阶段用优化的匹配网络构建一个新的跟踪器进行常规的训练。搜索算法为分类和回归分支确定不同的匹配网络。经过第一阶段的训练后，分类分支使用Simple-Transformer和FiLM，而回归分支使用FiLM 和 Pairwise-Relation。</p>
<h2 id="State-of-the-art-Comparison"><a href="#State-of-the-art-Comparison" class="headerlink" title="State-of-the-art Comparison"></a>State-of-the-art Comparison</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210809163533.png" alt=""></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210809163718.png" style="zoom:80%;" /></p>
<h2 id="Ablation-and-Analysis"><a href="#Ablation-and-Analysis" class="headerlink" title="Ablation and Analysis"></a>Ablation and Analysis</h2><p><strong>One or Many Manipulators</strong> 本文给匹配算子的每个通道都赋予一个调制器，作者也尝试了给每个匹配算子只赋予一个标量的调制器，性能会有所下降 OTB100 69.5，LaSOT 54.7。</p>
<p><strong>Random Search</strong> 随机选择匹配算子同样会使性能下降，OTB100 69.1，LaSOT 53.2。</p>
<p><strong>NAS-like Matching Cell</strong> 用DARTS的方法构建一个有向无环图进行搜索，如图7所示，同样不如本文的结果，且速度会大幅下降。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/46-AutoMatch/20210809165018.png" style="zoom:80%;" /></p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><ol>
<li>DW-Corr并不是孪生跟踪算法的最优选择，本文设计了6种新的匹配算子；</li>
<li>本文用一种自动搜索的方式组合出最优的匹配网络。</li>
</ol>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>ICCV2021</tag>
        <tag>matching</tag>
      </tags>
  </entry>
</search>
