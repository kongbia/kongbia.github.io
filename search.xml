<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Siam R-CNN: Visual Tracking by Re-Detection</title>
    <url>/2021/04/24/tracking/1-siamrcnn/</url>
    <content><![CDATA[<p>Siam R-CNN是亚琛工业大学&amp;牛津大学联合推出的，核心是通过重检测进行视觉跟踪，并构建了基于轨迹的动态规划算法，建模被跟踪对象和潜在干扰对象的完整历史。效率方面，该方法可以在 ResNet-101 上达到 4.7 FPS，在 ResNet-50 上达到 15 FPS 。</p>
<span id="more"></span>

<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/0-siamrcnn/20210424105346.png"></p>
<p><a href="https://arxiv.org/abs/1911.12836">论文</a><br><a href="https://github.com/SJTU-DL-lab/SiamRCNN">代码</a></p>
<h1 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h1><ol>
<li>将重检测思想用于跟踪。通过判断建议区域（region proposal）是否与模板区域（template region）相同，重检测图像中任何位置的模板对象，并对该对象的边界框进行回归，这种方法对目标大小和长宽比变化比较鲁棒；</li>
<li>提出一种新颖的 hard example mining 方法，专门针对困难干扰物训练；</li>
<li>提出 Tracklet Dynamic Programming Algorithm (TDPA)，可以同时跟踪所有潜在的目标，包括干扰物。通过重检测前一帧所有目标候选框，并将这些候选框随时间分组到tracklets(短目标轨迹)中。然后利用动态规划的思想，根据视频中所有目标和干扰物tracklets的完整历史选择当前时间步长的最佳对象。Siam R-CNN通过明确地建模所有潜在对象的运动和交互作用，并将检测到的相似信息汇集到tracklets中，能够有效地进行长时跟踪，同时抵抗跟踪器漂移，在物体消失后可以立即重检测目标。</li>
</ol>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/0-siamrcnn/20210424105651.png"></p>
<p>上图为Siam RCNN整体框架，作者设计了一个Siamese two-stage detection network作为跟踪器。第一阶段是RPN，第二阶段通过将感兴趣区域(RoI)的特征和参考特征拼接起来，包括以第一帧的GT作为参考和以前一帧的检测结果作为参考的两个redetction head。最后构建Tracklet Dynamic Programming Algorithm (TDPA)去跟踪所有潜在目标。</p>
<p>接下来按照论文结构对每一部分进行说明：</p>
<h2 id="Siam-RCNN"><a href="#Siam-RCNN" class="headerlink" title="Siam RCNN"></a>Siam RCNN</h2><p>本小节主要是讲如何将 Faster RCNN 的那一套用于重检测，核心是将固定类别的detection head换成本文的re-detection head。re-detection head的输出只有两类，即候选区域是否是参考对象目标。训练时backbone和RPN的参数冻结，只有re-detection head参与训练。</p>
<h2 id="Video-Hard-Example-Mining"><a href="#Video-Hard-Example-Mining" class="headerlink" title="Video Hard Example Mining"></a>Video Hard Example Mining</h2><p>在传统 Faster RCNN 训练中，负样本是从RPN得到的区域中采样得到的。但是，在许多图像中，仅有少量负样本。为了最大化 re-detection head 的判别能力，作者认为需要在难负样本（hard negative samples）上训练。并且与检测中的通用难样本不同，这里的难样本是从其他视频中检索出来的与参考目标类似的样本。</p>
<ul>
<li><p><strong>Embedding Network</strong> 最直接的方法就是从其他视频中寻找与当前对象属于同一个类别的对象作为难负样本。然而，物体的类别标签并不总是可靠，一些同类的物体很容易区分，不同类别的物体反而可能是理想的难负样本。所以，本文受到行人重识别的启发，提出利用 embedding network 的方法，将 GT Box 中的物体映射为 embedding vector。这个网络来源于 PReMVOS，用 batch-hard triplet loss 来训练，期望达到的效果是消除单个对象实例之间的歧义，例如，两个不同的人在嵌入空间中应该离得很远，而同一个人在不同帧中的向量距离应该很近。</p>
</li>
<li><p><strong>Index Structure</strong> 接下来为近邻 queries 构建一个有效的索引结构，将其用于寻找所需要跟踪的物体在 embedding space 中的最近邻。图 3 展示了一些检索得到的难负样本。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/0-siamrcnn/20210424105907.png"></p>
</li>
<li><p><strong>Training Procedure</strong> 训练时在其他视频上实时检索当前视频帧的难负样本过于耗时。本文预先对训练数据的每一个 GT Box 都提取其 RoI-aligned  features。在训练的每一步，随机选择一个 video 和 object，然后随机的选择一个 reference 和 target frame。在此之后，用上一节提到的 indexing structure 来检索 10000 个最近邻的 reference box，从中选择出 100 个 negative training examples。</p>
</li>
</ul>
<h2 id="Tracklet-Dynamic-Programming-Algorithm"><a href="#Tracklet-Dynamic-Programming-Algorithm" class="headerlink" title="Tracklet Dynamic Programming Algorithm"></a>Tracklet Dynamic Programming Algorithm</h2><p>本文所提出的 片段动态规划算法（Tracklet Dynamic Programming Algorithm）隐式地跟踪感兴趣目标和潜在干扰物，从而持续抑制干扰对象。为此，TDPA 维护一组 tracklets，即：short sequences of detections。然后利用动态规划的评分算法为模板对象在第一帧和当前帧之间选择最可能的 tracklets序列。每个detection都定义为：a bounding box, a re-dection score 和 RoI-aligned features。此外，每个 detection 都是 tracklet 的组成部分。tracklet有一个开始时间和一个结束时间，并由一组 detection 定义，从开始到结束时间的每一个时间步对应一个detection ，也就是说，在tracklet中不允许有空隙。</p>
<ul>
<li><strong>Tracklet Building</strong> </li>
</ul>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/0-siamrcnn/20210424110139.png"></p>
<p>Line 2 提取backbone特征</p>
<p>Line 3 RPN得到感兴趣区域，为了补偿潜在的假负例影响，加入上一帧的结果；</p>
<p>Line 4 将Roi和gt送入redetection head得到第一帧的重检测结果 $ det_{S_t} $（包括相似性得分和目标框）；</p>
<p>Line 5-6 将当前帧结果 $ det_{S_t} $ 与上一帧结果 $ det_{S_{t-1}} $ 送入redetection head计算每一对检测结果的相似性得分（为了减少计算，仅把当前帧与上一帧框的归一化空间距离小于 r 时才送入head计算，否则相似性得分设为负无穷。如图2，上一帧有3个结果，当前帧有2个结果，理论上两两之间应该计算6个相似得分，但通过框的空间距离约束，实际参与相似得分计算的只有4组）；</p>
<p>Line 7-20 扩展tracklets。遍历当前帧的检测结果 $d_t \in det_{S_t}$ 加入tracklet需要满足：</p>
<ol>
<li> 当前结果 $d_t$ 与上一帧结果 $\hat{d} _ {t-1}$ 的最大相似性得分 $s_{1}$ 大于阈值 $\alpha$</li>
<li>当前帧没有其他检测结果与 $ \hat{d} _ {t-1} $ 具有同样高的相似性，即当前帧除 $d_t$ 以外的结果与 $ \hat{d} _ {t-1} $ 的相似性要满足 $s_{2} \leq s_{1}-\beta$</li>
<li> 上一帧没有其他检测结果与当前的 $d_t$ 具有同样高的相似性，即上一帧除 $ \hat{d} _ {t-1} $ 以外的结果与 $d_t$ 的相似性要满足 $s_{3} \leq s_{1}-\beta$</li>
</ol>
<br/>

<ul>
<li><strong>Scoring</strong>  </li>
</ul>
<p>轨迹 $A=(a_1,…,a_N)$ 是一个包含N个不重复tracklets的序列，其中 $end(a_i) &lt; start(a_{i+1}), \forall i∈{1,…,N-1}$ ，即后一个tracklet的开始帧一定比前一个tracklet的结束帧大。一个轨迹的总分由衡量单个轨迹质量的一元分数 unuary 和惩罚轨迹之间空间距离的位置分数 loc_score 组成。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/0-siamrcnn/20210424111306.png" style="zoom:80%;" />

<p>其中 ff_score 表示tracklet $a_{i}$ 在 t 时刻以第一帧gt为参考的重检测分数；</p>
<p>而 ff_tracklet_score 表示 $a_{i}$ 以gt所在tracklet的最后一个检测结果为参考的重检测分数（因为一个tracklet的所有检测结果都有很高概率与这个tracklet的第一个检测结果相同，否则这个tracklet就会终止，所以gt所在的tracklet的最后一个检测结果也有很大概率是正确的）；</p>
<p>Location score是计算前一个tracklet最后一个检测结果和后一个tracklet的第一个检测结果的bbox之间的L1距离，是负的，作为惩罚项，越小越好。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/0-siamrcnn/20210424111534.png" style="zoom:80%;" />

<ul>
<li><strong>Online Dynamic Programming</strong> </li>
</ul>
<p>这一节介绍如何高效找到具有最大总得分的tracklets序列。</p>
<p>定义 $\theta[a]$ 表示从第一个tracklet开始到第a个tracklet的总得分。由于一旦tracklet不被扩展，就会终止。因此当新的一帧到来时，只有被扩展或者新建的tracklet需要重新计算score。</p>
<p>首先设置 $\theta[a_{ff}]=0$，$a_{ff}$ 是first-frame tracklet（即包含gt的tracklet），当每一个tracklet a被创建或需要更新时</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/0-siamrcnn/20210424111717.png" style="zoom:80%;" />

<p>在更新了当前帧的 $\theta$ 后，选择最大动态规划得分的tracklet $\hat{a}=arg max_{a} \theta[a]$；如果被选中的tracklet在当前帧中没有检测结果，则算法会指出目标不存在。由于benchmarks要求每帧都输出结果，所以我们用选定的tracklet最近一次的检测的box，并且将其得分置0。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>实验做的很充分，在目前主流数据集中都有结果。在short-term的测试中，大部分达到SOTA水平或者比SOTA稍低一些，在long-term的测试效果则非常好，基本吊打第二名。其中VOT的测试因为其自带一个重启，会与算法本身的重启冲突，所以作者专门做了一个short-term版本的用于VOT测试。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/0-siamrcnn/20210424111909.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/0-siamrcnn/20210424111922.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/0-siamrcnn/20210424112042.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/0-siamrcnn/20210424113951.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/0-siamrcnn/20210424111947.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/0-siamrcnn/20210424114012.png"></p>
<p>消融实验比较了 使用hard example mining的效果，TPDA与直接使用每一帧最大的重检测得分(Argmax)的对比，和专门用于测试VOT2018的short-term版本；以及改变backbone和ROI的参数量来提速的验证。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文最大的贡献在于将两阶段结构和重检测用于跟踪，设计了一套适合长时跟踪的算法。两阶段结构在SPM-Tracker已经证明了其对于鲁棒性和判别性之间有较好的平衡；而重检测这类检索的方法同样对于目标变化的适应性更强。整套算法做的非常完善，美中不足在于速度太慢。</p>
<p>P.S. SiamRCNN很长一段时间都作为各类跟踪数据集的天花板，直到CVPR2021各类transform架构的跟踪算法出现后才被超过，可以参考 <a href="https://kongbia.github.io/2021/04/23/tracking/33-transform-tracking/">Transform与目标跟踪</a></p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>CVPR2020</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测框架在目标跟踪中的应用</title>
    <url>/2021/04/24/tracking/2-detection-in-tracking/</url>
    <content><![CDATA[<p>从SiamRPN将跟踪问题定义为one-shot detection任务之后，出现了大量将检测组件由于跟踪的研究。不过Siamese系列一个很大的问题在于其本质仍然是一个模板匹配问题，网络关注的是寻找与target相似的东西，而忽视了区分target和distractor的判别能力，这正是目标检测任务所擅长的。目标检测和目标跟踪的关键差异在于检测是一个class-level的任务，而跟踪是一个instance-level的任务（即检测只关注类间差异而不重视类内差异，跟踪需要关注每一个实例，同时跟踪的类别是不可知的）。</p>
<p>本篇笔记关注如何将目标检测框架应用在跟踪中，主要介绍其思想，细节部分不做过多描述，记录论文包含：</p>
<span id="more"></span>

<ul>
<li><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Bridging_the_Gap_Between_Detection_and_Tracking_A_Unified_Approach_ICCV_2019_paper.pdf">Bridging the Gap Between Detection and Tracking: A Unified Approach</a></li>
<li><a href="https://arxiv.org/abs/1912.08531">GlobalTrack: A Simple and Strong Baseline for Long-term Tracking</a></li>
<li><a href="https://arxiv.org/abs/1910.11844">Learning to Track Any Object</a></li>
<li><a href="https://arxiv.org/abs/1911.12836">Siam R-CNN: Visual Tracking by Re-Detection</a></li>
<li><a href="https://arxiv.org/abs/2004.00830">Tracking by Instance Detection: A Meta-Learning Approach</a></li>
</ul>
<h1 id="Bridging-the-Gap-Between-Detection-and-Tracking-A-Unified-Approach"><a href="#Bridging-the-Gap-Between-Detection-and-Tracking-A-Unified-Approach" class="headerlink" title="Bridging the Gap Between Detection and Tracking: A Unified Approach"></a>Bridging the Gap Between Detection and Tracking: A Unified Approach</h1><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210424234243.png"></p>
<p>从结构图可以很直观的看出这就是Faster RCNN的框架，作者将目标跟踪任务看成是one-shot object detection 和 few-shot instance classification 的组合。前者是一个class-level的子任务用来寻找和目标相似的候选框，后者是instance-level的任务用来区分目标和干扰物。主要有两个模块：Target-guidence module(TGM) 和 few-shot instance classifier。</p>
<p>TGM对目标和搜索区域的特征以及它们在主干中的相互作用进行编码，相当于让网络更关注于与目标相关的instance，后面几篇文章也用了不同的方法来实现这个目的。</p>
<p>TGM虽然使检测器聚焦于与目标相关的物体，但忽略了周围的背景干扰。为了弥补这一点，提出了few-shot instance classifier。 然而，直接从头开始训练耗时且容易导过拟合。因此作者通过Model-Agnostic Meta Learning (MAML)进行few-shot finetune，增强判别性进一步消除distractors。</p>
<p>MAML的目的是训练一组初始化参数，<strong>通过在初始参数的基础上进行一或多步的梯度调整，来达到仅用少量数据就能快速适应新task的目的，</strong>示意图如下：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210424234331.png"></p>
<p>域自适应的检测器整体训练流程如下图：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210424234353.png"></p>
<p>输入是三元组，包括examplar, support, query，训练分为inner and outer optimization loops。对于Inner optimization loop中，在support image上计算的loss用来微调meta-layers即detector heads的参数，然后用微调后的参数计算meta-loss其梯度用于更新outer optimization loop。具体公式可以参考原文。</p>
<p>在线跟踪中将之前帧的检测结果作为训练样本在线更新detector head的参数。</p>
<p>作者称这是第一篇将目标检测框架应用到跟踪上的通用框架，检测模型可以用Faster RCNN，SSD等，速度上SSD模型为10FPS   Faster RCNN模型为3FPS。</p>
<hr>
<h1 id="GlobalTrack-A-Simple-and-Strong-Baseline-for-Long-term-Tracking"><a href="#GlobalTrack-A-Simple-and-Strong-Baseline-for-Long-term-Tracking" class="headerlink" title="GlobalTrack: A Simple and Strong Baseline for Long-term Tracking"></a>GlobalTrack: A Simple and Strong Baseline for Long-term Tracking</h1><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210424234652.png"></p>
<p>这篇文章构建了一个global instance search tracker，主要思想是利用target来引导网络搜索特定instance，与上一篇的TGM模块思想类似，不过这里对在RPN阶段和分类回归阶段都加入了target信息进行引导。对应的就是Query-guided RPN 和 Query-guided RCNN。</p>
<p><strong>Query-guided RPN</strong></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210424234747.png"></p>
<p>$z \in [k,k,c]$ 是query的ROI特征，$x \in [h,w,c]$ 是搜索图像的特征。$f_z$ 用 $k \times k$ 0-padding的卷积将 $z$ 转换为 $1 \times 1$ 的核作用于搜索区域，$f_x$ 使用 $3 \times 3$ 1-padding的卷积。$f_{out}$ 是 $1 \times 1 \times c$  的卷积将通道数变回为c，这个过程不使用正则化和激活函数。</p>
<p><strong>Query-guided RCNN</strong></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210424235530.png"></p>
<p> $z$ 定义同上，$x_i \in [k,k,c]$ 是提取的proposal的特征。$h_z$ 和 $h_x$ 均为 $3 \times 3$ 1-padding的卷积，$h_{out}$ 为$1 \times 1 \times c$  卷积。</p>
<p>GlobalTrack 对视频每一帧的跟踪完全不依赖相邻帧，没有累计误差使得它在长期跟踪问题中准确率保持稳定。速度为6FPS。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210426092832.png"></p>
<center>车牌在长期跟踪过程中消失了一段时间，当车牌再次出现的时候，其他跟踪算法就再也无法恢复跟踪了，而没有累计误差的 GlobalTrack不受前面的影响立刻跟踪到了目标。</center>

<hr>
<h1 id="Learning-to-Track-Any-Object"><a href="#Learning-to-Track-Any-Object" class="headerlink" title="Learning to Track Any Object"></a>Learning to Track Any Object</h1><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210426093105.png"></p>
<center>图1 (a)从基于图像的数据集学习一个通用对象先验，(b)通过计算一个封闭形式的目标和背景之间的线性判别器使其适应于一个感兴趣的特定对象(例如左上角的总线)。这允许跟踪物体通过显著的变形，而不捕获干扰物</center>

<p>本文重点在于将category-specific object detector 变成 category-agnostic, object-specific detector来做跟踪。想达到这个目的，需要处理如下两个关键的问题，如图1所示：</p>
<ol>
<li>如何将 category specific prior 改为 generic objectness prior？</li>
<li>如何进一步的将这种 generic prior 改为 particular instance of interst？</li>
</ol>
<p>针对问题1，作者构建了 a joint model for category-specific object detection and category-agnostic tracking。和之前类似，也是添加了目标特征的检测框架（基于 Mask R-CNN）如下图2所示 。其将目标模板作为输入，计算 feature embedding。然后该模板特征与测试图像计算相似性得到attention mask。attention mask又被用于重新加权空间特征，以检测感兴趣的物体。另外这个框架可以同时用于检测、跟踪和分割。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210426100348.png"></p>
<p>针对问题 2，本文计算一个线性分类器来区分第一帧的感兴趣目标和其他目标，通过最小二乘方法得到闭式解从而可以学习到一个更关注感兴趣instance的鲁棒特征。下图3通过一个例子说明，左下是直接用feature embedding计算的attention map，右下是用线性分类器计算的attention map，显然右下效果更好。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210426100433.png"></p>
<p>最后运行速度为7FPS。</p>
<hr>
<h1 id="Siam-R-CNN-Visual-Tracking-by-Re-Detection"><a href="#Siam-R-CNN-Visual-Tracking-by-Re-Detection" class="headerlink" title="Siam R-CNN: Visual Tracking by Re-Detection"></a>Siam R-CNN: Visual Tracking by Re-Detection</h1><p>这个就是用重检测的思想做跟踪，也是基于RCNN框架的，同时使用Tracklet Dynamic Programming Algorithm去跟踪所有潜在的目标。</p>
<p>具体解读见前一篇笔记</p>
<p><a href="https://kongbia.github.io/2021/04/24/tracking/1-siamrcnn/">Siam R-CNN: Visual Tracking by Re-Detection | CV home (kongbia.github.io)</a></p>
<hr>
<h1 id="Tracking-by-Instance-Detection-A-Meta-Learning-Approach"><a href="#Tracking-by-Instance-Detection-A-Meta-Learning-Approach" class="headerlink" title="Tracking by Instance Detection: A Meta-Learning Approach"></a>Tracking by Instance Detection: A Meta-Learning Approach</h1><p>同样是域自适应方法将检测器转化成跟踪器，此篇更像是第一篇Bridging the Gap Between Detection and Tracking: A Unified Approach的进阶，不同的是本文没有额外添加模板引导分支，而是直接用标准的检测器通过元学习的方式做域自适应。避免了冗余结构使得速度大幅提升，达到40FPS。另外就是训练的时候加入了很多来自MAML++喝MetaSGD的技巧，效果更好。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/1-detection-in-tracking/20210426100802.png"></p>
<p>具体的解读可以参考我b站的笔记，后续会搬运到博客。</p>
<p>[<a href="https://www.bilibili.com/read/cv5521209/">Note7] Tracking by Instance Detection: A Meta-Learning Approach - 哔哩哔哩专栏 (bilibili.com)</a></p>
<hr>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这几篇文章的一个共同思路都是融合了Siamese架构和目标检测框架，将目标实例信息以各种形式加入待检测图像中，从而将class-level的通用检测转变成instance-level的实例检测（跟踪）。借助目标检测对尺度，形变等复杂条件的优越性来解决跟踪中的问题，同时将跟踪转变成one-shot的检测任务也避免了更新带来的漂移（第一篇里面使用了MAML进行更新，主要原因猜测是单纯往RPN中融合目标信息还不够work，像globaltracker在head上也添加了instance，而第三篇则是构建一个分类器增强鲁棒性）。当然引入检测框架带来的计算开销也是很大的，最后一种方法避免了额外的模板分支相当于跳出了Siamese框架，给实时带来了可能。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title>Anchor Free的孪生目标跟踪</title>
    <url>/2021/04/26/tracking/3-anchorfree-siamese/</url>
    <content><![CDATA[<p>Anchor-free+孪生网络做跟踪在2020年非常火爆，相关笔记在b站记录。本文主要对其整合进行简单归纳。</p>
<p>[<a href="https://www.bilibili.com/read/cv4987634/?from=readlist">Note3] Anchor Free的目标跟踪 - 哔哩哔哩专栏 (bilibili.com)</a></p>
<p>[<a href="https://www.bilibili.com/read/cv6518222/?from=readlist">Note17] Anchor-free的目标跟踪(下) - 哔哩哔哩专栏 (bilibili.com)</a></p>
<p>跟踪任务可以看成是分类任务与状态估计任务的结合。分类任务的目的是精确定位目标的位置，而状态估计获得目标的姿态（即目标框）。SiamFC++一文将当前的跟踪器按照不同<strong>状态估计</strong>的方法分为三类：</p>
<span id="more"></span>

<ol>
<li>以DCF和SiamFC为主的跟踪器，构建多尺度金字塔，将搜索区域缩放到多个比例，选择最高得分对应的尺度，这种方式是最不精确的同时先验的固定长宽比不适合现实任务；</li>
<li>以ATOM为主的跟踪器，借鉴IOUNet，通过IOU的梯度迭代来细化box，提升精度的同时带来了较多的超参数以及时间上的消耗；</li>
<li>以SiamRPN为主的追踪器，通过RPN预设anchor来回归框，这类方法虽然很高效，但是anchor的设定不但会引入模糊的相似性得分，而且anchor的设置需要有大量的数据分布先验信息，与通用跟踪的目的不符合。</li>
</ol>
<p>本文主要记录用Anchor Free的思想来解决上述目标跟踪状态估计中存在的问题。目前比较主流的都是基于FCOS和CenterNet两种无锚框方式展开的。</p>
<h1 id="FCOS类"><a href="#FCOS类" class="headerlink" title="FCOS类"></a>FCOS类</h1><h2 id="SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines"><a href="#SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines" class="headerlink" title="SiamFC++: Towards Robust and Accurate Visual Tracking with Target Estimation Guidelines"></a>SiamFC++: Towards Robust and Accurate Visual Tracking with Target Estimation Guidelines</h2><p><a href="https://arxiv.org/abs/1911.06188">论文</a> <a href="https://github.com/MegviiDetection/video_analyst">代码</a></p>
<p>针对siam网络分析了之前的工作不合理的地方，提出了4条guidelines：</p>
<p>G1：decomposition of classification and state estimation：跟踪任务可以分解为分类与状态估计。分类影响鲁棒性，状态估计影响精确性。多尺度金字塔的方式忽略了状态估计所以精确性很低；</p>
<p>G2：non-ambiguous scoring：分类得分应该直接表示为目标在视野中存在的置信度分数，而不是像预定义的anchor那样匹配anchor和目标，这样容易产False positive；</p>
<p>G3：prior knowledge-free：跟踪器不应该依赖过多的先验知识（如尺度/长宽比）。现有的方法普遍存在对数据分布先验知识的依赖，阻碍了其泛化能力；</p>
<p>G4：estimation quality assessment：不能直接使用分类置信度来评价状态估计，需要使用独立于分类的质量评估方式。（如RPN系列直接就是选择分类置信度最高的位置进行边框预测，而ATOM，DIMP则另外加入了IOU信息来指导边框调整）</p>
<p>作者依据这4条guidelines设计了SiamFC++，将目标检测中的Anchor Free的FCOS应用到Siamese框架中，整体结构如下，细节部分可以去开头我在b站的专栏。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426104215.png"></p>
<hr>
<h2 id="SiamCAR-Siamese-Fully-Convolutional-Classification-and-Regression-for-Visual-Tracking"><a href="#SiamCAR-Siamese-Fully-Convolutional-Classification-and-Regression-for-Visual-Tracking" class="headerlink" title="SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking"></a>SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking</h2><p><a href="https://arxiv.org/abs/1911.07241v2">论文</a> <a href="https://github.com/ohhhyeahhh/SiamCAR">代码</a></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426111123.png"></p>
<p>这一篇和SiamFC++很类似，这里仅标注一些实践细节的差异。</p>
<ul>
<li>backbone采用了改造的resnet50；</li>
<li>multi-stage融合对相关结果拼接用1*1卷积降维/融合，而不是像siamrpn++那样对相关后的分类预测响应图加权相加；</li>
<li>分类和回归均由一个相关引出，而不是每个分支对应一个相关。这样计算量更小效率更高，而性能差不多；</li>
<li>inference阶段为了避免抖动取了中心点周围top-k的均值作为最终结果。</li>
</ul>
<p>细节同样参照开头b站专栏。</p>
<hr>
<h2 id="Siamese-Box-Adaptive-Network-for-Visual-Tracking"><a href="#Siamese-Box-Adaptive-Network-for-Visual-Tracking" class="headerlink" title="Siamese Box Adaptive Network for Visual Tracking"></a>Siamese Box Adaptive Network for Visual Tracking</h2><p><a href="https://arxiv.org/abs/2003.06761">论文</a> <a href="https://github.com/hqucv/siamban">代码</a> <a href="https://www.bilibili.com/read/cv5400217">解读</a></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426112506.png"></p>
<p>同样是FCOS的应用，比较insight的地方是打标签的时候使用椭圆标签，两个椭圆，小椭圆E2内的点是positive，大椭圆E1外的点是negative，两个椭圆中间的部分为ignore。椭圆标签能够更紧凑地标注正负样本，并且设置了缓冲(ignore)以忽略模棱两可的样本。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426112854.png" style="zoom:67%;" />

<hr>
<h2 id="Fully-Conventional-Anchor-Free-Siamese-Networks-for-Object-Tracking"><a href="#Fully-Conventional-Anchor-Free-Siamese-Networks-for-Object-Tracking" class="headerlink" title="Fully Conventional Anchor-Free Siamese Networks for Object Tracking"></a>Fully Conventional Anchor-Free Siamese Networks for Object Tracking</h2><p><a href="https://www.researchgate.net/publication/335467780_Fully_Conventional_Anchor-Free_Siamese_Networks_for_Object_Tracking">论文</a></p>
<p>将FCOS与级联结构结合，另一个就是分配GT到AFPN层时采用了FCOS一样的思路（划分[0,64], [64,128], [128,∞]）</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426114147.png"></p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426114239.png" style="zoom: 50%;" />

<hr>
<h2 id="Ocean-Object-aware-Anchor-free-Tracking"><a href="#Ocean-Object-aware-Anchor-free-Tracking" class="headerlink" title="Ocean: Object-aware Anchor-free Tracking"></a>Ocean: Object-aware Anchor-free Tracking</h2><p><a href="https://arxiv.org/abs/2006.10721">论文</a> <a href="https://github.com/researchmm/TracKit">代码</a> <a href="https://www.bilibili.com/read/cv6615313">解读</a></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426115451.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426115537.png"></p>
<ul>
<li>anchor-base方法对于弱预测的修正能力较差，因为训练时只考虑了IOU大于阈值的anchor的回归，对于跟踪过程中如果出现overlap很小的anchor很难去refine。而anchor-free可以针对每个点进行预测；</li>
<li>作者设计了一个feature alignment module来从预测框中学习object-aware feature（图2c），从而对物体尺度敏感；</li>
<li>特征融合上采用xy轴膨胀系数不同的膨胀卷积进行融合，不同膨胀的卷积可以捕获不同尺度的特征，提高最终融合特征的尺度不变性。</li>
</ul>
<hr>
<h1 id="CenterNet类"><a href="#CenterNet类" class="headerlink" title="CenterNet类"></a>CenterNet类</h1><h2 id="Siamese-Attentional-Keypoint-Network-for-High-Performance-Visual-Tracking"><a href="#Siamese-Attentional-Keypoint-Network-for-High-Performance-Visual-Tracking" class="headerlink" title="Siamese Attentional Keypoint Network for High Performance Visual Tracking"></a>Siamese Attentional Keypoint Network for High Performance Visual Tracking</h2><p><a href="https://arxiv.org/abs/1904.10128v2">论文</a></p>
<p>这篇将CenterNet和CornerNet结合到跟踪中，分别预测中心点和两个角点，以及运用了CBAM注意力机制强化上下文信息，应该是第一个将CenterNet/CornerNet用进来的，遗憾的是性能没有刷的很高。细节同样参照开头b站专栏。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426164747.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426164810.png"></p>
<hr>
<h2 id="Accurate-Anchor-Free-Tracking"><a href="#Accurate-Anchor-Free-Tracking" class="headerlink" title="Accurate Anchor Free Tracking"></a>Accurate Anchor Free Tracking</h2><p><a href="https://arxiv.org/abs/2006.07560">论文</a></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426165201.png"></p>
<p>这篇就是比较典型的CenterNet模式了，预测中心点，中心偏移以及宽高。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426165235.png"></p>
<p>作者另外设计了backbone，最后在VOT2018性能虽然比siamrpn++略低但是速度是它的3.9倍（136FPS v.s. 35FPS）。</p>
<hr>
<h2 id="Siamese-Keypoint-Prediction-Network-for-Visual-Object-Tracking"><a href="#Siamese-Keypoint-Prediction-Network-for-Visual-Object-Tracking" class="headerlink" title="Siamese Keypoint Prediction Network for Visual Object Tracking"></a>Siamese Keypoint Prediction Network for Visual Object Tracking</h2><p><a href="https://arxiv.org/abs/2006.04078">论文</a> <a href="https://github.com/ZekuiQin/SiamKPN">代码</a></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426171705.png"></p>
<p>这一篇将casscade的思想结合在centernet类的siamese跟踪器中，看上面图2结构已经很清晰了，KPN结构如下：</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426171748.png" style="zoom:80%;" />

<p>还有一个需要关注的就是每个stage训练的时候分类标签的高斯方差不一样，遵循的原则就是越高的stage峰值越收束。目的即随着级联的进行，监管信号越来越严格。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426171845.png"></p>
<hr>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="Correlation-Guided-Attention-for-Corner-Detection-Based-Visual-Tracking"><a href="#Correlation-Guided-Attention-for-Corner-Detection-Based-Visual-Tracking" class="headerlink" title="Correlation-Guided Attention for Corner Detection Based Visual Tracking"></a>Correlation-Guided Attention for Corner Detection Based Visual Tracking</h2><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Du_Correlation-Guided_Attention_for_Corner_Detection_Based_Visual_Tracking_CVPR_2020_paper.pdf">论文</a> <a href="https://www.bilibili.com/read/cv6647311">解读</a></p>
<p>作者为了解决跟踪中回归框估计不准确的问题，引入角点检测来得到更紧致的回归框。分析了之前一些角点检测方法在目标跟踪中无法取得好性能的原因，并提出了两阶段的correlation-guided attentional corner detection (CGACD)方法。第一阶段使用siamese网络得到目标区域的粗略ROI，第二阶段通过空间和通道两个correlation-guided attention来探索模板和ROI之间的关系，突出角点区域进行检测。速度可以达到70FPS。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426173132.png"></p>
<hr>
<h2 id="RPT-Learning-Point-Set-Representation-for-Siamese-Visual-Tracking"><a href="#RPT-Learning-Point-Set-Representation-for-Siamese-Visual-Tracking" class="headerlink" title="RPT: Learning Point Set Representation for Siamese Visual Tracking"></a>RPT: Learning Point Set Representation for Siamese Visual Tracking</h2><p><a href="https://arxiv.org/abs/2008.03467">论文</a> <a href="https://github.com/zhanght021/RPT">代码</a> <a href="https://zhuanlan.zhihu.com/p/257854666">原作者解读</a></p>
<p>现有的跟踪方法往往采用矩形框或四边形来表示目标的状态（位置和大小），这种方式忽略了目标自身会变化的特点（形变、姿态变化），因此作者受启发自Reppoints检测方法，采用表示点（Representative Points）方法来描述目标的外观特征，学习表示点的特征，根据表示点的分布确定目标的状态，实现更精确的目标状态估计。</p>
<p>具体可以参考原作者在知乎的解读，该方法取得了VOT2020-ST的冠军。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426173720.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426173734.png"></p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/3-anchorfree-siamese/20210426173759.png" style="zoom:80%;" />]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>Anchor-free</tag>
        <tag>Siamese</tag>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title>Transform与目标跟踪</title>
    <url>/2021/04/23/tracking/33-transform-tracking/</url>
    <content><![CDATA[<p>Transform在视觉领域遍地开花，终于目标跟踪也没能逃过。并行的长距离依赖（空间和时间皆可）对于目标跟踪似乎有着天然的优势，本篇笔记简要概述今年CVPR2021关于Transform在目标跟踪中的应用，主要介绍动机和结构，细节和实验部分以后有空再补充。</p>
<span id="more"></span>

<p>论文列表：</p>
<ul>
<li>Transformer Tracking</li>
<li>Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking</li>
<li>Learning Spatio-Temporal Transformer for Visual Tracking</li>
<li>Target Transformed Regression for Accurate Tracking</li>
</ul>
<h1 id="Transformer-Tracking"><a href="#Transformer-Tracking" class="headerlink" title="Transformer Tracking"></a>Transformer Tracking</h1><p><a href="https://arxiv.org/abs/2103.15436">论文</a><br><a href="https://github.com/chenxin-dlut/TransT">代码</a><br><a href="http://naotu.baidu.com/file/0348e011f8d04a784134e3329a328076?token=53a180e3aa5251ab">代码框架解析</a></p>
<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>跟踪中常用的correlation存在问题：<br>是一个局部线性匹配过程，没有利用全局上下文，容易陷入局部最优；<br>得到的相似图丢失一定程度的语义信息，导致对目标边界预测不准。</p>
<p>利用transform的attention有效融合模板特征和ROI特征，相比correlation能产生更多的语义特征。作者提出了基于self-attention的ego-context augment module (ECA)和基于cross-attention的cross-feature augment module (CFA)<br><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/1.png" style="zoom:80%;" /></p>
<h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p>重复N=4次fusion layer最后再接一个CFA<br><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/2.png"></p>
<center>整体跟踪框架</center>

<img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/3.png" style="zoom:80%;" />

<center>ECA和CFA结构</center>

<p>transform工作过程</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/4.png" style="zoom:80%;" />

<ul>
<li>n=1 self search 没有来自模板的信息，因此会看到所有目标，而self template关注模板的关键信息（蚂蚁上的红点）；cross search和template同时具有目标和搜索的特征，因此可以更关注重要信息；</li>
<li>n=2 每一个attention输入都同时包含目标和搜索特征，self search对相似物的响应被抑制了，而cross search此时非常确信其预测。template的注意力此时开始关注目标边界；</li>
<li>n=3 进一步强化，模板特征成为包含大量目标边界信息的信息库，而搜索区域特征保留了目标的空间信息；</li>
<li>n=4 模板的分布变得混乱，这可能是因为，在目标确定之后，模板分支的特征不再需要保留模板本身的信息，而是存储了大量目标的边界信息，成为一个为回归服务的特征库。</li>
</ul>
<hr>
<h1 id="Transformer-Meets-Tracker-Exploiting-Temporal-Context-for-Robust-Visual-Tracking"><a href="#Transformer-Meets-Tracker-Exploiting-Temporal-Context-for-Robust-Visual-Tracking" class="headerlink" title="Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking"></a>Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking</h1><p><a href="https://arxiv.org/abs/2103.11681">论文</a><br><a href="https://github.com/594422814/TransformerTrack">代码</a></p>
<h2 id="动机-1"><a href="#动机-1" class="headerlink" title="动机"></a>动机</h2><p>现有的跟踪器常常忽略连续帧之间的 temporal contexts</p>
<ol>
<li><p>单帧独立检测方法： 对时域信息的利用只有运动先验（余弦窗）</p>
</li>
<li><p>模型更新方法：视频帧是独立的，没有相互推理关系；噪声会污染模型更新</p>
</li>
</ol>
<p>transform中的注意机制，能够建立跨帧的像素对应关系，在时间域内自由传递各种信号。</p>
<p>本文将各个独立的视频帧进行桥接，并通过 transformer 架构来探索它们之间的 temporal contexts，以实现鲁棒的目标跟踪。与经典的 transformer 的结构不同，作者将其编码器和解码器分离成两个平行的分支，并在 Siamese-like 跟踪管道中对其精心设计。</p>
<h2 id="结构-1"><a href="#结构-1" class="headerlink" title="结构"></a>结构</h2><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/5.png" style="zoom:80%;" />

<img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/10.png" style="zoom:80%;" />

<p>编码器通过基于注意力的特征强化来促进目标模板，有利于高质量的跟踪模型生成；</p>
<p>解码器将之前模板中的跟踪线索传播到当前帧，有利于目标搜索过程。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/6.png" style="zoom:80%;" />

<p>与经典transform结构的差异：</p>
<ol>
<li>Encoder-decoder Separation. 没有将编码器和解码器级联，而是将编码器和解码器分离为两个分支，以适应Siamese-like跟踪方法；</li>
<li>Block Weight-sharing. 编码器和解码器中的self-attention(图4中的黄色方框)共享权值，将模板和搜索转换到同一特征空间中，便于进一步cross-attention；</li>
<li>Instance Normalization. 将Layer Norm换成Instance Norm；</li>
<li>Slimming Design. 移除FFN，并且使用single-head attention。</li>
</ol>
<p>图4编码器解码器结构细节：</p>
<p>编码器： 输入模板特征 $T \in [N_T, C], N_T=n \times H \times W $, $n$为模板数量；</p>
<p>解码器： 输入搜索特征 $S \in [N_S, C], N_S=H \times W $</p>
<p>高斯Mask     $M \in [N_T, 1] $</p>
<p>Mask Transformation 关注空间注意力，Feature Transformation 关注上下文信息</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/7.png" style="zoom:80%;" />

<h3 id="跟踪框架"><a href="#跟踪框架" class="headerlink" title="跟踪框架"></a>跟踪框架</h3><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/8.png" style="zoom:80%;" />

<ul>
<li><p>Siamese框架将编码器特征crop后和解码器特征做相关；</p>
</li>
<li><p>DCF框架用编码器特征训练Dimp的kernel，作用于解码器特征；</p>
</li>
<li><center>将上一帧中所有目标框的最小外接矩形bm扩大一定倍率得到搜素区域bs。<center></li>
<li><p>模板池每5帧更新一次，先入先出。</p>
</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>严重（完全）遮挡，出视野，高计算量</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/9.png" style="zoom:80%;" />

<hr>
<h1 id="Learning-Spatio-Temporal-Transformer-for-Visual-Tracking"><a href="#Learning-Spatio-Temporal-Transformer-for-Visual-Tracking" class="headerlink" title="Learning Spatio-Temporal Transformer for Visual Tracking"></a>Learning Spatio-Temporal Transformer for Visual Tracking</h1><p><a href="https://arxiv.org/abs/2103.17154">论文</a><br><a href="https://github.com/researchmm/Stark">代码</a></p>
<h2 id="动机-2"><a href="#动机-2" class="headerlink" title="动机"></a>动机</h2><p>卷积只处理空间或时间上的局部关系，不擅长建立长距离的全局依赖关系。因此在面对目标发生较大形变或频繁进出视野时容易失败。另外，当前的方法将空间和时间分离处理，并没有明确建模空间和时间之间的关系。</p>
<p>考虑到transform在建模全局依赖方面的优势，作者利用它整合空间和时间信息进行跟踪，生成判别的时空特征用于目标定位。</p>
<p>编码器对目标对象和搜索区域之间的全局时空特征依赖关系进行建模，而解码器学习一个查询嵌入来预测目标对象的空间位置。该方法将目标跟踪作为一个直接的边框预测问题（角点预测），没有后处理。</p>
<h2 id="结构-2"><a href="#结构-2" class="headerlink" title="结构"></a>结构</h2><h3 id="Baseline-spatial-only"><a href="#Baseline-spatial-only" class="headerlink" title="Baseline (spatial-only)"></a>Baseline (spatial-only)</h3><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/11.png" style="zoom:80%;" />

<p><strong>编码器</strong>输入将模板和搜索特征拉平拼接；</p>
<p><strong>解码器</strong>中query可以注意到模板和搜索区域的所有位置的特征，从而学习鲁棒表示，以进行边框预测；</p>
<p><strong>预测头</strong>将Encoder输出中的搜索特征和decoder输出经过图3的结构，通过概率预测两个角点，最后输出唯一的框，用L1和IOU loss优化。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/12.png" style="zoom:80%;" />

<img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/13.png" style="zoom:80%;" />

<h3 id="Spatio-Temporal-Transformer-Tracking"><a href="#Spatio-Temporal-Transformer-Tracking" class="headerlink" title="Spatio-Temporal Transformer Tracking"></a>Spatio-Temporal Transformer Tracking</h3><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/14.png" style="zoom:80%;" />

<p>相比baseline的改变：三元输入、增加分数预测头、训练&amp;推理策略</p>
<p>训练分为两阶段：第一阶段不训练score head，搜索图像全部包含目标；第二阶段固定其他参数单独训练score head，搜索图像中有一半不包含目标（训练时只要搜索图像包含目标则认为可以更新）；</p>
<p>推理时达到更新间隔且分数大于阈值更新模板</p>
<h3 id="本文结构与DETR的区别"><a href="#本文结构与DETR的区别" class="headerlink" title="本文结构与DETR的区别"></a>本文结构与DETR的区别</h3><ol>
<li>任务不同，检测vs跟踪</li>
<li>输入不同，detr输入整个图像，本文输入三元组，一个search和两个template；</li>
<li>query和训练策略，detr有100个query并且每个都需要匈牙利匹配gt，而本文只有一个query和唯一gt；</li>
<li>预测头不同，detr三层感知器，本文基于角点预测</li>
</ol>
<hr>
<h1 id="Target-Transformed-Regression-for-Accurate-Tracking"><a href="#Target-Transformed-Regression-for-Accurate-Tracking" class="headerlink" title="Target Transformed Regression for Accurate Tracking"></a>Target Transformed Regression for Accurate Tracking</h1><p><a href="https://arxiv.org/abs/2104.00403">论文</a><br><a href="https://github.com/MCG-NJU/TREG">代码</a></p>
<h2 id="动机-3"><a href="#动机-3" class="headerlink" title="动机"></a>动机</h2><p>如何将目标信息整合到回归分支中，<strong>保留精确的边界信息</strong>并<strong>及时处理各种目标变化</strong>对于跟踪是至关重要的。</p>
<p>dw-corr将整个目标当成滤波器，只有目标的全局信息，面对物体变形时难以准确反映边界；</p>
<p>pix-corr忽略了目标模板中的少量背景会对目标外部区域赋予较大的注意力权重。</p>
<p>作者利用transform的交叉注意力来建模模板和搜索区域的每个元素之间的pair-wise关系，并用其增强原始特征。这种特征表达能够增强目标相关信息，帮助精确定位边界，并由于其局部和密集匹配机制，在一定程度上适应目标变形。</p>
<p>此外，设计了一个简单的在线模板更新机制来选择可靠的模板，提高了对目标外观变化和几何变形的鲁棒性。</p>
<h2 id="结构-3"><a href="#结构-3" class="headerlink" title="结构"></a>结构</h2><p>设计准则：</p>
<ol>
<li>目标集成模块，保留充足的目标信息以生成精确目标边界；</li>
<li>像素级的上下文建模，以增强目标相关的特征和处理形变；</li>
<li>高效的在线机制，以处理连续序列中的外观变化。</li>
</ol>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/15.png"></p>
<center>TREG整体结构，核心是黄色的target-aware transformer，其余结构参考FCOT</center>

<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/16.png"></p>
<center>Online Target-aware Transformer for Regression. (a) Target-aware transformer (b) Online template update mechanism</center>

<p>将搜索特征看成query，目标被编码成key和value，对每一个query，都利用所有key和value为其提供加权聚合响应。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/17.png" style="zoom:80%;" />

<img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/18.png" style="zoom:80%;" />

<p>$x_i$是搜索特征，$t_j$是目标特征，$Ω_k$表示目标模板的所有位置，$k$表示模板池的序号；</p>
<p>$\theta_{x_i}, \phi_{t_j}, \omega_{t_j}$ 分别表示 query, key, value；</p>
<p>注意这里归一化使用1/N而不是softmax。</p>
<blockquote>
<p>The reason lies in that some positions in background and distractors of the search region are expected to have low dependency with target, while Softmax function will amplify this noise influence as the sum of attention weights between the query and all the keys is always 1.</p>
</blockquote>
<p>在线更新模板，构建模板序列，包含3个静态模板和4个动态模板，静态的由第一帧变换增广生成，动态的取每n帧中得分最高的。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/20.png" style="zoom:80%;" />

<h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><img src="https://gitee.com/zjphust/picgo/raw/master/blog/transform-tracking/19.png" style="zoom:80%;" />

<p>图4展示物体在序列发生了变化，本文的transform增强了目标包括头部和脚在内的边界。</p>
<p>表1 TAT-Cls表示将transform用于分类，效果稍微下降，因为pixel-to-pixel的匹配方法往往忽略了目标的整体信息，不适合区分相似的对象。</p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>本文的结构和CVPR2021另外一篇文章也有些类似，即Graph Attention Tracking，可以参考我在b站的<a href="https://www.bilibili.com/read/cv8692025">笔记</a>。作者将模板和搜索特征的每个位置看成节点，使用图注意力构建局部密集的匹配关系用于加强原始特征。实现方式也和transform的交叉注意力类似，可以说是殊途同归。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>transform</tag>
        <tag>CVPR2021</tag>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title>STMTrack: Template-free Visual Tracking with Space-time Memory Networks</title>
    <url>/2021/04/27/tracking/34-stmtrack/</url>
    <content><![CDATA[<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/34-stmtrack/20210427102245.png"></p>
<p><a href="https://arxiv.org/abs/2104.00324">论文</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>离线训练的Siamese跟踪器已经充分完全挖掘了第一帧模板信息，但它们抵抗目标外观变化的能力依然有限。现有的模板更新机制大多依赖耗时的数值优化或复杂的手工设计策略，这阻碍了它们的实时跟踪和实际应用。本文提出了一种基于时空记忆网络的跟踪框架，该框架能够充分利用与目标相关的历史信息，从而更好地适应跟踪过程中的外观变化。这样避免了模板更新，所以叫template-free。主要创新点包括：</p>
<ul>
<li>引入记忆机制存储目标的历史信息，引导跟踪器聚焦在当前帧中信息最丰富的区域；</li>
<li>memory network的像素级相似度计算能够生成更精确的目标框。</li>
</ul>
<p>运行速度37 FPS</p>
<span id="more"></span>

<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/34-stmtrack/20210427102842.png"></p>
<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>整体框架如图2所示，主要包括三部分：特征提取网络、时空记忆网络和头部预测。特征提取网络包括<font color=LightGreen>绿色</font>的记忆分支和<font color=LightBlue>蓝色</font>的查询分支。记忆分支输入多个历史帧以及对应的前背景标签，查询分支输入当前帧。特征提取后，时空记忆网络从所有记忆帧中检索与目标相关的信息，生成综合特征图进行最后的分类回归。</p>
<h2 id="Feature-Extraction-Network"><a href="#Feature-Extraction-Network" class="headerlink" title="Feature Extraction Network"></a>Feature Extraction Network</h2><p>对memory的每个分支，特征提取为：</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/34-stmtrack/20210427104555.png" style="zoom:80%;" />

<p>$m_i$ 表示第i帧图像，$c_i$ 表示第i个前背景标签（根据gt构建的0/1 mask），$ \varphi_0^m $ 和 $g$ 将图像和标签统一到同一维度从而相加，$ h_m $ 对特征降维到512。</p>
<p>对于query分支：</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/34-stmtrack/20210427105233.png" style="zoom:80%;" />

<p>$ \varphi^m $ 和 $ \varphi^q $ 结构一样但<strong>参数不共享</strong>。</p>
<h2 id="Space-time-Memory-Network"><a href="#Space-time-Memory-Network" class="headerlink" title="Space-time Memory Network"></a>Space-time Memory Network</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/34-stmtrack/20210427111921.png"></p>
<p>首先计算memory的 $f_m $ 和 query的 $f_q $  每个像素之间的相似性 $\omega \in THW \times HW $</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/34-stmtrack/20210427111950.png" style="zoom:80%;" />

<p>这个操作类似non-local或self-attention，相当于用query去检索历史帧中与目标相关的信息作为<strong>软权重</strong>来加权 $f_m $。最后将加权后的结果与$f_q $ 拼接起来</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/34-stmtrack/20210427112745.png" style="zoom:80%;" />

<p>与non-local区别在于该方法采用相似性矩阵作为软权值从多个记忆帧中检索目标信息，而不是计算特征图中每个像素对的非局部自注意。</p>
<h2 id="Head-Network"><a href="#Head-Network" class="headerlink" title="Head Network"></a>Head Network</h2><p>常规的FCOS类预测头。</p>
<h2 id="Inference-Phase"><a href="#Inference-Phase" class="headerlink" title="Inference Phase"></a>Inference Phase</h2><p>训练和推理时记忆帧的数量是可以不一致的，因为不影响优化参数的数量。</p>
<p>推理时的记忆帧采样策略为：首先保留最可靠的第一帧和最相似的上一帧，剩下的帧数均匀采样</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/34-stmtrack/20210427113629.png" style="zoom:80%;" />

<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p>主要分析了是否共享backbone参数，前背景标签的作用，记忆帧的数量的影响。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/34-stmtrack/20210427114407.png" style="zoom:80%;" />

<p>加了 fb_label 之后，不共享backbone参数效果更好。可能是因为此时两个分支的特征空间已经不一样了，期望学到的东西也是不一样的。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/34-stmtrack/20210427115130.png" style="zoom:80%;" />

<p>随着记忆帧的数量增加，性能都是先增后减。</p>
<p>训练时帧数越多，可以训练的目标模式越多，但与当前帧相似的帧也会越多。在这种情况下，网络倾向于比较最相似的图像对，而不是学习当前帧与有杂波背景或部分遮挡的帧之间的相似性。</p>
<h2 id="Comparison-with-the-stateoftheart"><a href="#Comparison-with-the-stateoftheart" class="headerlink" title="Comparison with the stateoftheart"></a>Comparison with the stateoftheart</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/34-stmtrack/20210427115649.png"></p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/34-stmtrack/20210427115737.png" style="zoom:80%;" />

<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/34-stmtrack/20210427115757.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/34-stmtrack/20210427115804.png"></p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/34-stmtrack/20210427115816.png" style="zoom:80%;" />

<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>提出了一种新的基于时空记忆网络的跟踪框架。该框架摒弃了传统的基于模板的跟踪机制，使用多个记忆帧和前背景标签映射来定位查询帧中的目标。在时空记忆网络中，通过查询帧自适应地检索存储在多个记忆帧中的目标信息，使跟踪器对目标变化具有较强的自适应能力。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>Siamese</tag>
        <tag>CVPR2021</tag>
        <tag>时序</tag>
      </tags>
  </entry>
  <entry>
    <title>Real-Time Visual Object Tracking via Few-Shot Learning</title>
    <url>/2021/04/27/tracking/35-FSL-tracking/</url>
    <content><![CDATA[<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427173306.png"></p>
<p><a href="https://arxiv.org/abs/2103.10130">论文</a></p>
<p>跟踪可以看成是一个特殊的 few-shot learning (FSL) 问题，本文提出了一个通用的两阶段框架，它能够使用大量的FSL算法并且保持较快的速度。第一阶段通过SiamRPN生成若干潜在候选框，第二阶段通过少样本分类的思想对候选框进行分类。按照这种coarse-to-fine结构，第一阶段为第二阶段提供稀疏的样本，在第二阶段可以更方便、高效地进行多种FSL算法。作者选取了几种基于优化的少样本学习方法进行证明。此外，该框架可将大多数FSL算法直接应用到视觉跟踪中，使研究人员能够在这两个领域相互交流。</p>
<span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>跟踪任务要求在有限的时间内通过少量数据学习对目标和背景的分类，这与FSL任务非常相似。在少样本学习中，我们假设训练任务和新任务之间存在共享的元知识；在视觉跟踪领域，这可以解释为模型在序列中跟踪任何未见过对象的适应性。</p>
<p>已经有一些方法将FSL的概念引入跟踪了，比如DiMP和一些MAML的方法，它们将在线更新纳入离线训练阶段作为内环 (inner loop)，使得在线更新可以由手工设计转变成数据驱动。然而，这些方法大多局限于对<strong>整个图像</strong>的<strong>特定卷积核</strong>进行优化设计，而不是像在FSL中，使用<strong>稀疏样本</strong>进行<strong>更定制化的权值学习</strong>(例如矩阵乘法因子)，这限制了直接引入各种新的FSL算法，因为直接应用各种FSL算法，将整个图像的所有位置作为输入样本，必然会牺牲其跟踪速度，而且大量简单负样本会在学习中占主导导致模型判别力下降。</p>
<p>因此作者提出这个通用的两阶段级联结构，在第一阶段过滤掉大量简单负样本，从而使得第二阶段可以应用各种FSL算法在信息丰富的稀疏样本上实现高效的跟踪。作者选取了几种具有不同目标函数，优化方法，或解空间的少样本学习方法进行验证，速度在40-60 FPS。</p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Cascaded-Tracking"><a href="#Cascaded-Tracking" class="headerlink" title="Cascaded Tracking"></a>Cascaded Tracking</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427203740.png"></p>
<p>整体跟踪框架如图1所示，第一阶段利用SiamRPN++生成若干个候选框，经过ROI Align后送入第二阶段进行少样本分类。第二阶段是一个N-shot-2-way的分类任务，所有候选使用在线生成的伪标签进行标记。</p>
<h2 id="Second-Stage-as-Few-Shot-Learning"><a href="#Second-Stage-as-Few-Shot-Learning" class="headerlink" title="Second Stage as Few-Shot Learning"></a>Second Stage as Few-Shot Learning</h2><p>首先用数学形式定义任务，公式比较多直接贴图：</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427205448.png" style="zoom:80%;" />

<p>其中 $\theta$ 是meta-training stage (inner loop)中使用base loss $ L_{base} $ 优化得到的sequence-specific参数；然后用$\theta$ 去计算meta loss $ L_{meta} $，用于 meta-testing (outer loop) stage 来更新few-shot learner 的参数 $\rho$ 。而$\varphi $ 表示feature embedding的参数，就是前面一大堆特征提取之类的。</p>
<p>这里的meta loss是用于分类，所以采用focal loss：</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427211809.png" style="zoom:80%;" />

<p>其中 $\gamma$ 是可学习的缩放因子，$\theta^T$ 是前景和背景的权重。$\alpha, \beta$ 是focal loss的超参数。</p>
<p>few-shot learner $\Lambda$ 的选择对于公式1的影响巨大，它会被用来分类第一阶段得到的稀疏候选样本，下面介绍几种常用的基于优化（optimization-based）的方法。</p>
<h2 id="Optimization-Based-Few-Shot-Learners"><a href="#Optimization-Based-Few-Shot-Learners" class="headerlink" title="Optimization-Based Few-Shot Learners"></a>Optimization-Based Few-Shot Learners</h2><p>首先默认目标是一个线性凸优化问题。</p>
<p><strong>RR-prim-itr</strong> 就是在原空间求解岭回归问题，base loss $ L_{base} $ 为L2 loss，可以看成MAML的一种特例。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427215927.png" style="zoom:80%;" />

<p>其中 $\omega_n$ 是每个样本的权重，为了加速优化，借鉴DiMP中的最速梯度下降：</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427220212.png" style="zoom:80%;" />

<p><strong>RR-dual-itr</strong> 迭代求解对偶空间中的岭回归，$\theta$ 当成训练集的特征向量的线性组合。对偶变量 $a$ 作为权重因子。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427220526.png" style="zoom:80%;" />

<p>将公式5带入公式3，得到优化目标</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427220643.png" style="zoom:80%;" />

<p>用二次规划的方式求解对偶变量 $a$ ，同样可以利用GPU加速。此外，特征向量在训练集中进行线性组合可以缓解过拟合问题。</p>
<p><strong>RR-dual-cls</strong> 在对偶空间中开发了岭回归的封闭解用于分类。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427221514.png" style="zoom:80%;" />

<p>优势就是把原始闭式解形式中的 $ \Phi_T\ \Phi$ 变成了 $ \Phi \Phi_T$，使得计算量由 $O(Nd^2)$ 下降到了$O(N^2d)$ ，样本量N远小于特征维数d。</p>
<p><strong>SVM-dual-itr</strong> 在对偶空间迭代求解稀疏核用于线性分类。主要解决岭回归容易过拟合以及对噪声不鲁棒的问题。用SVM替换了最小二乘，细节不太明白，要去看原文。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427222444.png" style="zoom:80%;" />

<img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427222555.png" style="zoom:80%;" />

<h2 id="Online-Tracking"><a href="#Online-Tracking" class="headerlink" title="Online Tracking"></a>Online Tracking</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427230231.png"></p>
<p><strong>Candidate Selection</strong> 候选框根据第一阶段的检测结果用阈值为0.2的NMS进行筛选，再通过ROI Align作用于第二阶段。初始化时选择24个样本，IOU最高的作为正样本，其余为负样本，并通过数据增广生成额外的8个正样本，共同用于初始化few-shot learner。跟踪过程中，选择NMS后M=8个样本送入few-shot learner进行判断输出最终结果。用融合得分（两个阶段的得分加权相加）top-k个候选框更新support set中最老的样本，其中最高的为正样本，其余为负样本，k=4。</p>
<p><strong>Support Set Maintenance</strong> 先入先出（FIFO），对于每个样本赋予权重 $\omega$，随着与当前帧的间隔而指数衰减。原空间中的求解存储1000个样本，对偶空间只有60个，一方面是对CPU资源要求高，另外60个的性能已经很好。此外，第一帧的正样本永远会留在support set中。</p>
<p><strong>Few-Shot Learner Update</strong> 原空间用公式4更新，对偶空间用滑动平均更新</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427231851.png" style="zoom:80%;" />

<p><strong>Stage Fusion</strong> 将两个阶段的分类和回归结果融合，第二阶段除了得到分类得分，还会进行类似RCNN的回归refine。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427232137.png" style="zoom:80%;" />

<h1 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h1><p>训练类似DiMP，每个task从一个视频中选择3个support和2个query。对于第二阶段，NMS后采样16个样本，IOU大于0.8为正，小于0.2为负。每张图片总共8个样本，其中最多两个正样本。meta-training和meta-test均按照上述准则。第一阶段的SiamRPN++用ATSS进行label assign。</p>
<p>训练loss如下：</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427233344.png" style="zoom:80%;" />

<p>两个阶段都需要分类回归，分类为focal loss，回归为L1 loss。注意这里rcnn的回归是在第一阶段的正样本基础上进行的。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427234324.png"></p>
<p>比较了baseline（第一栏），不使用FSL的基于全连接的距离度量方法（第二栏），metric-based少样本学习方法（第三栏），以及本文使用的四种optimization-based少样本学习方法（第四栏）。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427234748.png"></p>
<p>只用第一阶段方法，SiamRPN++远不如DiMP；加入第二阶段后，这个差距被消除了，且速度将近领先了3倍。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427234945.png"></p>
<p>support set数量的影响，对于不同数据集最佳的数量是不一样的。但是即使是最低限度的M=40鲁棒性依然不错。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210428093340.png"></p>
<p>最后可视化了候选框及置信度，可以看到第二阶段对于区分目标和干扰物非常有效。</p>
<h2 id="Comparison-with-State-of-the-Arts"><a href="#Comparison-with-State-of-the-Arts" class="headerlink" title="Comparison with State-of-the-Arts"></a>Comparison with State-of-the-Arts</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427235535.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427235554.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/35-FSL-tracking/20210427235609.png"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文的创新点主要集中在两阶段和少样本学习。这两个东西单独来看都分别有人做过了，像SPM，SPLT就是先检候选框再分类的两阶段方法；而DiMP还有一些用了MAML的跟踪方法都属于少样本学习。作者将二者结合，最大的好处在于第二阶段得到的稀疏样本更适合FSL的任务设置，可以不用局限于任何特定类型的FSL算法，大幅加强了跟踪和FSL的联系。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>Siamese</tag>
        <tag>arxiv</tag>
        <tag>FSL</tag>
      </tags>
  </entry>
  <entry>
    <title>Learning Target Candidate Association to Keep Track of What Not to Track</title>
    <url>/2021/04/28/tracking/36-learning-target-candidate-association/</url>
    <content><![CDATA[<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210428110851.png"></p>
<p><a href="https://arxiv.org/abs/2103.16556">论文</a> <a href="https://github.com/visionml/pytracking">代码</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>Martin参与的新作，出发点和之前的<a href="https://arxiv.org/abs/2003.11014">KYS</a>类似，均指出仅外观模型不足以区分目标和干扰物，因此需要对所有潜在目标保持跟踪。不同的是KYS是通过一个传播模块隐式地跟踪所有对象，最后作用于外观模型的输出来抑制干扰响应；而本文则是借助SuperGlue显式地匹配帧间所有的候选对象，构建跟踪链，有点多目标跟踪的意思，可解释性也更强。</p>
<ul>
<li>主流跟踪方法大多聚焦于建立强大的外观模型，然而仅依靠外观模型对于干扰物的鲁棒性较差；</li>
<li>作者提出另一种思路，即对干扰物也保持跟踪。为此构建一个可学习的关联网络（受启发自SuperGlue），允许在帧与帧之间传播所有候选目标；</li>
<li>针对跟踪数据集没有对干扰物标注的问题，提出了一种结合部分标注和自监督的训练策略。</li>
</ul>
<span id="more"></span>

<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210428113928.png"></p>
<p>整体框架如上图，base tracker使用SuperDiMP预测出分数图$s$，选择高得分的几个位置用于生成目标候选 $v_i$。然后对每个候选提取一系列特征，包括分类分数$s_i$，位置$c_i$，和 backbone提取的外观模型$f_i$，将这些特征编码编码成一个单独的特征向量$z_i$。将当前帧和前一帧的所有候选送入candidate embedding network，一起处理得到每个候选对象的丰富嵌入$h_i$。最后利用这些特征来计算相似矩阵S，并使用最优匹配策略估计两个连续帧之间的候选分配矩阵A。</p>
<p>有了这个分配概率矩阵，就可以建立当前帧的所有候选目标$O$与上一帧所有识别出来的目标$O’$之间的关联，包括消失和新出现的目标。用这种传播策略来推断当前帧中的真实目标对象$\hat{o}$。此外，在在线更新目标分类器时，通过计算目标检测置信度$\beta$来管理存储和控制样本权重。</p>
<h2 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h2><p>首先将问题公式化，定义候选目标集合 $V = {v_i} _ {i=1}^N$，N为候选个数，$V’$和$V$分别表示前一帧和当前帧的候选集合。两帧的候选关联问题就是寻找$V’$和$V$之间的分配矩阵$A$。若$v’ _ {i}$和$v_{j}$关联，那么$A_{i,j}=1$，否则$A_{i,j}=0$。</p>
<p>实际操作中，并不是每个候选都能找到相应的匹配，因此引入dustbin来处理未匹配的节点。大致的想法就是额外增加一行一列来存放未匹配的节点。比如候选$v_j$只在集合$V$中出现，则$A_{N’+1,j}=1$；类似的，若$v’ _ i$ 在集合$V$中没有匹配对象，则$A_{i,N+1}=1$。</p>
<p>作者设计了一种可学习方法来预测这个分配矩阵A，首先需要提取候选对象的特征表示，接下来讨论。</p>
<h2 id="Target-Candidate-Extraction"><a href="#Target-Candidate-Extraction" class="headerlink" title="Target Candidate Extraction"></a>Target Candidate Extraction</h2><p>目标候选对象需要满足两个条件，响应得分是局部最大，且要超过一定阈值。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210428170210.png" style="zoom:;" />

<p>实现时使用$5 \times 5$ max-pooling找到局部极值，阈值$\tau=0.05$。</p>
<p>接下来为目标候选构建特征编码</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210428170144.png"  />

<p>包括响应得分$s_i$，位置$c_i$，和 backbone在该位置提取的外观模型$f_i$。$\psi$ 表示MLP，将 s 和 c 变换到和 f 一样的维度。</p>
<h2 id="Candidate-Embedding-Network"><a href="#Candidate-Embedding-Network" class="headerlink" title="Candidate Embedding Network"></a>Candidate Embedding Network</h2><p>为了进一步丰富编码特征，特别是便于提取特征的同时又能识别邻近的候选特征，作者引入了候选嵌入网络。这里借鉴了SuperGlue中的self-attention和cross-attention交换不同节点的信息。最后经过一个线性变换，得到每个候选$v_i$的编码$h_i$。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210428171233.png"></p>
<h2 id="Candidate-Matching"><a href="#Candidate-Matching" class="headerlink" title="Candidate Matching"></a>Candidate Matching</h2><p>用特征点积来度量候选$v’ _ {i} \in V’$和$v_j \in V$之间的相似性，$S_{i,j}=&lt;h’_i,h_j&gt;$，h是对应的特征编码。</p>
<p>有了相似性得分矩阵S，接下来需要构建分配矩阵A，通过最大化总体得分$\Sigma_{i,j} S_{i,j} A_{i,j} $ 可得到A，这是一个最优传输问题。</p>
<p>在此之前，还需要考虑dustbin，它是一个虚拟概念，没有对应的特征编码h，因此不能直接预测相似性得分。只有当一个候选对象与所有其他候选对象的相似度分数足够低时，它才属于dustbin。上面得到的相似度矩阵S仅代表了不考虑dustbin的候选对象之间的初始关联预测。本文没有提及如何处理dustbin的相似性得分，参照SuperGlue是给所有dustbin赋予一个相同的可学习参数。</p>
<p>下面就是给这个最优匹配问题设计约束条件：</p>
<ul>
<li>当$v’ _ i$和$v_j$匹配时，需要同时满足 $\Sigma_{i=1}^{N’} A_{i,j}=1$ 和 $\Sigma_{j=1}^{N} A_{i,j}=1$，确保一对一的匹配，即(i, j)所在行和列仅有唯一的匹配位置；</li>
<li>所有未匹配到其他候选的候选必须匹配到dustbin，数学表达为$\Sigma_{j} A_{N’+1,j}=N-M$ 和$\Sigma_{i} A_{i,N+1}=N’-M$，其中$M = \Sigma_{(i\le N’,j \le N)} A_{i,j}$表示候选之间匹配上的数量。</li>
</ul>
<p>最后通过Sinkhorn算法（迭代）解出最优分配矩阵。</p>
<p>这一套流程也是借鉴了SuperGlue，这里贴出它的示意图。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210428181327.png"></p>
<h2 id="Learning-Candidate-Association"><a href="#Learning-Candidate-Association" class="headerlink" title="Learning Candidate Association"></a>Learning Candidate Association</h2><p>上述流程的训练需要知道所有候选之间的匹配关系作为标签，但跟踪数据集只有唯一目标的标注。所以作者提出了部分监督和自监督结合的方式。</p>
<p><strong>部分监督损失：</strong>只对有标注的那对目标候选计算loss，另外为了模拟遮挡和重检测，人为地排除一些候选并将其替换为dustbin</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210428182311.png"  />

<p>其中$(l’,l)=(i,j)$，$(l’,l)=(N’+1,j)$, $(l’,l)=(i,N+1)$.</p>
<p><strong>自监督损失：</strong>自监督的一个候选$V$是由另外的候选$V’$增广变换而来的，因此它们可以构建一一对应关系$C={(i,i)}^N_{i=1}$，增广策略包括随机平移位置$c_i$，增减响应得分$s_i$，以及变换图像对特征$f_i$进行变换。此外，也像上面部分监督一样模拟了遮挡和重检测。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210428202146.png"  />

<p>最后将二者相加得到最终loss。</p>
<p><strong>Data Mining:</strong> 有价值的训练样本是那些出现干扰物导致跟踪失败或者跟踪置信度很低的子序列，这样才能包含更多候选供网络学习。因此先使用base tracker跑一遍找到这些困难样本。</p>
<p><strong>Training Details:</strong> 先训练SuperDiMP（这里不训练在线分类的参数，可能是因为想要获取更多失败的样本），跑一遍数据找到难样本。然后冻结base tracker的参数只训练后半部分网络。</p>
<h2 id="Object-Association"><a href="#Object-Association" class="headerlink" title="Object Association"></a>Object Association</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210428205314.png"></p>
<p>在线跟踪时对所有潜在对象保持跟踪，并构建跟踪链，如图3所示。若当前的候选对象都与该对象没有关联，那么该对象就会从场景中消失。若有新的对象出现则新增链并赋予新的对象id。对于关联上的对象，将得分$s_i$添加到历史分数中。此外，当候选对应关系不确定，即分配概率小于$\tau=0.75$时，我们删除旧的对象并创建一个新的对象。</p>
<p>这样所有候选对象都与一个已经存在的或新创建的对象相关联，若上一帧检测到的真实目标在当前帧有关联对象，则暂定该关联对象是目标。为了避免该关联对象是干扰，还需将它的历史得分与当前其他候选对象的得分进行比较，如果另一个对象在当前帧中获得的分类分数高于当前选择对象在过去获得的所有分数，我们就选择这个对象作为目标；否则还是保持当前选择对象不变。</p>
<p>若上一帧检测到的真实目标在当前帧没有关联对象，则进行重检测，需满足：1. 分类得分最高；2. 分类得分大于阈值$\eta =0.25$。</p>
<p>整个算法流程如下图：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210429161216.png"></p>
<center>'.' 表示访问对象的属性</center>

<h2 id="Memory-Sample-Confidence"><a href="#Memory-Sample-Confidence" class="headerlink" title="Memory Sample Confidence"></a>Memory Sample Confidence</h2><p>base tracker在线更新的训练样本采用先入先出的策略进行替换，并仅基于存在的寿命时间对样本加权。本文则是将分类得分也考虑进来，计算在线优化损失时综合考虑了寿命时间和分类得分对样本进行加权：</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210429165636.png"  />

<p>$\alpha$ 表示样本存在的时间，越早的样本值越小；$\beta$表示分类得分。此外，训练样本更新时替换的是$\alpha \beta$最小的样本而不是简单替换最早的样本。</p>
<p><strong>Inference details</strong></p>
<p>base tracker搜索区域取决于当前估计的边界框大小。当目标物体被遮挡或出视野时，跟踪器通常只检测到目标的一小部分，并估计出比之前帧更小的边界框，相应的搜索区域也会变小，这对于跟踪失败后的重捕是不利的。因此，如果在目标丢失前搜索区域大幅缩小，我们将搜索区域重置为以前的大小，以便于重新检测。</p>
<p>此外，如果只有一个高分目标出现在前一帧和当前帧中。我们选择这个候选对象作为目标，省略运行目标候选关联网络来加速跟踪。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210429165050.png"></p>
<p>memory sample confidence 在LaSOT提升比较大，NFS和UAV123没啥影响，说明它在长时跟踪中的作用更大。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210429165254.png"></p>
<p>比较了提出的部分监督损失，自监督损失和数据挖掘的作用，每一项都有提升。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210429165349.png"></p>
<p>表3分析了学习在线分类器过程中加入分类得分的影响。第一列表示增加了分类得分来考虑替换训练样本，第二列表示在计算在线损失时增加分类得分对样本加权，第三列表示在线学习时忽略了得分较低的低质量样本。</p>
<h2 id="State-of-the-art-Comparison"><a href="#State-of-the-art-Comparison" class="headerlink" title="State-of-the-art Comparison"></a>State-of-the-art Comparison</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210429172159.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210429172214.png"></p>
<p>对LaSOT数据集，$T&lt;0.7$时本文方法指标明显更高，证明其鲁棒性。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/36-learning-target-candidate-association/20210429172329.png"></p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>arxiv</tag>
      </tags>
  </entry>
  <entry>
    <title>Learning to Filter: Siamese Relation Network for Robust Tracking</title>
    <url>/2021/05/08/tracking/37-siamrn/</url>
    <content><![CDATA[<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/37-siamrn/20210508213527.png"></p>
<p><a href="https://arxiv.org/abs/2104.00829">论文</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><ul>
<li>Siamese跟踪器的训练设置只是在大量的图像对中匹配同一个目标，而忽略了它们之间的区别，因此对相似干扰物的判别能力不够好；</li>
<li>分类和回归是独立优化的，造成二者之间的不匹配。具体来说，分类置信度最高的位置对应的目标框可能并不是最准确的（类似检测中general focal loss等文章的观点）。</li>
</ul>
<p>针对上述问题，作者提出了两个模块：</p>
<ul>
<li><strong>Relation Detector (RD)</strong> 构造了一个2-way-1-shot的少样本学习方法来过滤干扰物。并且使用对比训练策略 (contrastive training strategy)，不仅学习匹配相同的目标，而且学习如何区分不同的目标 ；</li>
<li><strong>Refinement Module (RM)</strong> 将RD和分类分支获得的信息进行整合，细化跟踪结果。RM可以联合优化分类分支和回归分支，缓解两个分支的不匹配。</li>
</ul>
<span id="more"></span>

<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/37-siamrn/20210509122001.png"></p>
<p>整体框架如图2，baseline是SiamBAN，经过特征提取后，将目标和<strong>回归分支</strong>生成的proposal送入RD计算它们之间的相似性(metric-based)，RD输出的结果与分类分支的结果融合后得到最终的目标输出。</p>
<h2 id="Relation-Detector"><a href="#Relation-Detector" class="headerlink" title="Relation Detector"></a>Relation Detector</h2><p>比较目标和proposal相似性的最简单的方法就是各种线性距离（欧式距离、余弦距离等），然而面对难以分辨的干扰物时很容易失效。因此，本文提出了一种自适应的非线性比较器。该方法受启发自CVPR2020的 Few-Shot Object Detection with Attention-RPN and Multi-Relation Detector，是一个metric-based的少样本学习方法，具体包括Global Detector, Local Detector, and Patch Detector三个模块，如图3所示。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/37-siamrn/20210509165405.png"></p>
<p>回归分支响应图的每个位置都会预测一个框，提取其ROI特征(query)，与目标模板的ROI特征(support)共同送入3个Detector中：</p>
<ul>
<li>Global Detector 将目标和proposal的ROI特征拼接后做全局池化，经过几层FC得到global分数；</li>
<li>Local Detector 将两个ROI特征做DW-Corr得到local分数；</li>
<li>Patch Detector 将两个ROI特征拼接后经过若干卷积得到patch分数。</li>
</ul>
<p>最后将三个得分加起来就得到最终的匹配相似性分数，该分数能够同时考虑全局、局部以及patch之间的关系。</p>
<h2 id="Contrastive-Training-Strategy"><a href="#Contrastive-Training-Strategy" class="headerlink" title="Contrastive Training Strategy"></a>Contrastive Training Strategy</h2><p>RD的训练是一个2-way-1-shot的少样本学习过程，2-way表示目标和非目标两类。在训练RD时，作者不仅对属于目标类别的物体进行匹配，还对非目标类别的干扰物进行区分。因此构建一个三元组训练集 $(s_c, q_c, s_n)$，其中$s_c$和$s_n$分别表示positive support图像和negative support图像，$q_c$是query图像。$s_c, q_c$取自同一个视频，而$s_n$取自其他视频（表示干扰物）。通过输入的三元组图像可以生成不同的样本组合，定义$s_p$为positive support的gt，$p_p$表示positive proposal，$n_n$表示negative support的gt，$p_n$表示negative proposal，将它们结合可以得到不同的样本对 $(s_p, p_p), (s_p, p_n), (n_n, p_p / p_n)$（这些样本对就是图3中输入的support和query），其比例为1: 2: 1。</p>
<p>在训练初期，应用简单样本可以使模型稳定收敛。为了进一步增强模型的判别能力，在训练中后期引入难样本挖掘，包括离线和在线两种方式。在线就是从IOU小于0.2的proposal中选择得分最高的作为难负样本；离线就是像SiamRCNN一样构建索引表，从中选取在嵌入空间中的最接近的难样本。</p>
<h2 id="Refinement-Module"><a href="#Refinement-Module" class="headerlink" title="Refinement Module"></a>Refinement Module</h2><p>分类和回归的独立优化导致分类得分最高的位置对应的框不一定是最准确的，甚至可能都不是目标，因此作者通过将RD嵌入孪生框架使得分类和回归的学习能够统一。具体来说，首先将RD的输出转换成$25 \times 25 \times 1$的匹配分数图，这个输出就反映了每个位置预测框内的物体与目标之间的相似性；将其作为权重与分类分支的分数图进行元素点乘，可以过滤掉背景中的干扰；最后将refine后的分数图经过一层卷积得到最后的分类得分，并获取最大响应位置对应的预测框。图4展示了一些响应图可视化结果，可以看到，RM将回归分支和分类分支的信息结合起来预测目标位置，从而缓解了不匹配的问题。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/37-siamrn/20210509213558.png" style="zoom:80%;" />

<h2 id="Ground-truth-and-Loss-function"><a href="#Ground-truth-and-Loss-function" class="headerlink" title="Ground-truth and Loss function"></a>Ground-truth and Loss function</h2><p>大部分都和SiamBAN一样，增加的RD模块采用MSE loss</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/37-siamrn/20210509215957.png"></p>
<p>其中$r_{i,j}$表示位置$(i,j)$处的relation score；$y_{i,j}$是对应标签，上文提及的三种样本对中，$(s_p, p_p)$标签为1，$(s_p, p_n), (n_n, p_p / p_n)$标签为0。</p>
<h2 id="Training-and-Inference"><a href="#Training-and-Inference" class="headerlink" title="Training and Inference"></a>Training and Inference</h2><p>训练首先从来自同一个序列的模板和搜索区域中选择16个正样本和48个负样本来训练分类和回归分支，然后再加入另一个序列中的搜索图像（负例）用于生成RD的训练样本。第5个epoch开始在线难样本挖掘，第15个epoch开始离线难样本挖掘。整个网络端到端训练不需要fine-tune。</p>
<p>推理阶段第一帧先提取模板特征并通过precise ROI pooling提取ROI特征。后续帧对回归分支每个位置的输出生成proposal，提取proposal的ROI特征与模板ROI特征共同送入RD计算它们之间的关系，最后将RD的输出与分类分支的输出送入RM得到最终结果。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Comparison-with-the-state-of-the-art"><a href="#Comparison-with-the-state-of-the-art" class="headerlink" title="Comparison with the state-of-the-art"></a>Comparison with the state-of-the-art</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/37-siamrn/20210509223013.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/37-siamrn/20210509223028.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/37-siamrn/20210509223105.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/37-siamrn/20210509223114.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/37-siamrn/20210509223124.png"></p>
<h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p>比较了多尺度预测和使用不同组合的Relation Head的结果。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/37-siamrn/20210509223228.png"></p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>Siamese</tag>
        <tag>arxiv</tag>
        <tag>FSL</tag>
      </tags>
  </entry>
  <entry>
    <title>Updatable Siamese Tracker with Two-stage One-shot Learning</title>
    <url>/2021/05/10/tracking/38-siamtol/</url>
    <content><![CDATA[<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510102825.png"></p>
<p><a href="https://arxiv.org/abs/2104.15049">论文</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>本文主要解决孪生跟踪器缺乏在线更新能力的问题。传统的线性模板更新难以处理目标的不规则变化和采样噪声，造成跟踪漂移；而一些像updatenet采用网络进行自适应更新的方法，其更新网络和跟踪器在结构上是分离的，不能从联合训练中受益，也不能以最佳方式合作。</p>
<p>为了实现高质量的自适应更新，作者从 one-shot learning的角度提出一个two-stage one-shot learner，利用不同阶段的目标样本预测分类器的参数。具体来说，除了使用模板分支来学习初始目标特征，作者额外增加了一个输入分支用于捕获后续帧中的目标特征，并设计了一个残差模块来使用这些特征更新初始模板。通过残差学习融合多帧目标特征，跟踪器可以用更合适的模板跟踪当前目标。此外，还设计了一种多方面(multi-aspect)的训练损失来避免过拟合。</p>
<span id="more"></span>

<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="One-shot-learning-formulation"><a href="#One-shot-learning-formulation" class="headerlink" title="One-shot learning formulation"></a>One-shot learning formulation</h2><p>SiamRPN指出孪生跟踪框架是一个one-shot learner，即通过初始帧的一次学习使得模型能够跟踪到后续帧中的目标，本节首先对其进行公式化定义。</p>
<p>目标跟踪的典型框架是判别分类器$\varphi (x,W)$，目标是在训练数据集上找到能使总损失L最小化的参数W：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510115919.png"></p>
<p>$n$表示训练样本数，$l_i$是样本$x_i$的标签。尽管分类器在目标跟踪上具有很强的竞争力，但需要大量计算量和样本在线训练学习。</p>
<p>另外一种跟踪框架是孪生网络，目标是学习模板和搜索区域之间的相似性度量：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510152537.png"></p>
<p>$z_i$和$x_i$分别表示模板和搜索区域，$\varphi ‘$和 $\zeta$ 分别表示特征提取网络和匹配网络。</p>
<p>进一步分析，我们发现公式2的孪生网络模型可以被重新解释为类似公式1的one-shot分类器模型。对于分类器$\varphi (x,W)$，若仅通过一个感兴趣样本$z_i$就能学到分类器参数$W$，那么这就是一个one-shot learner。因此，孪生网络的模板分支可以看成是一个元学习函数$\omega$，它将模板特征映射成分类器参数$W$；而搜索分支和互相关就是一个检测器，整个目标函数定义如下：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510154435.png"></p>
<p>至此，我们就把孪生框架解释为了one-shot learning，图1直观展示了这种表达方式</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510155519.png"></p>
<h2 id="Two-stage-One-shot-learner"><a href="#Two-stage-One-shot-learner" class="headerlink" title="Two-stage One-shot learner"></a>Two-stage One-shot learner</h2><p>孪生网络框架只能在初始帧这个stage通过one-shot learning学到目标信息，因此无法在线更新。那自然会想能不能让这个learner在不同的stage（跟踪阶段）去学习目标信息呢？因此作者提出了 two-stage one-shot learner (TOL)，可以结合具有不同属性的样本来预测分类器的参数$W$。目标函数定义如下：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510160306.png"></p>
<p>相比公式3，就是增加了一个来自后续帧中的样本$u_i$来学习$\omega$ 。</p>
<h2 id="Updatable-Siamese-Network"><a href="#Updatable-Siamese-Network" class="headerlink" title="Updatable Siamese Network"></a>Updatable Siamese Network</h2><p>基于上述的 two-stage one-shot learner，本文提出一个可更新的孪生网络，如下图</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510162506.png"></p>
<p>相比原始的孪生框架，增加了一个update分支，然后把两个分支的目标特征进行融合。其实抛开上面讲的那些one-shot learning也不妨碍理解这个结构，无非就是把初始帧和跟踪过程中的历史帧通过网络融合生成一个更好的模板以适应跟踪中的变化。</p>
<p>融合模板的过程（或者说元学习函数）定义如下：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510163640.png"></p>
<p>其中$zf$和$uf$分别表示模板和更新样本的特征，M表示特征融合网络（三层卷积）。</p>
<p><strong>Multi-aspect loss training</strong> 训练网络时在模板和搜索图像之间的间隔图像中额外扣一个更新样本，并且计算损失分别考虑了模板样本-搜索样本，更新样本-搜索样本，融合模板样本-搜索样本三方面损失，如图3所示。这样做是因为网络包括一个基本的孪生跟踪器和一个在线调整的更新器两部分，如果直接用一个整体损失训练，网络可能难以平衡这两部分。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510164536.png"></p>
<p><strong>Online update</strong> 在线跟踪过程中，更新样本每N=10帧更新一次，且满足置信度大于阈值0.9。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Comparison-with-the-state-of-the-arts"><a href="#Comparison-with-the-state-of-the-arts" class="headerlink" title="Comparison with the state-of-the-arts"></a>Comparison with the state-of-the-arts</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510165043.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510165257.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510165536.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510165551.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510165627.png"></p>
<h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/38-siamtol/20210510165730.png"></p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>Siamese</tag>
        <tag>arxiv</tag>
        <tag>FSL</tag>
        <tag>模型更新</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepMix: Online Auto Data Augmentation for Robust Visual Object Tracking</title>
    <url>/2021/05/10/tracking/39-deepmix/</url>
    <content><![CDATA[<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/39-deepmixl/20210510171656.png"></p>
<p><a href="https://arxiv.org/abs/2104.11585">论文</a></p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><ul>
<li>通过历史帧样本在线更新目标模型对跟踪具有重要意义。最近的研究主要集中在构建有效的更新方法，而忽略了用于学习判别模型的训练样本；</li>
<li>本文提出DeepMix，对历史帧样本的特征进行在线增广，从而强化模型的在线更新能力。具体包括通过object-aware filtering在线增强历史样本，以及通过离线训练的MixNet混合多个样本进行数据增强；</li>
<li>最后通过三个典型的跟踪器DiMP, DSiam和SiamRPN++验证提出的方法。</li>
</ul>
<span id="more"></span>

<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>首先点明数据增广对于提升模型判别能力非常重要，因此可以利用数据增广给现有的模型更新方法增加有效的训练样本。用公式表达如下：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/39-deepmixl/20210510203101.png"></p>
<p>其中$X_t$表示原始训练样本，$T$表示各种增广方法，$\hat {X_t}$表示增强后的样本。而现有的不管是传统还是深度学习的增广方法都不能直接适应跟踪的在线数据增强，主要包括两个原因：</p>
<ol>
<li>现有方法都是sample-level的，即会生成新的样本，对这些新样本需要额外花费大量时间提取它们的深度特征；</li>
<li>现有方法都是基于退化因素(例如，噪声、模糊、雾、雨等)，这些退化因素会破坏原始样本，降低模型判别力。</li>
</ol>
<p>为了解决上述问题，作者提出要对训练样本的<strong>特征</strong>(embedding)进行增强。具体来说，首先提取样本$X_t$的特征embedding得到${\varphi(I_i) \in \mathbb{R}^{C \times W \times H} | I_i \in X_t}$，然后将所有embedding拼接起来得到特征张量 $X_t \in \mathbb{R}^{N \times C \times W \times H} $，N是训练样本数量。之后把$X_t$通过数据增强映射到一个新的特征$\hat{X_t} \in \mathbb{R}^{K \times C \times W \times H} $ 送入更新模块以生成目标模型。</p>
<p>在特征上进行增强比直接在样本上增强更加高效，此外，作者在混合样本特征时加入了对应跟踪结果的指导。从直观上看，在视频采集过程中，感兴趣的物体可能位于场景中的任何位置，将物体放置在可能的背景区域中来增加训练样本是合理的。因此，作者根据跟踪框将样本分为目标区域和背景区域，并把它们混合以生成新样本。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/39-deepmixl/20210510211559.png"></p>
<p>其中 $M_o \in \mathbb{R}^{K \times C \times W \times H} $（此处作者笔误，第一个维度为K而不是N，已向作者求证）是二值掩膜，1表示目标区域，0表示背景区域。$W_{ {o \ or \ b} } \in \mathbb{R}^{K \times N \times 3 \times 3} $表示卷积，K是输出样本的个数。这个卷积的目的就是混合N个原始样本生成K个新样本，其中$W_o$和$W_b$分别负责混合目标和背景区域。下一步就需要考虑如何计算$W_o$和$W_b$，作者设计了MixNet，如图2所示。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/39-deepmixl/20210510212546.png"></p>
<p>输入$X_t$，通过两个结构一致参数不同的卷积分支生成kernel $W_o$和$W_b$。这个网络可以预先离线训练好，在线跟踪时只需一次前向传播就可以高效地获得增强样本。</p>
<p><strong>Implementation for SOTA Trackers</strong></p>
<p>为了验证DeepMix的效果，作者在SiamRPN++, DSiam和DiMP三个跟踪器上进行实验。将原始样本$X_t$和增强样本$\hat {X_t}$相加后 ($\alpha_1 \hat {X_t} + \alpha_2 X_t$) 送入更新模块，$\alpha_1=0.05, \alpha_2=0.8$。<strong>注意</strong> 不是直接用生成的新样本替换原始样本，而是将两者进行混合。</p>
<p>对于SiamRPN++和DSiam，将历史N=15帧样本混合成K=1个新样本，注意SiamRPN++虽然并没有更新模块，但是MixNet也可以混合历史样本输出一个更干净的搜索特征从而促进跟踪。而在训练时，将搜索图像随机增广成15个样本送入MixNet生成1个新样本与原始图像混合后输入到之后的模块。<strong>论文这里作者笔误写成了对模板进行增广，已向作者求证。</strong></p>
<p>对于DiMP，则将历史N=50帧样本混合成K=N个新样本。同样将MixNet嵌入DiMP的训练中，为了与测试保持一致，将训练过程中的3个样本改成了50个。</p>
<p>这里额外提一下关于生成样本的mask $M_o \in \mathbb{R}^{K \times C \times W \times H} $如何设置。对于SiamRPN++，K=1，mask根据上一帧的跟踪结果设置；对于DiMP，K=N，mask就直接根据这N帧跟踪结果设置。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="State-of-the-art-Comparison"><a href="#State-of-the-art-Comparison" class="headerlink" title="State-of-the-art Comparison"></a>State-of-the-art Comparison</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/39-deepmixl/20210510214422.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/39-deepmixl/20210510214430.png"></p>
<p>尽管baseline比原始论文得到的要低一些，但是加入了DeepMix性能依然明显提升。</p>
<h2 id="Ablation-study"><a href="#Ablation-study" class="headerlink" title="Ablation study"></a>Ablation study</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/39-deepmixl/20210510215021.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/39-deepmixl/20210510215028.png"></p>
<p>表3中DeepMix在ResNet18的提升比ResNet50大，可能是因为更强大的网络对于增广的依赖更小。</p>
<p>表4比较了DeepMix几种不同的实现方法，-Opt表示通过梯度下降在线优化目标函数来取代所提出的MixNet，-single表示不区分目标和模板区域只用一个分支生成新样本。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>这篇工作的切入点挺新奇的，以往的模型更新都在考虑如何设计更新方法，而本文则关注样本本身，从在线数据增广的角度切入，将历史样本混合生成新的样本用于模型更新。本文灵感应该还是来自于Mix类的增广方法，那是不是意味着我们可以把其他数据增广的方法也运用进来，刚好我最近也在思考类似的问题，本文可以给到一定的启发。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>模型更新</tag>
        <tag>数据增广</tag>
        <tag>ICME2021</tag>
      </tags>
  </entry>
  <entry>
    <title>Siamese Natural Language Tracker: Tracking by Natural Language Descriptions with Siamese Trackers</title>
    <url>/2021/05/19/tracking/40-snlt/</url>
    <content><![CDATA[<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519110756.png"></p>
<p><a href="https://arxiv.org/abs/1912.02048">论文</a></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文研究的课题为tracking by natural language(NL) 。人类的学习过程是视觉和语言共同作用的，而在基于外观的跟踪过程中引入语言描述同样可以使得跟踪器更加精确、灵活和鲁棒（如图1的例子）。因此，本文将孪生跟踪器与语言描述结合，将语言描述编码成一个卷积核嵌入到孪生框架中（SNL-RPN），并将视觉和语言的预测进行动态聚合（Dynamic Aggregation），为tracking by NL任务提供了一个新的baseline。具体贡献总结如下：</p>
<ol>
<li>提出一种新的tracking by NL的baseline，Siamese Natural Language Region Proposal Network (SNL-RPN)；</li>
<li>提出了一种基于视觉和语言预测的动态聚合（Dynamic Aggregation），将SNL-RPN转换为Siamese Natural Language Tracker (SNLT)；</li>
<li>在NL标注的数据集上将孪生跟踪器的性能提升了3-7个百分点，并且性能超过其他NL tracker，速度为50FPS。</li>
</ol>
<span id="more"></span>

<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519113920.png"></p>
<center>图1 （摘自CVPR2021 TNL2K） (a) 只利用box时跟踪器可能会混淆跟踪目标是自行车还是人的腿，加入语言描述使得跟踪对象更加清晰；(b) 当目标发生剧烈形变时加入语言描述可以减少漂移；(c) 语言描述可以更灵活地指定目标，如这里需要跟踪持球的运动员，使用传统跟踪器达到这一效果需要反复重新初始化跟踪器。  </center>

<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519115620.png"></p>
<p>上图展示了SNLT整体框架，将语言描述嵌入到SiamRPN++中。输入包含三部分：模板、搜索区域和查询语言Q。模板和搜索区域经过卷积提取特征$Z$和$X$，查询语言经过语言模型（BERT, GloVe, HGLMM）提取句子嵌入编码$Z_Q$。三元组$(Z,Z_Q,X)$送入SNL-RPN，分别预测视觉和语言的分类和回归响应。最后二者通过Dynamic Aggregation Module进行融合。</p>
<h2 id="Architecture-of-the-SNL-RPN"><a href="#Architecture-of-the-SNL-RPN" class="headerlink" title="Architecture of the SNL-RPN"></a>Architecture of the SNL-RPN</h2><p>SNL-RPN如上图b所示，包括蓝色部分的visual head和红色部分的NL head，都是通过DW-XCorr生成相应的的分类和回归响应。类似SiamRPN++，同样在ResNet的三个stage上进行预测，并将结果融合，融合权重通过离线训练得到。visual head和NL head各包含两个分支，总共有四组融合参数：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519154224.png"></p>
<p>$S_{VIS}, S_{NL}$表示分类分支的视觉和语言预测，$B_{VIS},B_{NL}$表示回归分支的两种预测。</p>
<h2 id="Aggregation-of-the-SNL-RPN-Predictions"><a href="#Aggregation-of-the-SNL-RPN-Predictions" class="headerlink" title="Aggregation of the SNL-RPN Predictions"></a>Aggregation of the SNL-RPN Predictions</h2><p>上一节对不同stage的预测融合，本节作者对视觉和语言的结果再次进行融合：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519160117.png"></p>
<p>$\omega_{VIS},\omega_{NL}$ 可以简单像上面一样离线训练得到，但是融合对象来自两个不同的输入，用固定的权重融合并不是最优的。因此，作者提出一个基于预测响应图熵的动态融合方式：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519160727.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519160737.png"></p>
<p>其中$\sigma$表示softmax函数，$\alpha$表示缩放因子。公式4的含义是<strong>熵越大的项赋予更小的权重</strong>。熵越大表示预测图分布越混乱，越不准确，所以权重更小。</p>
<h2 id="Training-the-SNL-RPN-and-Loss-Functions"><a href="#Training-the-SNL-RPN-and-Loss-Functions" class="headerlink" title="Training the SNL-RPN and Loss Functions"></a>Training the SNL-RPN and Loss Functions</h2><p>训练设置和siamese框架一样，额外增加了一个语言模型。损失函数分别计算视觉和语言对应的分类回归损失。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519161412.png"></p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>训练数据集使用的是 MSCOCO, YouTube-BB, VisualGenome, LaSOT, OTB-99-LANG，其中后三个都是有语言标注的，前两个只有类别标注。</p>
<p>测试数据集使用有语言标注的OTB-99-LANG和LaSOT。</p>
<h2 id="Comparison-with-Visual-and-NL-Trackers"><a href="#Comparison-with-Visual-and-NL-Trackers" class="headerlink" title="Comparison with Visual and NL Trackers"></a>Comparison with Visual and NL Trackers</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519161835.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519161846.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519161952.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519162121.png"></p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519162142.png" style="zoom:80%;" />

<p>表2最后一行表示只使用语义类别作为语言输入测试LaSOT（原始的LaSOT里语言描述是一句话），这样效果会下降，证明SNLT可以学到比语义类别更多的东西。</p>
<h2 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h2><img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519162612.png" style="zoom:80%;" />

<p>表1对比了不同语言模型的效果</p>
<p><img src="C:/Users/zjp/AppData/Roaming/Typora/typora-user-images/image-20210519162738919.png" alt="image-20210519162738919"></p>
<p>图6的两个例子，第一个展示了语言描述可以辅助跟踪器避免模型漂移，而第二个例子语言描述不能唯一地描述目标，导致跟踪器漂移。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/40-snlp/20210519163102.png" style="zoom:80%;" />

<p>图8展示了视觉和语言的分类响应图，NL head的响应更加准确，可能是由于遮挡阻碍了视觉模型。通过计算响应图的熵，赋予熵更小的NL响应图更大的权重。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>Siamese</tag>
        <tag>CVPR2021</tag>
        <tag>Natural Language</tag>
      </tags>
  </entry>
  <entry>
    <title>SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking</title>
    <url>/2021/05/26/tracking/41-siamrcr/</url>
    <content><![CDATA[<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/41-siamrcr/20210526210529.png"></p>
<p><a href="https://arxiv.org/abs/2105.11237">论文</a></p>
<p>本文解决的是老生常谈的分类和回归不匹配的问题。作者提出在分类和回归之间建立<strong>双向</strong>的连接，可以动态地重新加权每个正样本的损失。此外，增加了一个定位分支用于预测定位精度，可以在推理过程中替代回归辅助连接(regression assistance link)，使得训练和测试更加一致。最终运行速度为65FPS。</p>
<span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/41-siamrcr/20210526212918.png"></p>
<p>首先点出问题，即孪生跟踪架构中分类和回归是分开独立优化的，导致二者不匹配。如图1所示，分类得分最高的位置生成的预测框不一定是最好的，或者预测比较好的框分类得分很低。这个图和IOUNet中的图很相似，事实上，这个问题在检测任务中已经被很多学者研究，并且有些成果也被应用到跟踪中。比如SiamFC++借鉴FCOS架构增加了一个衡量定位精度的分支，ATOM/DiMP系列使用IOUNet进行回归。作者指出这些方法仍然存在不匹配，因为并没有解决分类和回归独立优化的问题。</p>
<p>因此，本文提出在分类和回归之间建立一个互惠关系 (reciprocal relationship)，使它们同步优化，以生成精度一致的输出。整体框架如图2所示，在分类和回归分支之间增加了两个连接 classification / regression assistance link。classification assistance 用分类置信度给回归损失加权，使得回归可以更关注高置信度的位置；regression assistance 用定位精度（预测框和gt的IOU）给分类损失加权，迫使分类分数与回归精度更加一致。</p>
<p>而在推理阶段，gt是未知的，无法通过计算IOU得到定位精度，因此额外增加一个定位分支专门用于预测定位精度。将分类置信度与定位预测置信度相乘，在推理阶段生成新的跟踪评分/置信度图，保证了与训练过程的一致性（类似FCOS / SiamFC++）。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/41-siamrcr/20210526221151.png"></p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>图2整体是一个anchor-free的孪生框架，预测头部分采用的是CenterNet的中心、中心偏移量、宽高的形式。这里将中心偏移量和宽高放在同一个分支输出，所以回归分支输出维度是4 ($t^{<em>}_{x,y}=(w^</em>, h^*, \Delta x^*, \Delta y^*)$)。</p>
<h2 id="Reciprocal-Classification-and-Regression"><a href="#Reciprocal-Classification-and-Regression" class="headerlink" title="Reciprocal Classification and Regression"></a>Reciprocal Classification and Regression</h2><p>下面开始介绍两种辅助连接，设计原则就是：</p>
<ol>
<li>当回归框的定位精度较低时，相应的分类得分不应该很高，因为如果该位置成为分类置信度的赢家，差的回归结果将导致跟踪性能较差；</li>
<li>当回归框的分类分数较低时，提高其定位精度是没有意义的，因为这个框一定不会是最后的输出。</li>
</ol>
<p><strong>Regression Assistance Link</strong> 针对准则1，将回归分支生成的框与gt计算IOU，看成一种动态的样本重加权作用于分类损失：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/41-siamrcr/20210526223907.png"></p>
<p>$N_{pos}$表示正样本个数，$B,B^*$分别表示预测框和gt。</p>
<p><strong>Classification Assistance Link</strong> 针对准则2，将分类置信度 $p^{cls}_{x,y}$ 动态地重新加权回归损失：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/41-siamrcr/20210526225051.png"></p>
<p><strong>Localization Score Branch</strong> 在训练时，regression assistance link使得分类分支考虑了回归精度，但这需要借助gt。而在推理阶段没有gt，为了保证训练和推理的一致性，使得推理阶段的分类分支也能考虑定位精度，作者额外增加了一个定位分支专门用于预测定位精度，作用类似FCOS / SiamFC++的centerness。其训练损失就是计算定位分支输出与IOU之间的交叉熵损失。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/41-siamrcr/20210526225941.png"></p>
<p>推理阶段示意图如图3，将分类分支和定位分支的得分相乘生成最后的跟踪分数图。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/41-siamrcr/20210527094602.png"></p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/41-siamrcr/20210526230610.png"></p>
<p>表1中这两个组件的性能提升几乎是正交的，联合使用的提升(5.05%)几乎等于单独使用时各自性能提升的和(3.54%+2.86%)。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/41-siamrcr/20210526234418.png"></p>
<p>图4展示了不同方法的跟踪分数和回归框IOU之间的相关性，(a)是baseline，(b)是centerness，(c)是basline+定位分支，(d)是SiamRCR。其中(b)和(c)类似，区别是分别使用IOU和centerness来衡量定位精度。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/41-siamrcr/20210526234837.png"></p>
<p>最后展示了划分正负样本时不同半径对结果的影响。</p>
<h2 id="Comparison-with-the-State-of-the-Art"><a href="#Comparison-with-the-State-of-the-Art" class="headerlink" title="Comparison with the State-of-the-Art"></a>Comparison with the State-of-the-Art</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/41-siamrcr/20210526234927.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/41-siamrcr/20210526234937.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/41-siamrcr/20210526234948.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/41-siamrcr/20210526234956.png"></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文解决了孪生跟踪框架中分类和回归不一致的问题。印象中最先研究这个问题的是检测领域的PISA (Prime Sample Attention in Object Detection)，根据IOU对样本进行排序然后选择prime sample。之后也有许多相关文章，包括本文的Regression Assistance Link也能看到一些检测里面的影子（吐槽一下我19年投了一篇文章有个点和这个一毛一样然而到现在都还没中hhh）。但目前的研究其实基本上都是在用回归矫正分类，而本文做了一个双向的矫正，（显式地）增加了用分类矫正回归的过程（为什么说是显式，因为其实只要把分类和回归乘到一起，这个作用就是相互的，比如公式2中，我可以把focal loss当成权重，IOU当成优化变量，那就变成了分类矫正回归）。这种显式的矫正似乎监督能力更强，如果有个消融实验对比一下单独使用公式3和公式4的效果就更直观了。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>Siamese</tag>
        <tag>IJCAI2021</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习在目标跟踪中的应用</title>
    <url>/2021/06/08/tracking/42-rl-tracking/</url>
    <content><![CDATA[<p>强化学习讨论的问题是智能体(agent) 如何在一个复杂不确定的环境(environment) 里去最大化它能获得的奖励。 今天介绍三篇关于强化学习在目标跟踪中的工作，分别利用强化学习来决策使用的特征，多个跟踪器的切换以及是否更新模板。</p>
<p>论文列表：</p>
<ul>
<li><a href="https://arxiv.org/abs/1708.02973">Learning Policies for Adaptive Tracking with Deep Feature Cascades</a></li>
<li><a href="https://proceedings.neurips.cc//paper/2020/file/885b2c7a6deb4fea10f319c4ce993e02-Paper.pdf">Online Decision Based Visual Tracking via Reinforcement Learning</a></li>
<li><a href="https://arxiv.org/abs/2004.07538">Fast Template Matching and Update for Video Object Tracking and Segmentation</a></li>
</ul>
<span id="more"></span>

<h1 id="强化学习方法"><a href="#强化学习方法" class="headerlink" title="强化学习方法"></a>强化学习方法</h1><p>强化学习包含环境, 动作和奖励三部分, 其本质是agent 通过与环境的交互, 使得其作出的action所得到的总奖励达到最大, 或者说是期望最大。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210608202931.png" style="zoom: 50%;" />

<p>强化学习方法主要可以分为Value-Based，Policy-Based以及二者结合的Actor-Critic方法。Value-Based方法通过Temporal Difference (TD) Learning学习动作价值函数；Policy-Based方法通过Policy Gradient学习策略函数；而Actor-Critic方法将二者结合，actor学习一个策略来得到尽量高的回报，critic对当前策略的值函数进行估计，即评估actor的好坏。</p>
<p>关于强化学习的更多细节可以参考王树森老师的视频课程<a href="https://www.bilibili.com/video/BV12o4y197US">【王树森】深度强化学习(DRL)</a></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210608203415.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210608204616.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210608204711.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210608204741.png"></p>
<hr>
<h1 id="Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades"><a href="#Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades" class="headerlink" title="Learning Policies for Adaptive Tracking with Deep Feature Cascades"></a>Learning Policies for Adaptive Tracking with Deep Feature Cascades</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>第一篇来自ICCV2017，出发点是不同复杂程度的跟踪场景对特征的需求是不同的，对于简单场景使用浅层特征（甚至像素特征）就能处理，而对于一些复杂场景才需要具有更强语义信息的深度特征。这个<strong>自适应决策</strong>的问题可以通过基于Q-learning的强化学习完成，如图1所示，学习一个agent来判断当前特征是否已经可以以较高的置信度定位目标，还是需要继续计算更深层的特征来寻找目标。</p>
<p>这样对简单目标提前终止的策略可以大幅提升推理速度，相比baseline平均速度提升了大约10倍，GPU速度158.9FPS，并且在cpu上也能以23.2FPS的速度接近实时运行。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210629104958.png"></p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>首先定义一些公式符号，孪生网络每一层的互相关层定义：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210629121458.png"></p>
<p>其中$\varphi_l$ 表示第 $l$ 层的特征，$F_l$ 表示第 $l$ 层的互相关结果。</p>
<p>整体框架如图2所示，在每一层互相关结果$F_l$后面接一个Q-Net，用于判断是否在该层停止，或者调整预测框的形状并继续使用下一层特征。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210629121328.png"></p>
<p>agent采用强化学习的方式训练，基本元素包括状态(state S) ，动作(action A)和奖励(reward R)。在每一个step即第l层中，agent根据当前状态$S_l$ 采取动作$A_l$来决定是否调整预测框或者在该层停止并输出结果，动作$A_l$的目的是减少预测的框的不确定性。训练时根据预测框与GT的IOU给出相应的奖励$R_l$（有正有负），通过最大化期望奖励，agent能学到最好的决策来采取行动，在精度和效率上取得平衡。</p>
<p><strong>Actions:</strong> 包括7个各向异性的的尺度变换和一个stop动作。7个尺度变换里包括2个全局的缩放和4个局部缩放，缩放比例为0.2。还有一个不缩放(no scaling)的动作，这一操作用于在当前响应图不明确或无法做出决策时推迟决策。</p>
<p><strong>States:</strong> 状态是一个包含响应图$F’_l$和历史动作$h_l$的二元组 $(F’_l,h_l)$。$F’_l$使用的是当前层和之前所有层响应图的平均，相当于结合了浅层的细节和深层的语义。$h_l$包含历史4个动作的向量，每个动作是8维的one-hot的向量，所以$h_l$总共是32维。</p>
<p><strong>Rewards:</strong> 奖励函数$R(S_{l-1},S_l)$反应了采取动作$A_l$后，从状态$S_{l-1}$到状态$S_l$的定位精度提升（或下降），精度采用IOU衡量，奖励函数计算如下：</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210629174619.png" style="zoom:80%;" />

<p>当动作不为stop时，若该动作能使IOU增大，则奖励+1，否则惩罚-1。若采取任何尺度变换都不能进一步提升IOU或者已经到达最后一层了，则采取stop动作，此时以IOU阈值0.6来决定奖惩。</p>
<p><strong>Deep Q-learning</strong>：本文使用value-based的DQN来选择动作，该方法需要学习一个动作-价值函数$Q(S_l,A_l)$, 选择能够使得Q最大的动作A。Q函数用网络模拟，如图2虚线框所示，包含两个128维的FC层，输出对应8维动作的回报。训练时采用TD learning进行迭代。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210629181015.png" style="zoom:80%;" />

<p>其中R表示当前奖励，$Q(S‘,A’)$表示未来总的回报，$\gamma$是折扣因子。</p>
<p>测试阶段无需奖励，只根据Q函数调整预测框直到输出stop动作。作者在OTB50上验证平均只需要2.1步输出结果，即只需要两层网络，因此可以大幅提速。</p>
<p>此外，这套策略还可以集成一些简单的特征，比如像素特征和hog特征，计算更快。</p>
<p>图3展示了一些early stop的例子，如跟踪清晰的人脸时只需C1-C2的特征，但跟踪一个模糊的人脸则需要更深层的C5特征。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210629194612.png"></p>
<hr>
<h1 id="Online-Decision-Based-Visual-Tracking-via-Reinforcement-Learning"><a href="#Online-Decision-Based-Visual-Tracking-via-Reinforcement-Learning" class="headerlink" title="Online Decision Based Visual Tracking via Reinforcement Learning"></a>Online Decision Based Visual Tracking via Reinforcement Learning</h1><h2 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h2><p>第二篇来自NIPS2020。目前主流的跟踪方法有基于检测的和基于模板匹配的，二者各有优劣。基于检测的方法容易受遮挡等影响错误更新网络，但是能适应形变；而基于模板匹配的方法只利用第一帧模板，与上述情况刚好相反。很自然会想到将二者结合，但这是两套完全不同的跟踪原理，直接融合并不能同时收敛到各自的最优解。因此本文提出了一个基于分层强化学习(HRL)的在线决策机制。决策机制实现了一种智能切换策略，其中检测器和模板跟踪器必须相互竞争，以便在它们擅长的不同场景中进行跟踪。</p>
<h2 id="Method-1"><a href="#Method-1" class="headerlink" title="Method"></a>Method</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210629205511.png"></p>
<p>整体框架如图2所示，包括决策模块和跟踪模块。决策模块是一个Actor-Critic（or Option-Critic？）结构，包括switch network和termination network。首先将初始帧模板和上一帧跟踪结果送入switch network，输出一个二元信号选择跟踪器。跟踪器结果送入termination network，输出终止当前跟踪器的概率。注意这里终止之后并不一定切换到另一个跟踪器，因为并不能保证另一个就更好，而是要经过switch network重新选择。</p>
<p><strong>Decision Module</strong></p>
<p>给定一组状态$S$和动作$A$，马尔可夫选项$\omega \in \Omega$ 包括三部分：intra-option policy $\pi: S \times A \rightarrow [0,1]$, termination condition $\beta: S^{+}  \rightarrow [0,1]$, initiation set $I \subseteq  S$。当option $\omega$选定后，根据$\pi_{\omega}$选择相应的动作，直到终止函数 $\beta_{\omega}$ 判断终止。</p>
<p>这是一个标准的Option-Critic结构，一大堆公式就省略了。但是最后作者却用Actor-Critic去解释图2，即switch network是option-value函数，作为Critic来评价option，并且为termination network提供更新梯度（参考上面Actor-Critic的ppt）。termination network作为Actor评估正在使用的跟踪器性能，以决定它是否应该在当前帧终止。</p>
<p>switch network的奖励函数定义如下：</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210629214943.png" style="zoom:80%;" />

<p>其中$P$表示被选中的跟踪器的跟踪框与GT的IOU，$P^*$表示未被选中的跟踪器的跟踪框与GT的IOU，$D_{IoU}$表示两者的差。按照公式7总共有3种情况：一个成功一个失败，两个均成功，两个均失败。</p>
<p>训练按照Actor-Critic训练，Critic使用贝尔曼方程（TD learning）更新，Actor使用策略梯度更新。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210629215908.png"></p>
<p>图5是一个可视化结果，其中终止概率1表示终止，0表示保持不变。可以看到初始是SiamFC（黄框）表现较好；当发生形变后，FCT的价值函数更大，终止概率趋近1，跟踪器切换；之后一直都是FCT表现更好，因此终止概率始终在0附近。</p>
<hr>
<h1 id="Fast-Template-Matching-and-Update-for-Video-Object-Tracking-and-Segmentation"><a href="#Fast-Template-Matching-and-Update-for-Video-Object-Tracking-and-Segmentation" class="headerlink" title="Fast Template Matching and Update for Video Object Tracking and Segmentation"></a>Fast Template Matching and Update for Video Object Tracking and Segmentation</h1><h2 id="Motivation-2"><a href="#Motivation-2" class="headerlink" title="Motivation"></a>Motivation</h2><p>第三篇来自CVPR2020。本文针对的任务是<strong>多实例半监督视频目标分割</strong>(VOS)。基于检测的算法被广泛应用于这一任务，难点在于选择匹配方法来预测结果，以及是否更新目标模板。 本文利用强化学习来同时做出这两个决策。具体来说，<strong>agent根据预测结果的质量来决定是否更新目标模板</strong>。 匹配方法的选择则基于agent的动作历史来确定。</p>
<p>目前大部分VOT或VOS方法主要分为三步：</p>
<ol>
<li> 对当前帧进行实例分割，生成一系列候选proposal；</li>
<li>将目标模板和所有proposal进行匹配，找到正确的proposal作为最终结果；</li>
<li>使用当前帧的预测结果替换目标模板。</li>
</ol>
<p>针对步骤2，基于外观的匹配方法（siamese）准确但非常耗时，而直接利用候选框与前一帧预测框的IOU进行快速匹配只适用于目标缓慢移动或变化。针对步骤3，现有方法简单地直接用当前结果替换模板，不考虑结果的正确性，会导致误差逐渐累积。因此需要利用强化学习智能切换。</p>
<h2 id="Method-2"><a href="#Method-2" class="headerlink" title="Method"></a>Method</h2><p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630113658.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630171122.png"></p>
<center>将上一帧中所有目标框的最小外接矩形bm扩大一定倍率得到搜素区域bs。</center>

<p>整体方法如图2所示，分为三个步骤：</p>
<ol>
<li>按照图3的方式确定目标搜索区域，采用实例分割网络（如YOLACT, Mask-RCNN）生成候选预测，然后利用基于IOU的匹配方法得到初步匹配结果；</li>
<li>通过agent判断初步结果的正确性和质量，决定是否更新模板；</li>
<li>确定是否需要切换到基于外观的匹配的方法。若连续N帧初步结果都不好（即第二步预测不更新模板），则切换到基于外观的匹配。此时会将整个图像送入网络。</li>
</ol>
<p>下面介绍将Actor-Critic的框架嵌入上述模型</p>
<p><strong>Action</strong></p>
<p>首先定义相关的符号，如图4所示，目标模板包括边界框 $T_{box}$，mask $T_{mask}$，$T_{box}$中的图像内容$T’<em>{box}$，$T</em>{mask}$中的图像内容$T’<em>{mask}$。而预测结果则是类似的$P</em>{box}, P_{mask}, P’<em>{box}, P’</em>{mask}$。</p>
<img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630173712.png" style="zoom:80%;" />

<p>第一个决策是是否更新模板，agent的动作 $a_i \in A$ 有两种情况，$a_0$表示用当前结果更新模板，$a_1$表示不更新。</p>
<p>第二个决策是匹配方法选择快速的基于IOU匹配还是精确的基于外观匹配，在精度和速度间取得平衡。前者分别计算模板和候选预测的box IOU和mask IOU，选择IOU最大的作为匹配结果；后者则是计算模板图像块$T’<em>{box}$和候选预测图像块$P’</em>{box}$的相似性，选择最像的作为匹配结果。<strong>注意这里没有另外增加一个agent</strong>，而是根据第一个agent的历史决策来决定。若agent连续N帧预测$a_1$，表示目标很可能丢失，此时需要切换到基于外观的匹配方法。</p>
<p><strong>State</strong> </p>
<p>输入agent的状态$s_t$包括两部分，如下式。第一部分$S_T$是模板图像，其中边框$T’_{box}$之内的内容保持不变，之外的内容填充黑色；第二部分$S_P$是搜索图像，同样将mask之内的内容保持不变，之外的内容填充黑色。提取这两种图像的特征并相加得到输入状态。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630175114.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630175123.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630175138.png"></p>
<p><strong>Reward</strong></p>
<p>奖励函数定义：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630175755.png"></p>
<p>$J_t$表示$P_{mask}$和GT mask之间的IOU。</p>
<p><strong>Actor-Critic Training</strong></p>
<p>基本元素确定后，按照Actor-Critic框架训练。</p>
<p>critic网络用value-based的方式训练：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630180416.png"></p>
<p>$\delta_t$ 表示TD error，公式8中梯度下降用加号是因为公式9减法顺序和常规的是反过来的。</p>
<p>actor用policy-based方法训练：</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630192837.png"></p>
<p>公式11减$V(s_t)$表示是带baseline的策略梯度。</p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630193309.png"></p>
<p><img src="https://gitee.com/zjphust/picgo/raw/master/blog/42-rl-tracking/20210630193324.png"></p>
<p>图1和图6表面本文的方法在VOT和VOS任务上均能在速度和精度上取得一个较好的平衡。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>以上三种方法分别介绍了利用强化学习来决策使用的跟踪特征，多个跟踪器的切换以及是否更新模板。可以发现，应用的方向基本都是把跟踪方法中某些需要启发式设计的模块换成了强化学习进行智能决策。此外，第一篇和第三篇均提到了引入强化学习可以在一定程度上提速，对于某些简单的情况，agent可以决策使用简单的方法进行跟踪。</p>
]]></content>
      <categories>
        <category>目标跟踪</category>
      </categories>
      <tags>
        <tag>综述</tag>
        <tag>强化学习</tag>
      </tags>
  </entry>
</search>
